<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://github.com/aws-samples/easy-model-deployer/print_page/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Print Site - Easy Model Deployer</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/print-site-enum-headings1.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings2.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings3.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings4.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings5.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings6.css" rel="stylesheet" />
        <link href="../css/print-site.css" rel="stylesheet" />
        <link href="../css/print-site-readthedocs.css" rel="stylesheet" />
        <link href="../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Print Site";
        var mkdocs_page_input_path = "print_page.md";
        var mkdocs_page_url = "/aws-samples/easy-model-deployer/print_page/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 

        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Easy Model Deployer
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="/aws-samples/easy-model-deployer//search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation/">Quick Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_models/">Supported Models</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../best_deployment_practices/">Best Deployment Practices</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../commands/">CLI Commands</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../api/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../sdk_api/">SDK API</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../local_deployment/">Local Deployment</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../sdk_integration/">LangChain Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../swift_chat_integration/">SwiftChat Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dify_integration/">Dify Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../langflow_integration/">LangFlow Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../flowise_integration/">Flowise Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ollama_integration/">Ollama Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nextchat_integration/">NextChat Integration</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Easy Model Deployer</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Print Site</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="installation"><h1 id="installation-quick-start-guide">Quick Start Guide</h1>
<p>This guide provides simple step-by-step instructions for using Easy Model Deployer (EMD).</p>
<h2 id="installation-prerequisites">Prerequisites</h2>
<p>Before installing Easy Model Deployer, ensure your environment meets the following requirements:</p>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>Version 3.9 or higher required. EMD leverages modern Python features for optimal performance.</td>
</tr>
<tr>
<td><strong>pip</strong></td>
<td>The Python package installer must be available to install EMD and its dependencies.</td>
</tr>
<tr>
<td><strong>AWS Account</strong></td>
<td>Required for deploying models to AWS services (SageMaker, ECS, EC2).</td>
</tr>
<tr>
<td><strong>AWS CLI</strong></td>
<td>Configured with appropriate credentials and permissions for resource creation.</td>
</tr>
<tr>
<td><strong>Internet Connection</strong></td>
<td>Required for downloading model artifacts and dependencies.</td>
</tr>
</tbody>
</table>
<p>For local deployments, additional requirements apply. See the <a href="#local_deployment">Local Deployment Guide</a> for details.</p>
<h2 id="installation-installation">Installation</h2>
<p><strong>Optional: Create a Virtual Environment</strong></p>
<p>It's recommended to install EMD in a virtual environment to avoid conflicts with other Python packages:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1"><span class="c1"># Create a virtual environment</span>
</span><span id="__span-0-2">python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>.venv
</span><span id="__span-0-3">
</span><span id="__span-0-4"><span class="c1"># Activate the virtual environment</span>
</span><span id="__span-0-5"><span class="c1"># On macOS/Linux:</span>
</span><span id="__span-0-6"><span class="nb">source</span><span class="w"> </span>.venv/bin/activate
</span><span id="__span-0-7"><span class="c1"># On Windows:</span>
</span><span id="__span-0-8"><span class="c1"># .venv\Scripts\activate</span>
</span></code></pre></div>
<p>After activating the virtual environment, your terminal prompt should change to indicate the active environment. You can then proceed with installation.</p>
<p>Easy Model Deployer can be installed using various package managers. Choose the method that best suits your workflow:</p>
<p><strong>Using pip:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-1-1">pip<span class="w"> </span>install<span class="w"> </span>easy-model-deployer
</span></code></pre></div></p>
<p><strong>Using pipx:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-2-1"><span class="c1"># Install pipx if you don&#39;t have it</span>
</span><span id="__span-2-2">pip<span class="w"> </span>install<span class="w"> </span>--user<span class="w"> </span>pipx
</span><span id="__span-2-3">pipx<span class="w"> </span>ensurepath
</span><span id="__span-2-4">
</span><span id="__span-2-5"><span class="c1"># Install EMD</span>
</span><span id="__span-2-6">pipx<span class="w"> </span>install<span class="w"> </span>easy-model-deployer
</span></code></pre></div></p>
<p><strong>Using uv:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-3-1"><span class="c1"># Install uv if you don&#39;t have it</span>
</span><span id="__span-3-2">pip<span class="w"> </span>install<span class="w"> </span>uv
</span><span id="__span-3-3">
</span><span id="__span-3-4"><span class="c1"># Install EMD</span>
</span><span id="__span-3-5">uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>easy-model-deployer
</span></code></pre></div></p>
<p><strong>Verification:</strong>
After installation, verify that EMD is working correctly by running:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-4-1">emd<span class="w"> </span>version
</span></code></pre></div>
This should display the installed version of Easy Model Deployer.</p>
<p><strong>Upgrading:</strong>
To upgrade to the latest version:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-5-1"><span class="c1"># Using pip</span>
</span><span id="__span-5-2">pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>easy-model-deployer
</span><span id="__span-5-3">
</span><span id="__span-5-4"><span class="c1"># Using pipx</span>
</span><span id="__span-5-5">pipx<span class="w"> </span>upgrade<span class="w"> </span>easy-model-deployer
</span><span id="__span-5-6">
</span><span id="__span-5-7"><span class="c1"># Using uv</span>
</span><span id="__span-5-8">uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>easy-model-deployer
</span></code></pre></div>
<blockquote>
<p><strong>Note</strong>: After upgrading, you should run <code>emd bootstrap</code> again to ensure your environment is updated.</p>
</blockquote>
<h2 id="installation-bootstrap">Bootstrap</h2>
<p>Before deploying models, you need to bootstrap the environment:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-6-1">emd<span class="w"> </span>bootstrap
</span></code></pre></div>
<blockquote>
<p><strong>Note</strong>: You need to run this command again after upgrading EMD with pip to update the environment.</p>
</blockquote>
<h2 id="installation-deploy-a-model">Deploy a Model</h2>
<p>Deploy a model using the interactive CLI:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-7-1">emd<span class="w"> </span>deploy
</span></code></pre></div>
<p>The CLI will guide you through selecting:
- Model series
- Specific model
- Deployment service (SageMaker, ECS, EC2, or Local)
- Instance type or GPU IDs
- Inference engine
- Additional parameters</p>
<p>You can also deploy models directly with command line parameters:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-8-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-series<span class="w"> </span>llama<span class="w"> </span>--model-name<span class="w"> </span>llama-3.3-70b-instruct-awq<span class="w"> </span>--service<span class="w"> </span>SageMaker
</span></code></pre></div>
<p>For secure API access, you can configure an API key during deployment using the <code>--extra-params</code> option:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-9-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>&lt;model-id&gt;<span class="w"> </span>--instance-type<span class="w"> </span>&lt;instance-type&gt;<span class="w"> </span>--engine-type<span class="w"> </span>&lt;engine-type&gt;<span class="w"> </span>--service-type<span class="w"> </span>&lt;service-type&gt;<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-9-2"><span class="s1">  &quot;service_params&quot;: {</span>
</span><span id="__span-9-3"><span class="s1">    &quot;api_key&quot;: &quot;your-secure-api-key&quot;</span>
</span><span id="__span-9-4"><span class="s1">  }</span>
</span><span id="__span-9-5"><span class="s1">}&#39;</span>
</span></code></pre></div>
<p>When using the interactive CLI (<code>emd deploy</code>), you can also provide this JSON configuration in the "Extra Parameters" step of the deployment process. Simply paste the JSON structure when prompted for additional parameters:</p>
<div class="language-json highlight"><pre><span></span><code><span id="__span-10-1"><span class="p">{</span>
</span><span id="__span-10-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-3"><span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;your-secure-api-key&quot;</span>
</span><span id="__span-10-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-10-5"><span class="p">}</span>
</span></code></pre></div>
<p>This API key will be required for authentication when accessing your deployed model's endpoint, enhancing security for your inference services.</p>
<blockquote>
<p><strong>Note</strong>: For local deployment options and detailed model configurations, see the <a href="#local_deployment">Local Deployment Guide</a>.</p>
</blockquote>
<h2 id="installation-check-deployment-status">Check Deployment Status</h2>
<p>Monitor the status of your deployment:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-11-1">emd<span class="w"> </span>status
</span></code></pre></div>
<p>This command shows all active deployments with their ModelIds, endpoints, and status.</p>
<h2 id="installation-invoke-the-model">Invoke the Model</h2>
<p>Test your deployed model using the CLI:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-12-1">emd<span class="w"> </span>invoke<span class="w"> </span>&lt;ModelId&gt;
</span></code></pre></div>
<p>Replace <code>&lt;ModelId&gt;</code> with the ID shown in the status output.</p>
<h2 id="installation-integration-options">Integration Options</h2>
<p>EMD provides an OpenAI-compatible API that allows you to integrate with your deployed models using standard tools and libraries:</p>
<ul>
<li><strong>OpenAI Compatible API</strong>: The primary integration method using the OpenAI API format. Once you have the base URL and API key from the <code>emd status</code> command, you can access the API using the OpenAI SDK or any OpenAI-compatible client. <a href="#api">API Documentation</a></li>
<li><strong>EMD Client</strong>: For direct Python integration</li>
<li><strong>LangChain Interface</strong>: For integration with LangChain applications</li>
</ul>
<p>The API uses an OpenAI-compatible format, making it easy to switch between OpenAI's services and your deployed models with minimal code changes.</p>
<h2 id="installation-destroy-the-deployment">Destroy the Deployment</h2>
<p>When you no longer need the model, remove the deployment:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-13-1">emd<span class="w"> </span>destroy<span class="w"> </span>&lt;ModelId&gt;
</span></code></pre></div>
<p>Replace <code>&lt;ModelId&gt;</code> with the ID shown in the status output.</p>
<h2 id="installation-advanced-options">Advanced Options</h2>
<p>For more detailed information on:</p>
<ul>
<li>Advanced deployment parameters: See <a href="https://aws-samples.github.io/easy-model-deployer/en/best_deployment_practices/">Best Deployment Practices</a></li>
<li>Architecture details: See <a href="https://aws-samples.github.io/easy-model-deployer/en/architecture/">Architecture</a></li>
<li>Supported models: See <a href="https://aws-samples.github.io/easy-model-deployer/en/supported_models/">Supported Models</a></li>
<li>Local deployment: See <a href="#local_deployment">Local Deployment Guide</a></li>
<li>CLI commands reference: See <a href="#commands">CLI Commands</a></li>
<li>API documentation: See <a href="#api">API Documentation</a></li>
</ul></section><section class="print-page" id="supported_models"><table>
<thead>
<tr>
<th style="text-align: left;">ModeId</th>
<th style="text-align: left;">ModelSeries</th>
<th style="text-align: left;">ModelType</th>
<th style="text-align: left;">Supported Instances</th>
<th style="text-align: left;">Supported Services</th>
<th style="text-align: left;">Support China Region</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">glm-4-9b-chat</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">GLM-4-9B-0414</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">GLM-4-32B-0414</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">GLM-Z1-9B-0414</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">GLM-Z1-32B-0414</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">GLM-Z1-Rumination-32B-0414</td>
<td style="text-align: left;">glm4</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">internlm2_5-20b-chat-4bit-awq</td>
<td style="text-align: left;">internlm2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">internlm2_5-20b-chat</td>
<td style="text-align: left;">internlm2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">internlm2_5-7b-chat</td>
<td style="text-align: left;">internlm2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">internlm2_5-7b-chat-4bit</td>
<td style="text-align: left;">internlm2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">internlm2_5-1_8b-chat</td>
<td style="text-align: left;">internlm2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-7B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,inf2.8xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-72B-Instruct-AWQ</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge,inf2.24xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-72B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-72B-Instruct-AWQ-128k</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-32B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-0.5B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-1.5B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-3B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-14B-Instruct-AWQ</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-14B-Instruct</td>
<td style="text-align: left;">qwen2.5</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">QwQ-32B-Preview</td>
<td style="text-align: left;">qwen reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">QwQ-32B</td>
<td style="text-align: left;">qwen reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-8B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-0.6B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-1.7B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-4B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-14B-AWQ</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-14B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-32B-AWQ</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-32B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-30B-A3B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-235B-A22B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-235B-A22B-FP8</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">llama-3.3-70b-instruct-awq</td>
<td style="text-align: left;">llama</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-32B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-14B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-7B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-1.5B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-1.5B_ollama</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-1.5B-GGUF</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Qwen-32B-GGUF</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-Distill-Llama-8B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">deepseek-r1-distill-llama-70b-awq</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">deepseek-r1-671b-1.58bit_gguf</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.8xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.4xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">deepseek-r1-671b-2.51bit_gguf</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">deepseek-r1-671b-4bit_gguf</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.24xlarge,g5.48xlarge,g6.24xlarge,g6.48xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek-R1-0528-Qwen3-8B</td>
<td style="text-align: left;">deepseek reasoning model</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">deepseek-v3-UD-IQ1_M_ollama</td>
<td style="text-align: left;">deepseek v3</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">Baichuan-M1-14B-Instruct</td>
<td style="text-align: left;">baichuan</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">ReaderLM-v2</td>
<td style="text-align: left;">jina</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g4dn.2xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">txgemma-9b-chat</td>
<td style="text-align: left;">txgemma</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">txgemma-27b-chat</td>
<td style="text-align: left;">txgemma</td>
<td style="text-align: left;">llm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-VL-72B-Instruct-AWQ</td>
<td style="text-align: left;">qwen2vl</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-VL-72B-Instruct-AWQ</td>
<td style="text-align: left;">qwen2vl</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-VL-32B-Instruct</td>
<td style="text-align: left;">qwen2vl</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">QVQ-72B-Preview-AWQ</td>
<td style="text-align: left;">qwen reasoning model</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-VL-7B-Instruct</td>
<td style="text-align: left;">qwen2vl</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">UI-TARS-1.5-7B</td>
<td style="text-align: left;">agent</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">InternVL2_5-78B-AWQ</td>
<td style="text-align: left;">internvl2.5</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">gemma-3-4b-it</td>
<td style="text-align: left;">gemma3</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">gemma-3-12b-it</td>
<td style="text-align: left;">gemma3</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">gemma-3-27b-it</td>
<td style="text-align: left;">gemma3</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-Small-3.1-24B-Instruct-2503</td>
<td style="text-align: left;">mistral</td>
<td style="text-align: left;">vlm</td>
<td style="text-align: left;">g5.12xlarge,g5.24xlarge,g5.48xlarge</td>
<td style="text-align: left;">sagemaker_realtime,sagemaker_async,ecs</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">txt2video-LTX</td>
<td style="text-align: left;">comfyui</td>
<td style="text-align: left;">video</td>
<td style="text-align: left;">g5.4xlarge,g5.8xlarge,g6e.2xlarge</td>
<td style="text-align: left;">sagemaker_async</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">whisper</td>
<td style="text-align: left;">whisper</td>
<td style="text-align: left;">whisper</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_async</td>
<td style="text-align: left;">❎</td>
</tr>
<tr>
<td style="text-align: left;">bce-embedding-base_v1</td>
<td style="text-align: left;">bce</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">bge-base-en-v1.5</td>
<td style="text-align: left;">bge</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">bge-m3</td>
<td style="text-align: left;">bge</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">jina-embeddings-v3</td>
<td style="text-align: left;">jina</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">jina-embeddings-v4</td>
<td style="text-align: left;">jina</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-Embedding-0.6B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-Embedding-4B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">Qwen3-Embedding-8B</td>
<td style="text-align: left;">qwen3</td>
<td style="text-align: left;">embedding</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">bge-reranker-v2-m3</td>
<td style="text-align: left;">bge</td>
<td style="text-align: left;">rerank</td>
<td style="text-align: left;">g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">bge-reranker-large</td>
<td style="text-align: left;">bge</td>
<td style="text-align: left;">rerank</td>
<td style="text-align: left;">g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
<tr>
<td style="text-align: left;">jina-reranker-v2-base-multilingual</td>
<td style="text-align: left;">jina</td>
<td style="text-align: left;">rerank</td>
<td style="text-align: left;">g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge</td>
<td style="text-align: left;">sagemaker_realtime,ecs</td>
<td style="text-align: left;">✅</td>
</tr>
</tbody>
</table></section><section class="print-page" id="best_deployment_practices"><h1 id="best_deployment_practices-best-deployment-practices">Best Deployment Practices</h1>
<p>This document provides examples of best practices for deploying models using EMD for various use cases.</p>
<h2 id="best_deployment_practices-example-model-deployments">Example Model Deployments</h2>
<h3 id="best_deployment_practices-qwen-3-series">Qwen 3 Series</h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1">emd deploy --model-id Qwen3-30B-A3B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span><span id="__span-0-2">
</span><span id="__span-0-3">emd deploy --model-id Qwen3-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span><span id="__span-0-4">
</span><span id="__span-0-5">emd deploy --model-id Qwen3-8B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span></code></pre></div>
<h3 id="best_deployment_practices-glm-z10414-series">GLM Z1/0414 Series</h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1">emd deploy --model-id GLM-Z1-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span><span id="__span-1-2">
</span><span id="__span-1-3">emd deploy --model-id GLM-4-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span></code></pre></div>
<h3 id="best_deployment_practices-mistral-small-series">Mistral Small Series</h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1">emd deploy --model-id Mistral-Small-3.1-24B-Instruct-2503 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span></code></pre></div>
<h3 id="best_deployment_practices-gemma-3-series">Gemma 3 Series</h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1">emd deploy --model-id gemma-3-27b-it --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime
</span></code></pre></div>
<h3 id="best_deployment_practices-qwen-series">Qwen Series</h3>
<h4 id="best_deployment_practices-qwen25-vl-32b-instruct">Qwen2.5-VL-32B-Instruct</h4>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-4-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-VL-32B-Instruct<span class="w"> </span>--instance-type<span class="w"> </span>g5.12xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime
</span></code></pre></div>
<h4 id="best_deployment_practices-qwq-32b">QwQ-32B</h4>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-5-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>QwQ-32B<span class="w"> </span>--instance-type<span class="w"> </span>g5.12xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime
</span></code></pre></div>
<h2 id="best_deployment_practices-deploying-to-specific-gpu-types">Deploying to Specific GPU Types</h2>
<p>Choosing the right GPU type is critical for optimal performance and cost-efficiency. Use the <code>--instance-type</code> parameter to specify the GPU instance.</p>
<h3 id="best_deployment_practices-example-deploying-qwen25-7b-on-g52xlarge">Example: Deploying Qwen2.5-7B on g5.2xlarge</h3>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-6-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>--instance-type<span class="w"> </span>g5.2xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime
</span></code></pre></div>
<h2 id="best_deployment_practices-achieving-longer-context-windows">Achieving Longer Context Windows</h2>
<p>To enable longer context windows, use the <code>--extra-params</code> option with engine-specific parameters.</p>
<h3 id="best_deployment_practices-example-deploying-model-with-16k-context-window">Example: Deploying model with 16k context window</h3>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-7-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>--instance-type<span class="w"> </span>g5.4xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-7-2"><span class="s1">  &quot;engine_params&quot;: {</span>
</span><span id="__span-7-3"><span class="s1">    &quot;cli_args&quot;: &quot;--max_model_len 16000 --max_num_seqs 4&quot;</span>
</span><span id="__span-7-4"><span class="s1">  }</span>
</span><span id="__span-7-5"><span class="s1">}&#39;</span>
</span></code></pre></div>
<h3 id="best_deployment_practices-example-deploying-model-on-g4dn-instance">Example: Deploying model on G4dn instance</h3>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-8-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-14B-Instruct-AWQ<span class="w"> </span>--instance-type<span class="w"> </span>g4dn.2xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-8-2"><span class="s1">  &quot;engine_params&quot;: {</span>
</span><span id="__span-8-3"><span class="s1">    &quot;environment_variables&quot;: &quot;export VLLM_ATTENTION_BACKEND=XFORMERS &amp;&amp; export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&quot;,</span>
</span><span id="__span-8-4"><span class="s1">    &quot;default_cli_args&quot;: &quot; --chat-template emd/models/chat_templates/qwen_2d5_add_prefill_chat_template.jinja --max_model_len 12000 --max_num_seqs 10  --gpu_memory_utilization 0.95 --disable-log-stats --enable-auto-tool-choice --tool-call-parser hermes&quot;</span>
</span><span id="__span-8-5"><span class="s1">  }</span>
</span><span id="__span-8-6"><span class="s1">}&#39;</span>
</span></code></pre></div>
<h2 id="best_deployment_practices-extra-parameters-usage">Extra Parameters Usage</h2>
<p>The <code>--extra-params</code> option allows you to customize various aspects of your deployment. This section provides a detailed reference for the available parameters organized by category.</p>
<h3 id="best_deployment_practices-parameter-structure">Parameter Structure</h3>
<p>The extra parameters are structured as a JSON object with the following top-level categories:</p>
<div class="language-json highlight"><pre><span></span><code><span id="__span-9-1"><span class="p">{</span>
</span><span id="__span-9-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
</span><span id="__span-9-3"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
</span><span id="__span-9-4"><span class="w">  </span><span class="nt">&quot;instance_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
</span><span id="__span-9-5"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
</span><span id="__span-9-6"><span class="w">  </span><span class="nt">&quot;framework_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{}</span>
</span><span id="__span-9-7"><span class="p">}</span>
</span></code></pre></div>
<h3 id="best_deployment_practices-model-parameters">Model Parameters</h3>
<p>Model parameters control how the model is loaded and prepared.</p>
<h4 id="best_deployment_practices-model-source-configuration">Model Source Configuration</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-10-1"><span class="p">{</span>
</span><span id="__span-10-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-10-3"><span class="w">    </span><span class="nt">&quot;model_files_s3_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://your-bucket/model-path&quot;</span><span class="p">,</span>
</span><span id="__span-10-4"><span class="w">    </span><span class="nt">&quot;model_files_local_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/path/to/local/model&quot;</span><span class="p">,</span>
</span><span id="__span-10-5"><span class="w">    </span><span class="nt">&quot;model_files_download_source&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;huggingface|modelscope|auto&quot;</span><span class="p">,</span>
</span><span id="__span-10-6"><span class="w">    </span><span class="nt">&quot;huggingface_model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;organization/model-name&quot;</span><span class="p">,</span>
</span><span id="__span-10-7"><span class="w">    </span><span class="nt">&quot;modelscope_model_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;organization/model-name&quot;</span><span class="p">,</span>
</span><span id="__span-10-8"><span class="w">    </span><span class="nt">&quot;need_prepare_model&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
</span><span id="__span-10-9"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-10-10"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>model_files_s3_path</code>: Load model directly from an S3 path</li>
<li><code>model_files_local_path</code>: Load model from a local path (only for local deployment)</li>
<li><code>model_files_download_source</code>: Specify the source for downloading model files</li>
<li><code>huggingface_model_id</code>: Specify a custom Hugging Face model ID</li>
<li><code>modelscope_model_id</code>: Specify a custom ModelScope model ID</li>
<li><code>need_prepare_model</code>: Set to <code>false</code> to skip downloading and uploading model files (reduces deployment time)</li>
</ul>
<h3 id="best_deployment_practices-service-parameters">Service Parameters</h3>
<p>Service parameters configure the deployment service behavior.</p>
<h4 id="best_deployment_practices-api-security">API Security</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-11-1"><span class="p">{</span>
</span><span id="__span-11-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-11-3"><span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;your-secure-api-key&quot;</span>
</span><span id="__span-11-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-11-5"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>api_key</code>: Set a custom API key for securing access to your model endpoint</li>
</ul>
<h4 id="best_deployment_practices-sagemaker-specific-parameters">SageMaker-specific Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-12-1"><span class="p">{</span>
</span><span id="__span-12-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-12-3"><span class="w">    </span><span class="nt">&quot;max_capacity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
</span><span id="__span-12-4"><span class="w">    </span><span class="nt">&quot;min_capacity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-12-5"><span class="w">    </span><span class="nt">&quot;auto_scaling_target_value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">15</span><span class="p">,</span>
</span><span id="__span-12-6"><span class="w">    </span><span class="nt">&quot;sagemaker_endpoint_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;custom-endpoint-name&quot;</span>
</span><span id="__span-12-7"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-12-8"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>max_capacity</code>: Maximum number of instances for auto-scaling</li>
<li><code>min_capacity</code>: Minimum number of instances for auto-scaling</li>
<li><code>auto_scaling_target_value</code>: Target value for auto-scaling (in requests per minute)</li>
<li><code>sagemaker_endpoint_name</code>: Custom name for the SageMaker endpoint</li>
</ul>
<h4 id="best_deployment_practices-ecs-specific-parameters">ECS-specific Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-13-1"><span class="p">{</span>
</span><span id="__span-13-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-13-3"><span class="w">    </span><span class="nt">&quot;desired_capacity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
</span><span id="__span-13-4"><span class="w">    </span><span class="nt">&quot;max_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
</span><span id="__span-13-5"><span class="w">    </span><span class="nt">&quot;vpc_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;vpc-12345&quot;</span><span class="p">,</span>
</span><span id="__span-13-6"><span class="w">    </span><span class="nt">&quot;subnet_ids&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;subnet-12345,subnet-67890&quot;</span>
</span><span id="__span-13-7"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-13-8"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>desired_capacity</code>: Desired number of ECS tasks</li>
<li><code>max_size</code>: Maximum number of ECS tasks for auto-scaling</li>
<li><code>vpc_id</code>: Custom VPC ID for deployment</li>
<li><code>subnet_ids</code>: Comma-separated list of subnet IDs</li>
</ul>
<h3 id="best_deployment_practices-engine-parameters">Engine Parameters</h3>
<p>Engine parameters control the behavior of the inference engine.</p>
<h4 id="best_deployment_practices-common-engine-parameters">Common Engine Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-14-1"><span class="p">{</span>
</span><span id="__span-14-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-14-3"><span class="w">    </span><span class="nt">&quot;environment_variables&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;export VAR1=value1 &amp;&amp; export VAR2=value2&quot;</span><span class="p">,</span>
</span><span id="__span-14-4"><span class="w">    </span><span class="nt">&quot;cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--specific-engine-arg value&quot;</span><span class="p">,</span>
</span><span id="__span-14-5"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--common-engine-arg value&quot;</span>
</span><span id="__span-14-6"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-14-7"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>environment_variables</code>: Set environment variables for the engine</li>
<li><code>cli_args</code>: Specific command line arguments for the engine</li>
<li><code>default_cli_args</code>: Default command line arguments for the engine</li>
</ul>
<h4 id="best_deployment_practices-vllm-specific-parameters">vLLM-specific Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-15-1"><span class="p">{</span>
</span><span id="__span-15-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-15-3"><span class="w">    </span><span class="nt">&quot;cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max_model_len 16000 --max_num_seqs 4 --gpu_memory_utilization 0.9&quot;</span><span class="p">,</span>
</span><span id="__span-15-4"><span class="w">    </span><span class="nt">&quot;environment_variables&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;export VLLM_ATTENTION_BACKEND=FLASHINFER &amp;&amp; export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&quot;</span>
</span><span id="__span-15-5"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-15-6"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>cli_args</code>: Command line arguments specific to vLLM</li>
<li>Common vLLM parameters:</li>
<li><code>--max_model_len</code>: Maximum context length</li>
<li><code>--max_num_seqs</code>: Maximum number of sequences</li>
<li><code>--gpu_memory_utilization</code>: GPU memory utilization (0.0-1.0)</li>
<li><code>--disable-log-stats</code>: Disable logging of statistics</li>
<li><code>--enable-auto-tool-choice</code>: Enable automatic tool choice</li>
<li><code>--tool-call-parser</code>: Specify tool call parser (e.g., hermes, pythonic)</li>
<li><code>--enable-reasoning</code>: Enable reasoning capabilities</li>
<li><code>--reasoning-parser</code>: Specify reasoning parser (e.g., deepseek_r1, granite)</li>
<li><code>--chat-template</code>: Path to chat template file</li>
</ul>
<h4 id="best_deployment_practices-tgi-specific-parameters">TGI-specific Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-16-1"><span class="p">{</span>
</span><span id="__span-16-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-16-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max-total-tokens 30000 --max-concurrent-requests 30&quot;</span>
</span><span id="__span-16-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-16-5"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li>Common TGI parameters:</li>
<li><code>--max-total-tokens</code>: Maximum total tokens</li>
<li><code>--max-concurrent-requests</code>: Maximum concurrent requests</li>
<li><code>--max-batch-size</code>: Maximum batch size</li>
<li><code>--max-input-tokens</code>: Maximum input tokens</li>
</ul>
<h3 id="best_deployment_practices-framework-parameters">Framework Parameters</h3>
<p>Framework parameters configure the web framework serving the model.</p>
<h4 id="best_deployment_practices-fastapi-parameters">FastAPI Parameters</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-17-1"><span class="p">{</span>
</span><span id="__span-17-2"><span class="w">  </span><span class="nt">&quot;framework_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-17-3"><span class="w">    </span><span class="nt">&quot;limit_concurrency&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span>
</span><span id="__span-17-4"><span class="w">    </span><span class="nt">&quot;timeout_keep_alive&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">120</span><span class="p">,</span>
</span><span id="__span-17-5"><span class="w">    </span><span class="nt">&quot;uvicorn_log_level&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;info&quot;</span>
</span><span id="__span-17-6"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-17-7"><span class="p">}</span>
</span></code></pre></div>
<ul>
<li><code>limit_concurrency</code>: Maximum number of concurrent connections</li>
<li><code>timeout_keep_alive</code>: Timeout for keeping connections alive (in seconds)</li>
<li><code>uvicorn_log_level</code>: Log level for Uvicorn server (debug, info, warning, error, critical)</li>
</ul>
<h3 id="best_deployment_practices-example-configurations">Example Configurations</h3>
<h4 id="best_deployment_practices-example-high-throughput-configuration">Example: High-throughput Configuration</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-18-1"><span class="p">{</span>
</span><span id="__span-18-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-18-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max_model_len 8000 --max_num_seqs 20 --gpu_memory_utilization 0.95&quot;</span>
</span><span id="__span-18-4"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-18-5"><span class="w">  </span><span class="nt">&quot;framework_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-18-6"><span class="w">    </span><span class="nt">&quot;limit_concurrency&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span>
</span><span id="__span-18-7"><span class="w">    </span><span class="nt">&quot;timeout_keep_alive&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">30</span>
</span><span id="__span-18-8"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-18-9"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-18-10"><span class="w">    </span><span class="nt">&quot;max_capacity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
</span><span id="__span-18-11"><span class="w">    </span><span class="nt">&quot;min_capacity&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
</span><span id="__span-18-12"><span class="w">    </span><span class="nt">&quot;auto_scaling_target_value&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span>
</span><span id="__span-18-13"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-18-14"><span class="p">}</span>
</span></code></pre></div>
<h4 id="best_deployment_practices-example-long-context-configuration">Example: Long Context Configuration</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-19-1"><span class="p">{</span>
</span><span id="__span-19-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-19-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max_model_len 32000 --max_num_seqs 2 --gpu_memory_utilization 0.9&quot;</span>
</span><span id="__span-19-4"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-19-5"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-19-6"><span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;your-secure-api-key&quot;</span>
</span><span id="__span-19-7"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-19-8"><span class="p">}</span>
</span></code></pre></div>
<h4 id="best_deployment_practices-example-secure-api-with-custom-endpoint-name">Example: Secure API with Custom Endpoint Name</h4>
<div class="language-json highlight"><pre><span></span><code><span id="__span-20-1"><span class="p">{</span>
</span><span id="__span-20-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-20-3"><span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;your-secure-api-key&quot;</span><span class="p">,</span>
</span><span id="__span-20-4"><span class="w">    </span><span class="nt">&quot;sagemaker_endpoint_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;my-custom-llm-endpoint&quot;</span>
</span><span id="__span-20-5"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-20-6"><span class="p">}</span>
</span></code></pre></div>
<h3 id="best_deployment_practices-model-source-configuration_1">Model Source Configuration</h3>
<p>You can load models from different locations by adding appropriate values in the extra-params parameter:</p>
<ol>
<li>Load model from S3
<div class="language-json highlight"><pre><span></span><code><span id="__span-21-1"><span class="p">{</span>
</span><span id="__span-21-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:{</span>
</span><span id="__span-21-3"><span class="w">    </span><span class="nt">&quot;model_files_s3_path&quot;</span><span class="p">:</span><span class="s2">&quot;&lt;S3_PATH&gt;&quot;</span>
</span><span id="__span-21-4"><span class="w">    </span><span class="p">}</span>
</span><span id="__span-21-5"><span class="p">}</span>
</span></code></pre></div></li>
<li>Load model from local path (only applicable for local deployment)
<div class="language-json highlight"><pre><span></span><code><span id="__span-22-1"><span class="p">{</span>
</span><span id="__span-22-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">    </span><span class="nt">&quot;model_files_local_path&quot;</span><span class="p">:</span><span class="s2">&quot;&lt;LOCAL_PATH&gt;&quot;</span>
</span><span id="__span-22-3"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-22-4"><span class="p">}</span>
</span></code></pre></div></li>
<li>Skip downloading and uploading model files in codebuild, which will significantly reduce deployment time
<div class="language-json highlight"><pre><span></span><code><span id="__span-23-1"><span class="p">{</span>
</span><span id="__span-23-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-23-3"><span class="w">    </span><span class="nt">&quot;need_prepare_model&quot;</span><span class="p">:</span><span class="kc">false</span>
</span><span id="__span-23-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-23-5"><span class="p">}</span>
</span></code></pre></div></li>
<li>Specify the download source for model files
<div class="language-json highlight"><pre><span></span><code><span id="__span-24-1"><span class="p">{</span>
</span><span id="__span-24-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:{</span>
</span><span id="__span-24-3"><span class="w">    </span><span class="nt">&quot;model_files_download_source&quot;</span><span class="p">:</span><span class="s2">&quot;huggingface|modelscope|auto(default)&quot;</span>
</span><span id="__span-24-4"><span class="w">    </span><span class="p">}</span>
</span><span id="__span-24-5"><span class="p">}</span>
</span></code></pre></div></li>
<li>Specify the model ID on huggingface or modelscope
<div class="language-json highlight"><pre><span></span><code><span id="__span-25-1"><span class="p">{</span>
</span><span id="__span-25-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-25-3"><span class="w">    </span><span class="nt">&quot;huggingface_model_id&quot;</span><span class="p">:</span><span class="s2">&quot;model id on huggingface&quot;</span><span class="p">,</span><span class="nt">&quot;modelscope_model_id&quot;</span><span class="p">:</span><span class="s2">&quot;model id on modelscope&quot;</span>
</span><span id="__span-25-4"><span class="w">    </span><span class="p">}</span>
</span><span id="__span-25-5"><span class="p">}</span>
</span></code></pre></div></li>
</ol>
<h2 id="best_deployment_practices-environmental-variables">Environmental variables</h2>
<ul>
<li><code>LOCAL_DEPLOY_PORT:</code> Local deployment port, default: <code>8080</code></li>
</ul>
<h2 id="best_deployment_practices-common-troubleshooting">Common Troubleshooting</h2>
<p>This section covers common issues you might encounter during model deployment and their solutions.</p>
<h3 id="best_deployment_practices-memory-related-issues">Memory-Related Issues</h3>
<p>If your deployment fails with out-of-memory (OOM) errors:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-26-1">CUDA out of memory. Tried to allocate X.XX GiB
</span></code></pre></div>
<p>Try these solutions:</p>
<ol>
<li><strong>Use a larger instance type</strong>:</li>
<li>Upgrade to an instance with more GPU memory (e.g., from g5.2xlarge to g5.4xlarge)</li>
<li>
<p>For large models (&gt;30B parameters), consider using multiple GPUs with g5.12xlarge or g5.48xlarge</p>
</li>
<li>
<p><strong>Adjust engine parameters</strong>:
   <div class="language-json highlight"><pre><span></span><code><span id="__span-27-1"><span class="p">{</span>
</span><span id="__span-27-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-27-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max_model_len 8000 --max_num_seqs 4 --gpu_memory_utilization 0.8&quot;</span>
</span><span id="__span-27-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-27-5"><span class="p">}</span>
</span></code></pre></div></p>
</li>
<li>Reduce <code>max_model_len</code> to decrease context window size</li>
<li>Lower <code>max_num_seqs</code> to reduce concurrent sequences</li>
<li>
<p>Set <code>gpu_memory_utilization</code> to a lower value (e.g., 0.8 instead of the default 0.9)</p>
</li>
<li>
<p><strong>Use quantized models</strong>:</p>
</li>
<li>Deploy AWQ or GPTQ quantized versions of models (e.g., Qwen2.5-7B-Instruct-AWQ instead of Qwen2.5-7B-Instruct)</li>
</ol>
<h3 id="best_deployment_practices-deployment-timeout-issues">Deployment Timeout Issues</h3>
<p>If your deployment times out during model preparation:</p>
<ol>
<li>
<p><strong>Skip model preparation</strong>:
   <div class="language-json highlight"><pre><span></span><code><span id="__span-28-1"><span class="p">{</span>
</span><span id="__span-28-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-28-3"><span class="w">    </span><span class="nt">&quot;need_prepare_model&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
</span><span id="__span-28-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-28-5"><span class="p">}</span>
</span></code></pre></div></p>
</li>
<li>
<p><strong>Use pre-downloaded models</strong>:
   <div class="language-json highlight"><pre><span></span><code><span id="__span-29-1"><span class="p">{</span>
</span><span id="__span-29-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-29-3"><span class="w">    </span><span class="nt">&quot;model_files_s3_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;s3://your-bucket/model-path&quot;</span>
</span><span id="__span-29-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-29-5"><span class="p">}</span>
</span></code></pre></div></p>
</li>
</ol>
<h3 id="best_deployment_practices-api-connection-issues">API Connection Issues</h3>
<p>If you can't connect to your deployed model's API:</p>
<ol>
<li>
<p><strong>Check endpoint status</strong>:
   <div class="language-bash highlight"><pre><span></span><code><span id="__span-30-1">emd<span class="w"> </span>status
</span></code></pre></div>
   Ensure the status is "InService" or "Running"</p>
</li>
<li>
<p><strong>Verify API key</strong>:</p>
</li>
<li>Ensure you're using the correct API key in your requests</li>
<li>
<p>If you've set a custom API key, make sure to include it in your requests</p>
</li>
<li>
<p><strong>Test with curl</strong>:
   <div class="language-bash highlight"><pre><span></span><code><span id="__span-31-1">curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions<span class="w"> </span><span class="se">\</span>
</span><span id="__span-31-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-31-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer your-api-key&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-31-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;model&quot;: &quot;your-model-id&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot;}]}&#39;</span>
</span></code></pre></div></p>
</li>
</ol>
<h3 id="best_deployment_practices-performance-optimization">Performance Optimization</h3>
<p>If your model is running slowly:</p>
<ol>
<li>
<p><strong>Increase GPU utilization</strong>:
   <div class="language-json highlight"><pre><span></span><code><span id="__span-32-1"><span class="p">{</span>
</span><span id="__span-32-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-32-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--gpu_memory_utilization 0.95&quot;</span>
</span><span id="__span-32-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-32-5"><span class="p">}</span>
</span></code></pre></div></p>
</li>
<li>
<p><strong>Adjust batch size and concurrency</strong>:
   <div class="language-json highlight"><pre><span></span><code><span id="__span-33-1"><span class="p">{</span>
</span><span id="__span-33-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-33-3"><span class="w">    </span><span class="nt">&quot;default_cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--max_num_seqs 20&quot;</span>
</span><span id="__span-33-4"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-33-5"><span class="w">  </span><span class="nt">&quot;framework_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-33-6"><span class="w">    </span><span class="nt">&quot;limit_concurrency&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span>
</span><span id="__span-33-7"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-33-8"><span class="p">}</span>
</span></code></pre></div></p>
</li>
<li>
<p><strong>Enable optimizations</strong> (for vLLM):
   <div class="language-json highlight"><pre><span></span><code><span id="__span-34-1"><span class="p">{</span>
</span><span id="__span-34-2"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-34-3"><span class="w">    </span><span class="nt">&quot;environment_variables&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;export VLLM_ATTENTION_BACKEND=FLASHINFER &amp;&amp; export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True&quot;</span>
</span><span id="__span-34-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-34-5"><span class="p">}</span>
</span></code></pre></div></p>
</li>
</ol></section><section class="print-page" id="commands"><h1 id="commands-cli-commands">CLI Commands</h1>
<p>This document provides a comprehensive guide to the command-line interface (CLI) commands available in the Easy Model Deployer (EMD) tool.</p>
<h2 id="commands-overview">Overview</h2>
<p>EMD provides the following main commands:</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bootstrap</code></td>
<td>Initialize AWS resources for model deployment</td>
</tr>
<tr>
<td><code>deploy</code></td>
<td>Deploy models to AWS infrastructure</td>
</tr>
<tr>
<td><code>status</code></td>
<td>Display status of deployed models</td>
</tr>
<tr>
<td><code>invoke</code></td>
<td>Test deployed models with sample requests</td>
</tr>
<tr>
<td><code>example</code></td>
<td>Generate sample code for API integration</td>
</tr>
<tr>
<td><code>destroy</code></td>
<td>Remove deployed models and clean up resources</td>
</tr>
<tr>
<td><code>list-supported-models</code></td>
<td>Display available models</td>
</tr>
<tr>
<td><code>profile</code></td>
<td>Configure AWS profile credentials</td>
</tr>
<tr>
<td><code>version</code></td>
<td>Display tool version information</td>
</tr>
</tbody>
</table>
<h2 id="commands-command-details">Command Details</h2>
<h3 id="commands-bootstrap">bootstrap</h3>
<p>Initialize AWS resources required for model deployment.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1">emd<span class="w"> </span>bootstrap<span class="w"> </span><span class="o">[</span>OPTIONS<span class="o">]</span>
</span></code></pre></div>
<p><strong>Options:</strong></p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--skip-confirm</code></td>
<td>Skip confirmation prompts</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong></p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-1-1">emd<span class="w"> </span>bootstrap
</span></code></pre></div>
<p>This command creates the necessary AWS resources, including an S3 bucket and CloudFormation stack, required for model deployment.</p>
<h3 id="commands-deploy">deploy</h3>
<p>Deploy models to AWS infrastructure.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-2-1">emd<span class="w"> </span>deploy<span class="w"> </span><span class="o">[</span>OPTIONS<span class="o">]</span>
</span></code></pre></div>
<p><strong>Options:</strong></p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--model-id TEXT</code></td>
<td>Model ID to deploy</td>
</tr>
<tr>
<td><code>-i, --instance-type TEXT</code></td>
<td>The instance type to use</td>
</tr>
<tr>
<td><code>-e, --engine-type TEXT</code></td>
<td>The name of the inference engine</td>
</tr>
<tr>
<td><code>-s, --service-type TEXT</code></td>
<td>The name of the service</td>
</tr>
<tr>
<td><code>--framework-type TEXT</code></td>
<td>The name of the framework</td>
</tr>
<tr>
<td><code>--model-tag TEXT</code></td>
<td>Custom tag for the model deployment</td>
</tr>
<tr>
<td><code>--extra-params TEXT</code></td>
<td>Extra parameters in JSON format</td>
</tr>
<tr>
<td><code>--skip-confirm</code></td>
<td>Skip confirmation prompts</td>
</tr>
<tr>
<td><code>--force-update-env-stack</code></td>
<td>Force update environment stack</td>
</tr>
<tr>
<td><code>--allow-local-deploy</code></td>
<td>Allow local instance deployment</td>
</tr>
<tr>
<td><code>--only-allow-local-deploy</code></td>
<td>Only allow local instance deployment</td>
</tr>
<tr>
<td><code>--dockerfile-local-path TEXT</code></td>
<td>Custom Dockerfile path for building the model image</td>
</tr>
<tr>
<td><code>--local-gpus TEXT</code></td>
<td>Local GPU IDs to deploy the model (e.g., <code>0,1,2</code>)</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Deploy a model with interactive prompts:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-3-1">emd<span class="w"> </span>deploy
</span></code></pre></div></p>
<p>Deploy a specific model with parameters:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-4-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>--instance-type<span class="w"> </span>g5.2xlarge<span class="w"> </span>--engine-type<span class="w"> </span>vllm<span class="w"> </span>--service-type<span class="w"> </span>sagemaker_realtime
</span></code></pre></div></p>
<p>Deploy a model locally:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-5-1">emd<span class="w"> </span>deploy<span class="w"> </span>--allow-local-deploy
</span></code></pre></div></p>
<p>Deploy with custom parameters:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-6-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{&quot;engine_params&quot;: {&quot;cli_args&quot;: &quot;--max_model_len 16000 --max_num_seqs 4&quot;}}&#39;</span>
</span></code></pre></div></p>
<h3 id="commands-status">status</h3>
<p>Display the status of deployed models.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-7-1">emd<span class="w"> </span>status<span class="w"> </span><span class="o">[</span>MODEL_ID<span class="o">]</span><span class="w"> </span><span class="o">[</span>MODEL_TAG<span class="o">]</span>
</span></code></pre></div>
<p><strong>Arguments:</strong></p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MODEL_ID</code></td>
<td>Optional model ID to check status for</td>
</tr>
<tr>
<td><code>MODEL_TAG</code></td>
<td>Optional model tag (defaults to "dev")</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Check status of all deployed models:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-8-1">emd<span class="w"> </span>status
</span></code></pre></div></p>
<p>Check status of a specific model:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-9-1">emd<span class="w"> </span>status<span class="w"> </span>Qwen2.5-7B-Instruct
</span></code></pre></div></p>
<p>Check status of a specific model with a custom tag:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-10-1">emd<span class="w"> </span>status<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>custom-tag
</span></code></pre></div></p>
<h3 id="commands-invoke">invoke</h3>
<p>Test deployed models with sample requests.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-11-1">emd<span class="w"> </span>invoke<span class="w"> </span>MODEL_ID<span class="w"> </span><span class="o">[</span>MODEL_TAG<span class="o">]</span>
</span></code></pre></div>
<p><strong>Arguments:</strong></p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MODEL_ID</code></td>
<td>Model ID to invoke</td>
</tr>
<tr>
<td><code>MODEL_TAG</code></td>
<td>Optional model tag (defaults to "dev")</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Invoke a model:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-12-1">emd<span class="w"> </span>invoke<span class="w"> </span>DeepSeek-R1-Distill-Qwen-7B
</span></code></pre></div></p>
<p>Invoke a model with a custom tag:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-13-1">emd<span class="w"> </span>invoke<span class="w"> </span>DeepSeek-R1-Distill-Qwen-7B<span class="w"> </span>custom-tag
</span></code></pre></div></p>
<h3 id="commands-example">example</h3>
<p>Generate sample code for API integration with a deployed model.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1">emd<span class="w"> </span>example<span class="w"> </span>MODEL_ID/MODEL_TAG
</span></code></pre></div>
<p><strong>Arguments:</strong></p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MODEL_ID/MODEL_TAG</code></td>
<td>Model ID and optional tag (separated by "/")</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Generate examples for a model:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-15-1">emd<span class="w"> </span>example<span class="w"> </span>Qwen2.5-7B-Instruct
</span></code></pre></div></p>
<p>Generate examples for a model with a custom tag:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-16-1">emd<span class="w"> </span>example<span class="w"> </span>Qwen2.5-7B-Instruct/custom-tag
</span></code></pre></div></p>
<h3 id="commands-destroy">destroy</h3>
<p>Remove deployed models and clean up resources.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-17-1">emd<span class="w"> </span>destroy<span class="w"> </span>MODEL_ID<span class="w"> </span><span class="o">[</span>MODEL_TAG<span class="o">]</span>
</span></code></pre></div>
<p><strong>Arguments:</strong></p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MODEL_ID</code></td>
<td>Model ID to destroy</td>
</tr>
<tr>
<td><code>MODEL_TAG</code></td>
<td>Optional model tag (defaults to "dev")</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Destroy a model:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-18-1">emd<span class="w"> </span>destroy<span class="w"> </span>Qwen2.5-7B-Instruct
</span></code></pre></div></p>
<p>Destroy a model with a custom tag:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-19-1">emd<span class="w"> </span>destroy<span class="w"> </span>Qwen2.5-7B-Instruct<span class="w"> </span>custom-tag
</span></code></pre></div></p>
<h3 id="commands-list-supported-models">list-supported-models</h3>
<p>Display available models that can be deployed.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-20-1">emd<span class="w"> </span>list-supported-models<span class="w"> </span><span class="o">[</span>MODEL_ID<span class="o">]</span><span class="w"> </span><span class="o">[</span>OPTIONS<span class="o">]</span>
</span></code></pre></div>
<p><strong>Arguments:</strong></p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MODEL_ID</code></td>
<td>Optional model ID to filter results</td>
</tr>
</tbody>
</table>
<p><strong>Options:</strong></p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-a, --detail</code></td>
<td>Output model information in detail</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>List all supported models:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-21-1">emd<span class="w"> </span>list-supported-models
</span></code></pre></div></p>
<p>List detailed information for all models:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-22-1">emd<span class="w"> </span>list-supported-models<span class="w"> </span>--detail
</span></code></pre></div></p>
<p>List information for a specific model:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-23-1">emd<span class="w"> </span>list-supported-models<span class="w"> </span>Qwen2.5-7B-Instruct
</span></code></pre></div></p>
<h3 id="commands-profile">profile</h3>
<p>Configure AWS profile credentials for deployment.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-24-1">emd<span class="w"> </span>profile<span class="w"> </span>COMMAND<span class="w"> </span><span class="o">[</span>ARGS<span class="o">]</span>
</span></code></pre></div>
<p><strong>Commands:</strong></p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>set-default-profile-name [NAME]</code></td>
<td>Set the default profile name for deployment</td>
</tr>
<tr>
<td><code>show-default-profile-name</code></td>
<td>Show current default profile</td>
</tr>
<tr>
<td><code>remove-default-profile-name</code></td>
<td>Remove the default profile</td>
</tr>
</tbody>
</table>
<p><strong>Examples:</strong></p>
<p>Set a default AWS profile:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-25-1">emd<span class="w"> </span>profile<span class="w"> </span>set-default-profile-name<span class="w"> </span>my-profile
</span></code></pre></div></p>
<p>Show the current default profile:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-26-1">emd<span class="w"> </span>profile<span class="w"> </span>show-default-profile-name
</span></code></pre></div></p>
<p>Remove the default profile:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-27-1">emd<span class="w"> </span>profile<span class="w"> </span>remove-default-profile-name
</span></code></pre></div></p>
<h3 id="commands-version">version</h3>
<p>Display the current version of the EMD tool.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-28-1">emd<span class="w"> </span>version
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-29-1">emd<span class="w"> </span>version
</span></code></pre></div>
<h2 id="commands-environment-variables">Environment Variables</h2>
<ul>
<li><code>LOCAL_DEPLOY_PORT</code>: Local deployment port (default: <code>8080</code>)</li>
</ul>
<h2 id="commands-additional-resources">Additional Resources</h2>
<ul>
<li><a href="#installation">Installation Guide</a></li>
<li><a href="#best_deployment_practices">Best Deployment Practices</a></li>
<li><a href="#supported_models">Supported Models</a></li>
<li><a href="#langchain_interface">Langchain Interface</a></li>
</ul></section><section class="print-page" id="api"><h1 id="api-api-documentation">API Documentation</h1>
<blockquote>
<p><strong>Getting Started</strong>: To obtain the base URL and API key for your deployed models, run <code>emd status</code> in your terminal. The command will display a table with your deployed models and their details, including a link to retrieve the API key from AWS Secrets Manager. The base URL is shown at the bottom of the output.</p>
<p>Example output:
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1">Models
</span><span id="__span-0-2">┌────────────────────────┬───────────────────────────────────────────────────────────────────────┐
</span><span id="__span-0-3">│ Model ID               │ Qwen2.5-0.5B-Instruct/dev                                             │
</span><span id="__span-0-4">│ Status                 │ CREATE_COMPLETE                                                       │
</span><span id="__span-0-5">│ Service Type           │ Amazon SageMaker AI Real-time inference with OpenAI Compatible API    │
</span><span id="__span-0-6">│ Instance Type          │ ml.g5.2xlarge                                                         │
</span><span id="__span-0-7">│ Create Time            │ 2025-05-08 12:27:05 UTC                                               │
</span><span id="__span-0-8">│ Query Model API Key    │ https://console.aws.amazon.com/secretsmanager/secret?name=EMD-APIKey- │
</span><span id="__span-0-9">│                        │ Secrets&amp;region=us-east-1                                              │
</span><span id="__span-0-10">│ SageMakerEndpointName  │ EMD-Model-qwen2-5-0-5b-instruct-endpoint                              │
</span><span id="__span-0-11">└────────────────────────┴───────────────────────────────────────────────────────────────────────┘
</span><span id="__span-0-12">
</span><span id="__span-0-13">Base URL
</span><span id="__span-0-14">http://your-emd-endpoint.region.elb.amazonaws.com/v1
</span></code></pre></div></p>
</blockquote>
<h2 id="api-list-models">List Models</h2>
<p>Returns a list of available models.</p>
<p><strong>Endpoint:</strong> <code>GET /v1/models</code></p>
<p><strong>Curl Example:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-1-1">curl<span class="w"> </span>https://BASE_URL/v1/models
</span></code></pre></div></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-2-2">
</span><span id="__span-2-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-2-4">    <span class="c1"># No API key needed for listing models</span>
</span><span id="__span-2-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-2-6"><span class="p">)</span>
</span><span id="__span-2-7">
</span><span id="__span-2-8"><span class="c1"># List available models</span>
</span><span id="__span-2-9"><span class="n">models</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
</span><span id="__span-2-10"><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>
</span><span id="__span-2-11">    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="api-chat-completions">Chat Completions</h2>
<p>Create a model response for a conversation.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/chat/completions</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>model</code> (required): ID of the model to use (e.g., "Qwen2.5-7B-Instruct/dev", "Llama-3.3-70B-Instruct/dev")</li>
<li><code>messages</code> (required): Array of message objects with <code>role</code> and <code>content</code></li>
<li><code>temperature</code>: Sampling temperature (0-2, default: 1)</li>
<li><code>top_p</code>: Nucleus sampling parameter (0-1, default: 1)</li>
<li><code>n</code>: Number of chat completion choices to generate (default: 1)</li>
<li><code>stream</code>: Whether to stream partial progress (default: false)</li>
<li><code>stop</code>: Sequences where the API will stop generating</li>
<li><code>max_tokens</code>: Maximum number of tokens to generate</li>
<li><code>presence_penalty</code>: Penalty for new tokens based on presence (-2.0 to 2.0)</li>
<li><code>frequency_penalty</code>: Penalty for new tokens based on frequency (-2.0 to 2.0)</li>
<li><code>function_call</code>: Controls how the model responds to function calls</li>
<li><code>functions</code>: List of functions the model may generate JSON inputs for</li>
</ul>
<p><strong>Curl Example:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-3-1">curl<span class="w"> </span>https://BASE_URL/v1/chat/completions<span class="w"> </span><span class="se">\</span>
</span><span id="__span-3-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer YOUR_API_KEY&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-3-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-3-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-3-5"><span class="s1">    &quot;model&quot;: &quot;Qwen2.5-7B-Instruct/dev&quot;,</span>
</span><span id="__span-3-6"><span class="s1">    &quot;messages&quot;: [</span>
</span><span id="__span-3-7"><span class="s1">      {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
</span><span id="__span-3-8"><span class="s1">      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}</span>
</span><span id="__span-3-9"><span class="s1">    ],</span>
</span><span id="__span-3-10"><span class="s1">    &quot;temperature&quot;: 0.7</span>
</span><span id="__span-3-11"><span class="s1">  }&#39;</span>
</span></code></pre></div></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-4-2">
</span><span id="__span-4-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-4-4">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-4-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-4-6"><span class="p">)</span>
</span><span id="__span-4-7">
</span><span id="__span-4-8"><span class="c1"># Create a chat completion</span>
</span><span id="__span-4-9"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-4-10">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct/dev&quot;</span><span class="p">,</span>  <span class="c1"># Model ID with tag</span>
</span><span id="__span-4-11">    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-4-12">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
</span><span id="__span-4-13">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">}</span>
</span><span id="__span-4-14">    <span class="p">],</span>
</span><span id="__span-4-15">    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
</span><span id="__span-4-16">    <span class="n">stream</span><span class="o">=</span><span class="kc">False</span>
</span><span id="__span-4-17"><span class="p">)</span>
</span><span id="__span-4-18">
</span><span id="__span-4-19"><span class="c1"># Print the response</span>
</span><span id="__span-4-20"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Streaming Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-5-2">
</span><span id="__span-5-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-5-4">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-5-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-5-6"><span class="p">)</span>
</span><span id="__span-5-7">
</span><span id="__span-5-8"><span class="c1"># Create a streaming chat completion</span>
</span><span id="__span-5-9"><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-5-10">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-3.3-70B-Instruct/dev&quot;</span><span class="p">,</span>  <span class="c1"># Model ID with tag</span>
</span><span id="__span-5-11">    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-5-12">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
</span><span id="__span-5-13">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a short poem about AI.&quot;</span><span class="p">}</span>
</span><span id="__span-5-14">    <span class="p">],</span>
</span><span id="__span-5-15">    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-5-16"><span class="p">)</span>
</span><span id="__span-5-17">
</span><span id="__span-5-18"><span class="c1"># Process the stream</span>
</span><span id="__span-5-19"><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
</span><span id="__span-5-20">    <span class="k">if</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-5-21">        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-5-22"><span class="nb">print</span><span class="p">()</span>
</span></code></pre></div></p>
<h2 id="api-embeddings">Embeddings</h2>
<p>Get vector representations of text.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/embeddings</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>model</code> (required): ID of the model to use (e.g., "bge-m3/dev")</li>
<li><code>input</code> (required): Input text to embed or array of texts</li>
<li><code>user</code>: A unique identifier for the end-user</li>
</ul>
<p><strong>Curl Example:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-6-1">curl<span class="w"> </span>https://BASE_URL/v1/embeddings<span class="w"> </span><span class="se">\</span>
</span><span id="__span-6-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer YOUR_API_KEY&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-6-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-6-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-6-5"><span class="s1">    &quot;model&quot;: &quot;bge-m3/dev&quot;,</span>
</span><span id="__span-6-6"><span class="s1">    &quot;input&quot;: &quot;The food was delicious and the waiter...&quot;</span>
</span><span id="__span-6-7"><span class="s1">  }&#39;</span>
</span></code></pre></div></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-7-2">
</span><span id="__span-7-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-7-4">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-7-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-7-6"><span class="p">)</span>
</span><span id="__span-7-7">
</span><span id="__span-7-8"><span class="c1"># Get embeddings for a single text</span>
</span><span id="__span-7-9"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-7-10">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3/dev&quot;</span><span class="p">,</span>  <span class="c1"># Embedding model ID with tag</span>
</span><span id="__span-7-11">    <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;The food was delicious and the service was excellent.&quot;</span>
</span><span id="__span-7-12"><span class="p">)</span>
</span><span id="__span-7-13">
</span><span id="__span-7-14"><span class="c1"># Print the embedding vector</span>
</span><span id="__span-7-15"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</span><span id="__span-7-16">
</span><span id="__span-7-17"><span class="c1"># Get embeddings for multiple texts</span>
</span><span id="__span-7-18"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-7-19">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3/dev&quot;</span><span class="p">,</span>  <span class="c1"># Embedding model ID with tag</span>
</span><span id="__span-7-20">    <span class="nb">input</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-7-21">        <span class="s2">&quot;The food was delicious and the service was excellent.&quot;</span><span class="p">,</span>
</span><span id="__span-7-22">        <span class="s2">&quot;The restaurant was very expensive and the food was mediocre.&quot;</span>
</span><span id="__span-7-23">    <span class="p">]</span>
</span><span id="__span-7-24"><span class="p">)</span>
</span><span id="__span-7-25">
</span><span id="__span-7-26"><span class="c1"># Print the number of embeddings</span>
</span><span id="__span-7-27"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2"> embeddings&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="api-rerank">Rerank</h2>
<p>Rerank a list of documents based on their relevance to a query.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/rerank</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>model</code> (required): ID of the model to use (e.g., "bge-reranker-v2-m3/dev")</li>
<li><code>query</code> (required): The search query</li>
<li><code>documents</code> (required): List of documents to rerank</li>
<li><code>max_rerank</code>: Maximum number of documents to rerank (default: all)</li>
<li><code>return_metadata</code>: Whether to return metadata (default: false)</li>
</ul>
<p><strong>Curl Example:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-8-1">curl<span class="w"> </span>https://BASE_URL/v1/rerank<span class="w"> </span><span class="se">\</span>
</span><span id="__span-8-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer YOUR_API_KEY&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-8-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-8-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-8-5"><span class="s1">    &quot;model&quot;: &quot;bge-reranker-v2-m3/dev&quot;,</span>
</span><span id="__span-8-6"><span class="s1">    &quot;query&quot;: &quot;What is the capital of France?&quot;,</span>
</span><span id="__span-8-7"><span class="s1">    &quot;documents&quot;: [</span>
</span><span id="__span-8-8"><span class="s1">      &quot;Paris is the capital of France.&quot;,</span>
</span><span id="__span-8-9"><span class="s1">      &quot;Berlin is the capital of Germany.&quot;,</span>
</span><span id="__span-8-10"><span class="s1">      &quot;London is the capital of England.&quot;</span>
</span><span id="__span-8-11"><span class="s1">    ]</span>
</span><span id="__span-8-12"><span class="s1">  }&#39;</span>
</span></code></pre></div></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-9-2">
</span><span id="__span-9-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-9-4">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-9-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-9-6"><span class="p">)</span>
</span><span id="__span-9-7">
</span><span id="__span-9-8"><span class="c1"># Rerank documents based on a query</span>
</span><span id="__span-9-9"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">reranking</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-9-10">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-reranker-v2-m3/dev&quot;</span><span class="p">,</span>  <span class="c1"># Reranking model ID with tag</span>
</span><span id="__span-9-11">    <span class="n">query</span><span class="o">=</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span>
</span><span id="__span-9-12">    <span class="n">documents</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-9-13">        <span class="s2">&quot;Paris is the capital of France.&quot;</span><span class="p">,</span>
</span><span id="__span-9-14">        <span class="s2">&quot;Berlin is the capital of Germany.&quot;</span><span class="p">,</span>
</span><span id="__span-9-15">        <span class="s2">&quot;London is the capital of England.&quot;</span>
</span><span id="__span-9-16">    <span class="p">],</span>
</span><span id="__span-9-17">    <span class="n">max_rerank</span><span class="o">=</span><span class="mi">3</span>
</span><span id="__span-9-18"><span class="p">)</span>
</span><span id="__span-9-19">
</span><span id="__span-9-20"><span class="c1"># Print the reranked documents</span>
</span><span id="__span-9-21"><span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>
</span><span id="__span-9-22">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">document</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-9-23">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Relevance Score: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">relevance_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-9-24">    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="api-invocations">Invocations</h2>
<p>General-purpose endpoint for model invocations.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/invocations</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>model</code> (required): ID of the model to use</li>
<li><code>input</code>: Input data for the model</li>
<li><code>parameters</code>: Additional parameters for the model</li>
</ul>
<p><strong>Curl Example:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-10-1">curl<span class="w"> </span>https://BASE_URL/v1/invocations<span class="w"> </span><span class="se">\</span>
</span><span id="__span-10-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer YOUR_API_KEY&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-10-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-10-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-10-5"><span class="s1">    &quot;model&quot;: &quot;Qwen2.5-7B-Instruct/dev&quot;,</span>
</span><span id="__span-10-6"><span class="s1">    &quot;input&quot;: {</span>
</span><span id="__span-10-7"><span class="s1">      &quot;query&quot;: &quot;What is machine learning?&quot;</span>
</span><span id="__span-10-8"><span class="s1">    },</span>
</span><span id="__span-10-9"><span class="s1">    &quot;parameters&quot;: {</span>
</span><span id="__span-10-10"><span class="s1">      &quot;max_tokens&quot;: 100</span>
</span><span id="__span-10-11"><span class="s1">    }</span>
</span><span id="__span-10-12"><span class="s1">  }&#39;</span>
</span></code></pre></div></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
</span><span id="__span-11-2"><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="__span-11-3">
</span><span id="__span-11-4"><span class="c1"># Set up the API endpoint and headers</span>
</span><span id="__span-11-5"><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://BASE_URL/v1/invocations&quot;</span>
</span><span id="__span-11-6"><span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-11-7">    <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="s2">&quot;Bearer YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-11-8">    <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span>
</span><span id="__span-11-9"><span class="p">}</span>
</span><span id="__span-11-10">
</span><span id="__span-11-11"><span class="c1"># Prepare the payload</span>
</span><span id="__span-11-12"><span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-11-13">    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;Qwen2.5-7B-Instruct/dev&quot;</span><span class="p">,</span>  <span class="c1"># Model ID with tag</span>
</span><span id="__span-11-14">    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-11-15">        <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;What is machine learning?&quot;</span>
</span><span id="__span-11-16">    <span class="p">},</span>
</span><span id="__span-11-17">    <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-11-18">        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span>
</span><span id="__span-11-19">    <span class="p">}</span>
</span><span id="__span-11-20"><span class="p">}</span>
</span><span id="__span-11-21">
</span><span id="__span-11-22"><span class="c1"># Make the API call</span>
</span><span id="__span-11-23"><span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>
</span><span id="__span-11-24">
</span><span id="__span-11-25"><span class="c1"># Print the response</span>
</span><span id="__span-11-26"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</span></code></pre></div></p>
<h2 id="api-vision-models">Vision Models</h2>
<p>Process images along with text prompts.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/chat/completions</code></p>
<p><strong>Parameters:</strong>
Same as Chat Completions, but with messages that include image content.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-12-2"><span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
</span><span id="__span-12-3">
</span><span id="__span-12-4"><span class="c1"># Function to encode the image</span>
</span><span id="__span-12-5"><span class="k">def</span><span class="w"> </span><span class="nf">encode_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
</span><span id="__span-12-6">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">image_file</span><span class="p">:</span>
</span><span id="__span-12-7">        <span class="k">return</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">image_file</span><span class="o">.</span><span class="n">read</span><span class="p">())</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
</span><span id="__span-12-8">
</span><span id="__span-12-9"><span class="c1"># Path to your image</span>
</span><span id="__span-12-10"><span class="n">image_path</span> <span class="o">=</span> <span class="s2">&quot;path/to/your/image.jpg&quot;</span>
</span><span id="__span-12-11"><span class="n">base64_image</span> <span class="o">=</span> <span class="n">encode_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</span><span id="__span-12-12">
</span><span id="__span-12-13"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-12-14">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-12-15">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-12-16"><span class="p">)</span>
</span><span id="__span-12-17">
</span><span id="__span-12-18"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-12-19">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen2-VL-7B-Instruct/dev&quot;</span><span class="p">,</span>  <span class="c1"># Vision model ID with tag</span>
</span><span id="__span-12-20">    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span><span id="__span-12-21">        <span class="p">{</span>
</span><span id="__span-12-22">            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
</span><span id="__span-12-23">            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-12-24">                <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s in this image?&quot;</span><span class="p">},</span>
</span><span id="__span-12-25">                <span class="p">{</span>
</span><span id="__span-12-26">                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image_url&quot;</span><span class="p">,</span>
</span><span id="__span-12-27">                    <span class="s2">&quot;image_url&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-12-28">                        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data:image/jpeg;base64,</span><span class="si">{</span><span class="n">base64_image</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-12-29">                    <span class="p">}</span>
</span><span id="__span-12-30">                <span class="p">}</span>
</span><span id="__span-12-31">            <span class="p">]</span>
</span><span id="__span-12-32">        <span class="p">}</span>
</span><span id="__span-12-33">    <span class="p">]</span>
</span><span id="__span-12-34"><span class="p">)</span>
</span><span id="__span-12-35">
</span><span id="__span-12-36"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="api-audio-transcription">Audio Transcription</h2>
<p>Transcribe audio files to text.</p>
<p><strong>Endpoint:</strong> <code>POST /v1/audio/transcriptions</code></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-13-2">
</span><span id="__span-13-3"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-13-4">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">,</span>
</span><span id="__span-13-5">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://BASE_URL&quot;</span>
</span><span id="__span-13-6"><span class="p">)</span>
</span><span id="__span-13-7">
</span><span id="__span-13-8"><span class="n">audio_file_path</span> <span class="o">=</span> <span class="s2">&quot;path/to/audio.mp3&quot;</span>
</span><span id="__span-13-9"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audio_file_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
</span><span id="__span-13-10">    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-13-11">        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;whisper-large-v3/dev&quot;</span><span class="p">,</span>  <span class="c1"># ASR model ID with tag</span>
</span><span id="__span-13-12">        <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span>
</span><span id="__span-13-13">    <span class="p">)</span>
</span><span id="__span-13-14">
</span><span id="__span-13-15"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># Transcribed text</span>
</span></code></pre></div></p></section><section class="print-page" id="sdk_api"><h1 id="sdk_api-sdk-api-documentation">SDK API Documentation</h1>
<blockquote>
<p><strong>Getting Started</strong>: The EMD SDK provides a comprehensive Python interface for deploying, managing, and invoking machine learning models on AWS infrastructure. Install the SDK with <code>pip install easy-model-deployer</code> and import the modules you need.</p>
<p>Quick example:
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk</span><span class="w"> </span><span class="kn">import</span> <span class="n">bootstrap</span><span class="p">,</span> <span class="n">deploy</span><span class="p">,</span> <span class="n">destroy</span>
</span><span id="__span-0-2"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-0-3">
</span><span id="__span-0-4"><span class="c1"># Bootstrap infrastructure</span>
</span><span id="__span-0-5"><span class="n">bootstrap</span><span class="p">()</span>
</span><span id="__span-0-6">
</span><span id="__span-0-7"><span class="c1"># Deploy a model</span>
</span><span id="__span-0-8"><span class="n">result</span> <span class="o">=</span> <span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-0-9">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-0-10">    <span class="n">instance_type</span><span class="o">=</span><span class="s2">&quot;ml.g5.xlarge&quot;</span><span class="p">,</span>
</span><span id="__span-0-11">    <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
</span><span id="__span-0-12">    <span class="n">service_type</span><span class="o">=</span><span class="s2">&quot;sagemaker&quot;</span>
</span><span id="__span-0-13"><span class="p">)</span>
</span><span id="__span-0-14">
</span><span id="__span-0-15"><span class="c1"># Use the deployed model</span>
</span><span id="__span-0-16"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">)</span>
</span><span id="__span-0-17"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-0-18">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">}]</span>
</span><span id="__span-0-19"><span class="p">})</span>
</span></code></pre></div></p>
</blockquote>
<h2 id="sdk_api-bootstrap-infrastructure">Bootstrap Infrastructure</h2>
<p>Initialize AWS resources required for model deployment.</p>
<p><strong>Function:</strong> <code>bootstrap()</code></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.bootstrap</span><span class="w"> </span><span class="kn">import</span> <span class="n">bootstrap</span>
</span><span id="__span-1-2">
</span><span id="__span-1-3"><span class="c1"># Initialize AWS infrastructure</span>
</span><span id="__span-1-4"><span class="n">bootstrap</span><span class="p">()</span>
</span></code></pre></div></p>
<p><strong>Advanced Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.bootstrap</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_env_stack</span>
</span><span id="__span-2-2">
</span><span id="__span-2-3"><span class="c1"># Create environment stack with custom parameters</span>
</span><span id="__span-2-4"><span class="n">create_env_stack</span><span class="p">(</span>
</span><span id="__span-2-5">    <span class="n">region</span><span class="o">=</span><span class="s2">&quot;us-east-1&quot;</span><span class="p">,</span>
</span><span id="__span-2-6">    <span class="n">stack_name</span><span class="o">=</span><span class="s2">&quot;my-emd-env-stack&quot;</span><span class="p">,</span>
</span><span id="__span-2-7">    <span class="n">bucket_name</span><span class="o">=</span><span class="s2">&quot;my-emd-bucket&quot;</span><span class="p">,</span>
</span><span id="__span-2-8">    <span class="n">force_update</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-2-9"><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-deploy-models">Deploy Models</h2>
<p>Deploy machine learning models to AWS services.</p>
<p><strong>Function:</strong> <code>deploy(model_id, instance_type, engine_type, service_type, **kwargs)</code></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>model_id</code> (required): Model identifier (e.g., "Qwen2.5-7B-Instruct", "DeepSeek-R1-Distill-Llama-8B")</li>
<li><code>instance_type</code> (required): AWS instance type (e.g., "ml.g5.xlarge", "g5.2xlarge")</li>
<li><code>engine_type</code> (required): Inference engine ("vllm", "tgi", "huggingface")</li>
<li><code>service_type</code> (required): AWS service ("sagemaker", "ecs", "ec2")</li>
<li><code>framework_type</code>: API framework (default: "fastapi")</li>
<li><code>model_tag</code>: Model version tag (default: "dev")</li>
<li><code>waiting_until_deploy_complete</code>: Wait for completion (default: True)</li>
<li><code>extra_params</code>: Additional deployment parameters</li>
</ul>
<p><strong>Basic Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.deploy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deploy</span>
</span><span id="__span-3-2">
</span><span id="__span-3-3"><span class="c1"># Deploy a model to SageMaker</span>
</span><span id="__span-3-4"><span class="n">result</span> <span class="o">=</span> <span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-3-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-3-6">    <span class="n">instance_type</span><span class="o">=</span><span class="s2">&quot;ml.g5.xlarge&quot;</span><span class="p">,</span>
</span><span id="__span-3-7">    <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
</span><span id="__span-3-8">    <span class="n">service_type</span><span class="o">=</span><span class="s2">&quot;sagemaker&quot;</span>
</span><span id="__span-3-9"><span class="p">)</span>
</span><span id="__span-3-10">
</span><span id="__span-3-11"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deployment ID: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;pipeline_execution_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-3-12"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model Stack: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;model_stack_name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Advanced Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.deploy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deploy</span>
</span><span id="__span-4-2">
</span><span id="__span-4-3"><span class="c1"># Deploy with custom parameters</span>
</span><span id="__span-4-4"><span class="n">result</span> <span class="o">=</span> <span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-4-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;DeepSeek-R1-Distill-Llama-8B&quot;</span><span class="p">,</span>
</span><span id="__span-4-6">    <span class="n">instance_type</span><span class="o">=</span><span class="s2">&quot;ml.g5.2xlarge&quot;</span><span class="p">,</span>
</span><span id="__span-4-7">    <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
</span><span id="__span-4-8">    <span class="n">service_type</span><span class="o">=</span><span class="s2">&quot;sagemaker&quot;</span><span class="p">,</span>
</span><span id="__span-4-9">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;production&quot;</span><span class="p">,</span>
</span><span id="__span-4-10">    <span class="n">extra_params</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-4-11">        <span class="s2">&quot;engine_params&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-4-12">            <span class="s2">&quot;cli_args&quot;</span><span class="p">:</span> <span class="s2">&quot;--max_model_len 16000 --max_num_seqs 4&quot;</span>
</span><span id="__span-4-13">        <span class="p">},</span>
</span><span id="__span-4-14">        <span class="s2">&quot;service_params&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-4-15">            <span class="s2">&quot;api_key&quot;</span><span class="p">:</span> <span class="s2">&quot;your-secure-api-key&quot;</span>
</span><span id="__span-4-16">        <span class="p">}</span>
</span><span id="__span-4-17">    <span class="p">}</span>
</span><span id="__span-4-18"><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Local Deployment Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.deploy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deploy_local</span>
</span><span id="__span-5-2">
</span><span id="__span-5-3"><span class="c1"># Deploy locally for testing</span>
</span><span id="__span-5-4"><span class="n">deploy_local</span><span class="p">(</span>
</span><span id="__span-5-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-5-6">    <span class="n">instance_type</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span id="__span-5-7">    <span class="n">service_type</span><span class="o">=</span><span class="s2">&quot;local&quot;</span><span class="p">,</span>
</span><span id="__span-5-8">    <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
</span><span id="__span-5-9">    <span class="n">extra_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">}</span>
</span><span id="__span-5-10"><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-model-status">Model Status</h2>
<p>Check the deployment status of models.</p>
<p><strong>Function:</strong> <code>get_model_status(model_id, model_tag)</code></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.status</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_model_status</span>
</span><span id="__span-6-2">
</span><span id="__span-6-3"><span class="c1"># Check status of a specific model</span>
</span><span id="__span-6-4"><span class="n">status</span> <span class="o">=</span> <span class="n">get_model_status</span><span class="p">(</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">)</span>
</span><span id="__span-6-5">
</span><span id="__span-6-6"><span class="c1"># Check in-progress deployments</span>
</span><span id="__span-6-7"><span class="k">for</span> <span class="n">deployment</span> <span class="ow">in</span> <span class="n">status</span><span class="p">[</span><span class="s2">&quot;inprogress&quot;</span><span class="p">]:</span>
</span><span id="__span-6-8">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model: </span><span class="si">{</span><span class="n">deployment</span><span class="p">[</span><span class="s1">&#39;model_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-9">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Status: </span><span class="si">{</span><span class="n">deployment</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-10">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stage: </span><span class="si">{</span><span class="n">deployment</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stage_name&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;N/A&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-11">
</span><span id="__span-6-12"><span class="c1"># Check completed deployments</span>
</span><span id="__span-6-13"><span class="k">for</span> <span class="n">deployment</span> <span class="ow">in</span> <span class="n">status</span><span class="p">[</span><span class="s2">&quot;completed&quot;</span><span class="p">]:</span>
</span><span id="__span-6-14">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model: </span><span class="si">{</span><span class="n">deployment</span><span class="p">[</span><span class="s1">&#39;model_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-15">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Service: </span><span class="si">{</span><span class="n">deployment</span><span class="p">[</span><span class="s1">&#39;service_type&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-6-16">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Endpoint: </span><span class="si">{</span><span class="n">deployment</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;endpoint_name&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;N/A&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Pipeline Status Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.status</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_pipeline_execution_status</span>
</span><span id="__span-7-2">
</span><span id="__span-7-3"><span class="c1"># Check specific pipeline execution</span>
</span><span id="__span-7-4"><span class="n">status</span> <span class="o">=</span> <span class="n">get_pipeline_execution_status</span><span class="p">(</span>
</span><span id="__span-7-5">    <span class="n">pipeline_execution_id</span><span class="o">=</span><span class="s2">&quot;execution-123&quot;</span><span class="p">,</span>
</span><span id="__span-7-6">    <span class="n">region</span><span class="o">=</span><span class="s2">&quot;us-east-1&quot;</span>
</span><span id="__span-7-7"><span class="p">)</span>
</span><span id="__span-7-8">
</span><span id="__span-7-9"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Status: </span><span class="si">{</span><span class="n">status</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-7-10"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Succeeded: </span><span class="si">{</span><span class="n">status</span><span class="p">[</span><span class="s1">&#39;is_succeeded&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-sagemaker-client">SageMaker Client</h2>
<p>Interact with models deployed on Amazon SageMaker.</p>
<p><strong>Initialization:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-8-2">
</span><span id="__span-8-3"><span class="c1"># Initialize with model ID</span>
</span><span id="__span-8-4"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-8-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-8-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span>
</span><span id="__span-8-7">    <span class="n">region_name</span><span class="o">=</span><span class="s2">&quot;us-east-1&quot;</span>
</span><span id="__span-8-8"><span class="p">)</span>
</span><span id="__span-8-9">
</span><span id="__span-8-10"><span class="c1"># Or initialize with endpoint name directly</span>
</span><span id="__span-8-11"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-8-12">    <span class="n">endpoint_name</span><span class="o">=</span><span class="s2">&quot;my-sagemaker-endpoint&quot;</span><span class="p">,</span>
</span><span id="__span-8-13">    <span class="n">region_name</span><span class="o">=</span><span class="s2">&quot;us-east-1&quot;</span>
</span><span id="__span-8-14"><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Synchronous Invocation:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><span class="c1"># Basic chat completion</span>
</span><span id="__span-9-2"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-9-3">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-9-4">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
</span><span id="__span-9-5">        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is machine learning?&quot;</span><span class="p">}</span>
</span><span id="__span-9-6">    <span class="p">],</span>
</span><span id="__span-9-7">    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
</span><span id="__span-9-8">    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span>
</span><span id="__span-9-9"><span class="p">})</span>
</span><span id="__span-9-10">
</span><span id="__span-9-11"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>
</span></code></pre></div></p>
<p><strong>Streaming Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><span class="c1"># Stream response tokens</span>
</span><span id="__span-10-2"><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-10-3">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a story&quot;</span><span class="p">}],</span>
</span><span id="__span-10-4">    <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-10-5">    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">500</span>
</span><span id="__span-10-6"><span class="p">}):</span>
</span><span id="__span-10-7">    <span class="k">if</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;choices&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;delta&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">):</span>
</span><span id="__span-10-8">        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;delta&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Asynchronous Invocation:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><span class="c1"># For long-running tasks</span>
</span><span id="__span-11-2"><span class="n">async_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke_async</span><span class="p">({</span>
</span><span id="__span-11-3">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a detailed analysis&quot;</span><span class="p">}],</span>
</span><span id="__span-11-4">    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">2000</span>
</span><span id="__span-11-5"><span class="p">})</span>
</span><span id="__span-11-6">
</span><span id="__span-11-7"><span class="c1"># Wait for result</span>
</span><span id="__span-11-8"><span class="n">result</span> <span class="o">=</span> <span class="n">async_response</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>
</span><span id="__span-11-9"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-ecs-client">ECS Client</h2>
<p>Interact with models deployed on Amazon ECS.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">ECSClient</span>
</span><span id="__span-12-2">
</span><span id="__span-12-3"><span class="c1"># Initialize client</span>
</span><span id="__span-12-4"><span class="n">client</span> <span class="o">=</span> <span class="n">ECSClient</span><span class="p">(</span>
</span><span id="__span-12-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-12-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span>
</span><span id="__span-12-7"><span class="p">)</span>
</span><span id="__span-12-8">
</span><span id="__span-12-9"><span class="c1"># Invoke model</span>
</span><span id="__span-12-10"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-12-11">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">}],</span>
</span><span id="__span-12-12">    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
</span><span id="__span-12-13">    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.8</span>
</span><span id="__span-12-14"><span class="p">})</span>
</span><span id="__span-12-15">
</span><span id="__span-12-16"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>
</span></code></pre></div></p>
<p><strong>Streaming Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><span class="c1"># Stream response from ECS deployment</span>
</span><span id="__span-13-2"><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-13-3">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain quantum physics&quot;</span><span class="p">}],</span>
</span><span id="__span-13-4">    <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">True</span>
</span><span id="__span-13-5"><span class="p">}):</span>
</span><span id="__span-13-6">    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-conversation-interface">Conversation Interface</h2>
<p>High-level interface for conversational AI interactions.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.invoke</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConversationInvoker</span>
</span><span id="__span-14-2">
</span><span id="__span-14-3"><span class="c1"># Initialize conversation</span>
</span><span id="__span-14-4"><span class="n">conversation</span> <span class="o">=</span> <span class="n">ConversationInvoker</span><span class="p">(</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">)</span>
</span><span id="__span-14-5">
</span><span id="__span-14-6"><span class="c1"># Set system message</span>
</span><span id="__span-14-7"><span class="n">conversation</span><span class="o">.</span><span class="n">add_system_message</span><span class="p">(</span><span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">)</span>
</span><span id="__span-14-8">
</span><span id="__span-14-9"><span class="c1"># Add user message and get response</span>
</span><span id="__span-14-10"><span class="n">conversation</span><span class="o">.</span><span class="n">add_user_message</span><span class="p">(</span><span class="s2">&quot;What is artificial intelligence?&quot;</span><span class="p">)</span>
</span><span id="__span-14-11"><span class="n">response</span> <span class="o">=</span> <span class="n">conversation</span><span class="o">.</span><span class="n">invoke</span><span class="p">()</span>
</span><span id="__span-14-12"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span id="__span-14-13">
</span><span id="__span-14-14"><span class="c1"># Continue conversation</span>
</span><span id="__span-14-15"><span class="n">conversation</span><span class="o">.</span><span class="n">add_assistant_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span id="__span-14-16"><span class="n">conversation</span><span class="o">.</span><span class="n">add_user_message</span><span class="p">(</span><span class="s2">&quot;Can you give me examples?&quot;</span><span class="p">)</span>
</span><span id="__span-14-17"><span class="n">response</span> <span class="o">=</span> <span class="n">conversation</span><span class="o">.</span><span class="n">invoke</span><span class="p">()</span>
</span><span id="__span-14-18"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Streaming Conversation:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><span class="c1"># Stream conversation response</span>
</span><span id="__span-15-2"><span class="n">conversation</span><span class="o">.</span><span class="n">add_user_message</span><span class="p">(</span><span class="s2">&quot;Tell me about the future of AI&quot;</span><span class="p">)</span>
</span><span id="__span-15-3"><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">conversation</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="__span-15-4">    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-destroy-deployments">Destroy Deployments</h2>
<p>Remove deployed models and clean up resources.</p>
<p><strong>Function:</strong> <code>destroy(model_id, model_tag, waiting_until_complete)</code></p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.destroy</span><span class="w"> </span><span class="kn">import</span> <span class="n">destroy</span>
</span><span id="__span-16-2">
</span><span id="__span-16-3"><span class="c1"># Destroy a deployed model</span>
</span><span id="__span-16-4"><span class="n">destroy</span><span class="p">(</span>
</span><span id="__span-16-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-16-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span>
</span><span id="__span-16-7">    <span class="n">waiting_until_complete</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-16-8"><span class="p">)</span>
</span></code></pre></div></p>
<p><strong>Stop Pipeline Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.destroy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stop_pipeline_execution</span>
</span><span id="__span-17-2">
</span><span id="__span-17-3"><span class="c1"># Stop an active deployment pipeline</span>
</span><span id="__span-17-4"><span class="n">stop_pipeline_execution</span><span class="p">(</span>
</span><span id="__span-17-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-17-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span><span class="p">,</span>
</span><span id="__span-17-7">    <span class="n">waiting_until_complete</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-17-8"><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-embedding-models">Embedding Models</h2>
<p>Work with text embedding models.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-18-2">
</span><span id="__span-18-3"><span class="c1"># Initialize embedding model client</span>
</span><span id="__span-18-4"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-18-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">,</span>
</span><span id="__span-18-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span>
</span><span id="__span-18-7"><span class="p">)</span>
</span><span id="__span-18-8">
</span><span id="__span-18-9"><span class="c1"># Get embeddings for single text</span>
</span><span id="__span-18-10"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-18-11">    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;Machine learning is transforming technology&quot;</span><span class="p">,</span>
</span><span id="__span-18-12">    <span class="s2">&quot;normalize&quot;</span><span class="p">:</span> <span class="kc">True</span>
</span><span id="__span-18-13"><span class="p">})</span>
</span><span id="__span-18-14">
</span><span id="__span-18-15"><span class="n">embedding</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span>
</span><span id="__span-18-16"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding dimension: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-18-17">
</span><span id="__span-18-18"><span class="c1"># Get embeddings for multiple texts</span>
</span><span id="__span-18-19"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-18-20">    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-18-21">        <span class="s2">&quot;First document text&quot;</span><span class="p">,</span>
</span><span id="__span-18-22">        <span class="s2">&quot;Second document text&quot;</span><span class="p">,</span>
</span><span id="__span-18-23">        <span class="s2">&quot;Third document text&quot;</span>
</span><span id="__span-18-24">    <span class="p">]</span>
</span><span id="__span-18-25"><span class="p">})</span>
</span><span id="__span-18-26">
</span><span id="__span-18-27"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2"> embeddings&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-reranking-models">Reranking Models</h2>
<p>Rerank documents based on relevance to a query.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-19-2">
</span><span id="__span-19-3"><span class="c1"># Initialize reranking model client</span>
</span><span id="__span-19-4"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-19-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;bge-reranker-v2-m3&quot;</span><span class="p">,</span>
</span><span id="__span-19-6">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span>
</span><span id="__span-19-7"><span class="p">)</span>
</span><span id="__span-19-8">
</span><span id="__span-19-9"><span class="c1"># Rerank documents</span>
</span><span id="__span-19-10"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-19-11">    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
</span><span id="__span-19-12">    <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-19-13">        <span class="s2">&quot;Machine learning is a subset of artificial intelligence.&quot;</span><span class="p">,</span>
</span><span id="__span-19-14">        <span class="s2">&quot;Paris is the capital of France.&quot;</span><span class="p">,</span>
</span><span id="__span-19-15">        <span class="s2">&quot;Deep learning uses neural networks.&quot;</span>
</span><span id="__span-19-16">    <span class="p">],</span>
</span><span id="__span-19-17">    <span class="s2">&quot;max_rerank&quot;</span><span class="p">:</span> <span class="mi">3</span>
</span><span id="__span-19-18"><span class="p">})</span>
</span><span id="__span-19-19">
</span><span id="__span-19-20"><span class="c1"># Print ranked results</span>
</span><span id="__span-19-21"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]):</span>
</span><span id="__span-19-22">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-19-23">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;relevance_score&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-19-24">    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-vision-models">Vision Models</h2>
<p>Process images with vision-language models.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-20-2"><span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
</span><span id="__span-20-3">
</span><span id="__span-20-4"><span class="c1"># Function to encode image</span>
</span><span id="__span-20-5"><span class="k">def</span><span class="w"> </span><span class="nf">encode_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
</span><span id="__span-20-6">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">image_file</span><span class="p">:</span>
</span><span id="__span-20-7">        <span class="k">return</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">image_file</span><span class="o">.</span><span class="n">read</span><span class="p">())</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
</span><span id="__span-20-8">
</span><span id="__span-20-9"><span class="c1"># Initialize vision model client</span>
</span><span id="__span-20-10"><span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-20-11">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2-VL-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-20-12">    <span class="n">model_tag</span><span class="o">=</span><span class="s2">&quot;dev&quot;</span>
</span><span id="__span-20-13"><span class="p">)</span>
</span><span id="__span-20-14">
</span><span id="__span-20-15"><span class="c1"># Process image with text</span>
</span><span id="__span-20-16"><span class="n">base64_image</span> <span class="o">=</span> <span class="n">encode_image</span><span class="p">(</span><span class="s2">&quot;path/to/image.jpg&quot;</span><span class="p">)</span>
</span><span id="__span-20-17"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-20-18">    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-20-19">        <span class="p">{</span>
</span><span id="__span-20-20">            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
</span><span id="__span-20-21">            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span id="__span-20-22">                <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s in this image?&quot;</span><span class="p">},</span>
</span><span id="__span-20-23">                <span class="p">{</span>
</span><span id="__span-20-24">                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image_url&quot;</span><span class="p">,</span>
</span><span id="__span-20-25">                    <span class="s2">&quot;image_url&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span id="__span-20-26">                        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data:image/jpeg;base64,</span><span class="si">{</span><span class="n">base64_image</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-20-27">                    <span class="p">}</span>
</span><span id="__span-20-28">                <span class="p">}</span>
</span><span id="__span-20-29">            <span class="p">]</span>
</span><span id="__span-20-30">        <span class="p">}</span>
</span><span id="__span-20-31">    <span class="p">]</span>
</span><span id="__span-20-32"><span class="p">})</span>
</span><span id="__span-20-33">
</span><span id="__span-20-34"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>
</span></code></pre></div></p>
<h2 id="sdk_api-aws-lambda-integration">AWS Lambda Integration</h2>
<p>Use the SDK in AWS Lambda functions.</p>
<p><strong>Lambda Function Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="__span-21-2"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-21-3">
</span><span id="__span-21-4"><span class="k">def</span><span class="w"> </span><span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
</span><span id="__span-21-5">    <span class="c1"># Initialize client</span>
</span><span id="__span-21-6">    <span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span>
</span><span id="__span-21-7">        <span class="n">model_id</span><span class="o">=</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;model_id&#39;</span><span class="p">],</span>
</span><span id="__span-21-8">        <span class="n">region_name</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">invoked_function_arn</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span>
</span><span id="__span-21-9">    <span class="p">)</span>
</span><span id="__span-21-10">
</span><span id="__span-21-11">    <span class="c1"># Invoke model</span>
</span><span id="__span-21-12">    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-21-13">        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">],</span>
</span><span id="__span-21-14">        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">event</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_tokens&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span><span id="__span-21-15">    <span class="p">})</span>
</span><span id="__span-21-16">
</span><span id="__span-21-17">    <span class="k">return</span> <span class="p">{</span>
</span><span id="__span-21-18">        <span class="s1">&#39;statusCode&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
</span><span id="__span-21-19">        <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span id="__span-21-20">    <span class="p">}</span>
</span></code></pre></div></p>
<p><strong>Model Management Lambda:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="__span-22-2"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk</span><span class="w"> </span><span class="kn">import</span> <span class="n">deploy</span><span class="p">,</span> <span class="n">destroy</span><span class="p">,</span> <span class="n">get_model_status</span>
</span><span id="__span-22-3">
</span><span id="__span-22-4"><span class="k">def</span><span class="w"> </span><span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
</span><span id="__span-22-5">    <span class="n">action</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span>
</span><span id="__span-22-6">    <span class="n">model_id</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">&#39;model_id&#39;</span><span class="p">]</span>
</span><span id="__span-22-7">
</span><span id="__span-22-8">    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;deploy&#39;</span><span class="p">:</span>
</span><span id="__span-22-9">        <span class="n">result</span> <span class="o">=</span> <span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-22-10">            <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
</span><span id="__span-22-11">            <span class="n">instance_type</span><span class="o">=</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;instance_type&#39;</span><span class="p">],</span>
</span><span id="__span-22-12">            <span class="n">engine_type</span><span class="o">=</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;engine_type&#39;</span><span class="p">],</span>
</span><span id="__span-22-13">            <span class="n">service_type</span><span class="o">=</span><span class="n">event</span><span class="p">[</span><span class="s1">&#39;service_type&#39;</span><span class="p">],</span>
</span><span id="__span-22-14">            <span class="n">waiting_until_deploy_complete</span><span class="o">=</span><span class="kc">False</span>
</span><span id="__span-22-15">        <span class="p">)</span>
</span><span id="__span-22-16">        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;statusCode&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="p">)}</span>
</span><span id="__span-22-17">
</span><span id="__span-22-18">    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;destroy&#39;</span><span class="p">:</span>
</span><span id="__span-22-19">        <span class="n">destroy</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">waiting_until_complete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-22-20">        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;statusCode&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s1">&#39;status&#39;</span><span class="p">:</span> <span class="s1">&#39;initiated&#39;</span><span class="p">})}</span>
</span><span id="__span-22-21">
</span><span id="__span-22-22">    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;status&#39;</span><span class="p">:</span>
</span><span id="__span-22-23">        <span class="n">status</span> <span class="o">=</span> <span class="n">get_model_status</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span><span id="__span-22-24">        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;statusCode&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span> <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">status</span><span class="p">)}</span>
</span></code></pre></div></p>
<h2 id="sdk_api-error-handling">Error Handling</h2>
<p>Handle common errors when using the SDK.</p>
<p><strong>Python Example:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk</span><span class="w"> </span><span class="kn">import</span> <span class="n">deploy</span>
</span><span id="__span-23-2"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.sdk.clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerClient</span>
</span><span id="__span-23-3"><span class="kn">from</span><span class="w"> </span><span class="nn">botocore.exceptions</span><span class="w"> </span><span class="kn">import</span> <span class="n">ClientError</span>
</span><span id="__span-23-4">
</span><span id="__span-23-5"><span class="k">try</span><span class="p">:</span>
</span><span id="__span-23-6">    <span class="c1"># Deploy model</span>
</span><span id="__span-23-7">    <span class="n">result</span> <span class="o">=</span> <span class="n">deploy</span><span class="p">(</span>
</span><span id="__span-23-8">        <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-23-9">        <span class="n">instance_type</span><span class="o">=</span><span class="s2">&quot;ml.g5.xlarge&quot;</span><span class="p">,</span>
</span><span id="__span-23-10">        <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
</span><span id="__span-23-11">        <span class="n">service_type</span><span class="o">=</span><span class="s2">&quot;sagemaker&quot;</span>
</span><span id="__span-23-12">    <span class="p">)</span>
</span><span id="__span-23-13">
</span><span id="__span-23-14">    <span class="c1"># Initialize client</span>
</span><span id="__span-23-15">    <span class="n">client</span> <span class="o">=</span> <span class="n">SageMakerClient</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">)</span>
</span><span id="__span-23-16">
</span><span id="__span-23-17">    <span class="c1"># Invoke model</span>
</span><span id="__span-23-18">    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
</span><span id="__span-23-19">        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello&quot;</span><span class="p">}]</span>
</span><span id="__span-23-20">    <span class="p">})</span>
</span><span id="__span-23-21">
</span><span id="__span-23-22"><span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-23-23">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deployment error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-23-24"><span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-23-25">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Configuration error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-23-26"><span class="k">except</span> <span class="n">ClientError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-23-27">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AWS error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-23-28"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-23-29">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<h2 id="sdk_api-complete-workflow-example">Complete Workflow Example</h2>
<p>End-to-end example of deploying and using a model.</p>
<p><strong>Python Example:</strong>
```python
from emd.sdk import bootstrap, deploy, get_model_status, destroy
from emd.sdk.clients import SageMakerClient
import time</p>
<h1 id="sdk_api-1-bootstrap-infrastructure">1. Bootstrap infrastructure</h1>
<p>print("Setting up AWS infrastructure...")
bootstrap()</p>
<h1 id="sdk_api-2-deploy-model">2. Deploy model</h1>
<p>print("Deploying model...")
deployment = deploy(
    model_id="Qwen2.5-7B-Instruct",
    instance_type="ml.g5.xlarge",
    engine_type="vllm",
    service_type="sagemaker",
    extra_params={
        "engine_params": {
            "cli_args": "--max_model_len 8000 --max_num_seqs 10"
        }
    }
)</p>
<p>print(f"Deployment started: {deployment['pipeline_execution_id']}")</p>
<h1 id="sdk_api-3-wait-for-deployment-to-complete">3. Wait for deployment to complete</h1>
<p>print("Waiting for deployment...")
while True:
    status = get_model_status("Qwen2.5-7B-Instruct")
    if status["completed"]:
        print("Deployment completed!")
        break
    elif status["inprogress"]:
        print("Still deploying...")
        time.sleep(30)
    else:
        print("No deployment found")
        break</p>
<h1 id="sdk_api-4-use-the-deployed-model">4. Use the deployed model</h1>
<p>client = SageMakerClient(model_id="Qwen2.5-7B-Instruct")</p>
<h1 id="sdk_api-test-basic-functionality">Test basic functionality</h1>
<p>response = client.invoke({
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain machine learning in simple terms."}
    ],
    "max_tokens": 200,
    "temperature": 0.7
})</p>
<p>print("Model response:")
print(response["choices"][0]["message"]["content"])</p>
<h1 id="sdk_api-test-streaming">Test streaming</h1>
<p>print("\nStreaming response:")
for chunk in client.invoke({
    "messages": [{"role": "user", "content": "Count from 1 to 10"}],
    "stream": True
}):
    if chunk.get("choices") and chunk["choices"][0].get("delta", {}).get("content"):
        print(chunk["choices"][0]["delta"]["content"], end="")</p>
<h1 id="sdk_api-5-clean-up-optional">5. Clean up (optional)</h1>
<h1 id="sdk_api-destroyqwen25-7b-instruct">destroy("Qwen2.5-7B-Instruct")</h1></section><section class="print-page" id="local_deployment"><h1 id="local_deployment-local-model-deployment-guide">Local Model Deployment Guide</h1>
<p>This guide provides detailed instructions for deploying models locally using Easy Model Deployer (EMD).</p>
<h2 id="local_deployment-local-deployment-on-ec2-instance">Local Deployment on EC2 Instance</h2>
<p>For deploying models using local GPU resources:</p>
<h3 id="local_deployment-prerequisites">Prerequisites</h3>
<p>It is recommended to launch an EC2 instance using the AMI "<strong>Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.6 (Ubuntu 22.04)</strong>".</p>
<h3 id="local_deployment-deployment-command">Deployment Command</h3>
<p>Deploy with:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1">emd<span class="w"> </span>deploy<span class="w"> </span>--allow-local-deploy
</span></code></pre></div>
<h2 id="local_deployment-command-line-deployment">Command Line Deployment</h2>
<p>You can deploy models directly with command line parameters:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-1-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-series<span class="w"> </span>llama<span class="w"> </span>--model-name<span class="w"> </span>llama-3.3-70b-instruct-awq<span class="w"> </span>--service<span class="w"> </span>Local<span class="w"> </span>--gpu-ids<span class="w"> </span><span class="m">0</span>,1,2,3
</span></code></pre></div>
<h2 id="local_deployment-additional-parameters">Additional Parameters</h2>
<p>You can provide additional parameters as a JSON string:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-2-1">emd<span class="w"> </span>deploy<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{&quot;engine_params&quot;:{&quot;api_key&quot;:&quot;YOUR_API_KEY&quot;, &quot;default_cli_args&quot;: &quot;--max-total-tokens 30000 --max-concurrent-requests 30&quot;}}&#39;</span>
</span></code></pre></div>
<p>Or from a file:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-3-1">emd<span class="w"> </span>deploy<span class="w"> </span>--extra-params<span class="w"> </span>path/to/params.json
</span></code></pre></div>
<p>The extra parameters format:</p>
<div class="language-json highlight"><pre><span></span><code><span id="__span-4-1"><span class="p">{</span>
</span><span id="__span-4-2"><span class="w">  </span><span class="nt">&quot;model_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-4-3"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-4-4"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:{</span>
</span><span id="__span-4-5"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-4-6"><span class="w">  </span><span class="nt">&quot;instance_params&quot;</span><span class="p">:{</span>
</span><span id="__span-4-7"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-4-8"><span class="w">  </span><span class="nt">&quot;engine_params&quot;</span><span class="p">:{</span>
</span><span id="__span-4-9"><span class="w">      </span><span class="nt">&quot;cli_args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;command line arguments of current engine&gt;&quot;</span>
</span><span id="__span-4-10"><span class="w">  </span><span class="p">},</span>
</span><span id="__span-4-11"><span class="w">  </span><span class="nt">&quot;framework_params&quot;</span><span class="p">:{</span>
</span><span id="__span-4-12"><span class="w">      </span><span class="nt">&quot;uvicorn_log_level&quot;</span><span class="p">:</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
</span><span id="__span-4-13"><span class="w">      </span><span class="nt">&quot;limit_concurrency&quot;</span><span class="p">:</span><span class="mi">200</span>
</span><span id="__span-4-14"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-4-15"><span class="p">}</span>
</span></code></pre></div>
<h2 id="local_deployment-common-model-configurations">Common Model Configurations</h2>
<h3 id="local_deployment-non-reasoning-models">Non-reasoning Models</h3>
<h4 id="local_deployment-qwen25-72b-instruct-awq">Qwen2.5-72B-Instruct-AWQ</h4>
<div class="language-text highlight"><pre><span></span><code><span id="__span-5-1">? Select the model series: qwen2.5
</span><span id="__span-5-2">? Select the model name: Qwen2.5-72B-Instruct-AWQ
</span><span id="__span-5-3">? Select the service for deployment: Local
</span><span id="__span-5-4">? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3
</span><span id="__span-5-5">? Select the inference engine to use: tgi
</span><span id="__span-5-6">? (Optional) Additional deployment parameters: {&quot;engine_params&quot;:{&quot;api_key&quot;:&quot;&lt;YOUR_API_KEY&gt;&quot;, &quot;default_cli_args&quot;: &quot;--max-total-tokens 30000 --max-concurrent-requests 30&quot;}}
</span></code></pre></div>
<h4 id="local_deployment-llama-33-70b-instruct-awq">llama-3.3-70b-instruct-awq</h4>
<div class="language-text highlight"><pre><span></span><code><span id="__span-6-1">? Select the model series: llama
</span><span id="__span-6-2">? Select the model name: llama-3.3-70b-instruct-awq
</span><span id="__span-6-3">? Select the service for deployment: Local
</span><span id="__span-6-4">? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3
</span><span id="__span-6-5">? Select the inference engine to use: tgi
</span><span id="__span-6-6">? (Optional) Additional deployment parameters: {&quot;engine_params&quot;:{&quot;api_key&quot;:&quot;&lt;YOUR_API_KEY&gt;&quot;, &quot;default_cli_args&quot;: &quot;--max-total-tokens 30000 --max-concurrent-requests 30&quot;}}
</span></code></pre></div>
<h3 id="local_deployment-reasoning-models">Reasoning Models</h3>
<h4 id="local_deployment-deepseek-r1-distill-qwen-32b">DeepSeek-R1-Distill-Qwen-32B</h4>
<div class="language-text highlight"><pre><span></span><code><span id="__span-7-1">? Select the model series: deepseek reasoning model
</span><span id="__span-7-2">? Select the model name: DeepSeek-R1-Distill-Qwen-32B
</span><span id="__span-7-3">? Select the service for deployment: Local
</span><span id="__span-7-4">? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3
</span><span id="__span-7-5">? Select the inference engine to use: vllm
</span><span id="__span-7-6">? (Optional) Additional deployment parameters: {&quot;engine_params&quot;:{&quot;api_key&quot;:&quot;&lt;YOUR_API_KEY&gt;&quot;, &quot;default_cli_args&quot;: &quot;--enable-reasoning --reasoning-parser deepseek_r1 --max_model_len 16000 --disable-log-stats --chat-template emd/models/chat_templates/deepseek_r1_distill.jinja --max_num_seq 20 --gpu_memory_utilization 0.9&quot;}}
</span></code></pre></div>
<h4 id="local_deployment-deepseek-r1-distill-llama-70b-awq">deepseek-r1-distill-llama-70b-awq</h4>
<div class="language-text highlight"><pre><span></span><code><span id="__span-8-1">? Select the model series: deepseek reasoning model
</span><span id="__span-8-2">? Select the model name: deepseek-r1-distill-llama-70b-awq
</span><span id="__span-8-3">? Select the service for deployment: Local
</span><span id="__span-8-4">? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3
</span><span id="__span-8-5">? Select the inference engine to use: tgi
</span><span id="__span-8-6">? (Optional) Additional deployment parameters: {&quot;engine_params&quot;:{&quot;api_key&quot;:&quot;&lt;YOUR_API_KEY&gt;&quot;, &quot;default_cli_args&quot;: &quot;--max-total-tokens 30000 --max-concurrent-requests 30&quot;}}
</span></code></pre></div>
<h2 id="local_deployment-tips-for-local-deployment">Tips for Local Deployment</h2>
<ul>
<li>When you see "Waiting for model: ...", it means the deployment task has started. You can press <code>Ctrl+C</code> to stop the terminal output without affecting the deployment.</li>
<li>For multi-GPU deployments, ensure all specified GPUs are available and have sufficient memory.</li>
<li>Monitor GPU usage with tools like <code>nvidia-smi</code> during deployment and inference.</li>
<li>For optimal performance, consider the recommended GPU memory requirements for each model.</li>
</ul>
<h2 id="local_deployment-advanced-options">Advanced Options</h2>
<p>For more detailed information on:
- Advanced deployment parameters: See <a href="https://aws-samples.github.io/easy-model-deployer/en/best_deployment_practices/">Best Deployment Practices</a>
- Architecture details: See <a href="https://aws-samples.github.io/easy-model-deployer/en/architecture/">Architecture</a>
- Supported models: See <a href="https://aws-samples.github.io/easy-model-deployer/en/supported_models/">Supported Models</a></p></section><section class="print-page" id="architecture"><h1 id="architecture-architecture">Architecture</h1>
<p>EMD deploys models to AWS using a simple three-step process:</p>
<p><img alt="EMD Architecture Diagram" src="../emd-architecture.png" /></p>
<ol>
<li>
<p>User/Client initiates model deployment task, triggering pipeline to start model building.</p>
</li>
<li>
<p>AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR.</p>
</li>
<li>
<p>AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).</p>
</li>
</ol>
<h2 id="architecture-key-aws-services-used">Key AWS Services Used</h2>
<ul>
<li><strong>CodePipeline</strong>: Orchestrates the entire workflow</li>
<li><strong>CodeBuild</strong>: Builds model containers</li>
<li><strong>CloudFormation</strong>: Provisions infrastructure</li>
<li><strong>ECR</strong>: Stores model containers</li>
<li><strong>Target Services</strong>: SageMaker, EC2, or ECS hosts your model</li>
</ul>
<p>EMD handles all IAM permissions and security configurations automatically.</p>
<h2 id="architecture-model-deployment-cost-estimation">Model Deployment Cost Estimation</h2>
<p>EMD leverages several AWS services to deploy models. Below is an estimated cost breakdown for deploying a single model (assuming a 5GB model file and 10-minute CodeBuild execution).</p>
<h3 id="architecture-us-east-n-virginia-region-cost-estimation">US East (N. Virginia) Region Cost Estimation</h3>
<table>
<thead>
<tr>
<th>Service</th>
<th>Usage</th>
<th>Estimated Cost (USD)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>S3 Storage</strong></td>
<td>5GB model file</td>
<td>$0.00/month</td>
<td>$0.023 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months</td>
</tr>
<tr>
<td><strong>CodeBuild</strong></td>
<td>BUILD_GENERAL1_LARGE for 10 minutes</td>
<td>$0.10</td>
<td>$0.005 per build-minute</td>
</tr>
<tr>
<td><strong>CodePipeline</strong></td>
<td>1 pipeline execution</td>
<td>$0.00</td>
<td>First pipeline is free, then $1.00 per active pipeline/month</td>
</tr>
<tr>
<td><strong>CloudFormation</strong></td>
<td>Stack creation</td>
<td>$0.00</td>
<td>No charge for CloudFormation service</td>
</tr>
<tr>
<td><strong>ECR</strong></td>
<td>~2GB Docker image</td>
<td>$0.10/month</td>
<td>$0.10 per GB-month for private repository storage</td>
</tr>
<tr>
<td><strong>Total Deployment Cost</strong></td>
<td></td>
<td><strong>$0.10</strong> + $0.10/month</td>
<td>One-time deployment cost + monthly storage</td>
</tr>
</tbody>
</table>
<h4 id="architecture-target-service-costs-post-deployment">Target Service Costs (Post-Deployment)</h4>
<ul>
<li><strong>SageMaker</strong>: ml.g4dn.xlarge: ~$0.736/hour</li>
<li><strong>EC2</strong>: g4dn.xlarge: ~$0.526/hour</li>
<li><strong>ECS</strong>: Fargate or EC2 costs for container hosting</li>
<li><strong>Secrets Manager</strong>: $0.40/month for API key storage</li>
</ul>
<h3 id="architecture-china-north-beijing-region-cost-estimation">China North (Beijing) Region Cost Estimation</h3>
<table>
<thead>
<tr>
<th>Service</th>
<th>Usage</th>
<th>Estimated Cost (CNY)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>S3 Storage</strong></td>
<td>5GB model file</td>
<td>¥0.00/month</td>
<td>¥0.21 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months (verify availability in China regions)</td>
</tr>
<tr>
<td><strong>CodeBuild</strong></td>
<td>BUILD_GENERAL1_LARGE for 10 minutes</td>
<td>¥0.80</td>
<td>¥0.08 per build-minute</td>
</tr>
<tr>
<td><strong>CodePipeline</strong></td>
<td>1 pipeline execution</td>
<td>¥0.00</td>
<td>First pipeline is free, then ¥7.00 per active pipeline/month</td>
</tr>
<tr>
<td><strong>CloudFormation</strong></td>
<td>Stack creation</td>
<td>¥0.00</td>
<td>No charge for CloudFormation service</td>
</tr>
<tr>
<td><strong>ECR</strong></td>
<td>~2GB Docker image</td>
<td>¥0.84/month</td>
<td>¥0.42 per GB-month for private repository storage</td>
</tr>
<tr>
<td><strong>Total Deployment Cost</strong></td>
<td></td>
<td><strong>¥0.80</strong> + ¥0.84/month</td>
<td>One-time deployment cost + monthly storage</td>
</tr>
</tbody>
</table>
<h4 id="architecture-target-service-costs-post-deployment_1">Target Service Costs (Post-Deployment)</h4>
<ul>
<li><strong>SageMaker</strong>: ml.g4dn.xlarge: ~¥6.18/hour</li>
<li><strong>EC2</strong>: g4dn.xlarge: ~¥4.42/hour</li>
<li><strong>ECS</strong>: Fargate or EC2 costs for container hosting</li>
<li><strong>Secrets Manager</strong>: ¥3.36/month for API key storage</li>
</ul>
<blockquote>
<p><strong>Note</strong>: All prices are estimates as of 2024. Actual costs may vary based on your specific AWS region, usage patterns, and any applicable discounts. We recommend using AWS Cost Explorer to monitor and forecast your actual costs.</p>
</blockquote>
<h2 id="architecture-security-considerations">Security Considerations</h2>
<h3 id="architecture-api-key-authentication">API Key Authentication</h3>
<p>EMD supports API key authentication for securing access to your deployed models:</p>
<h4 id="architecture-setting-up-api-keys">Setting Up API Keys</h4>
<p><strong>Using Command Line:</strong>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1">emd<span class="w"> </span>deploy<span class="w"> </span>--model-id<span class="w"> </span>&lt;model-id&gt;<span class="w"> </span>--instance-type<span class="w"> </span>&lt;instance-type&gt;<span class="w"> </span>--engine-type<span class="w"> </span>&lt;engine-type&gt;<span class="w"> </span>--service-type<span class="w"> </span>&lt;service-type&gt;<span class="w"> </span>--extra-params<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-0-2"><span class="s1">  &quot;service_params&quot;: {</span>
</span><span id="__span-0-3"><span class="s1">    &quot;api_key&quot;: &quot;your-secure-api-key&quot;</span>
</span><span id="__span-0-4"><span class="s1">  }</span>
</span><span id="__span-0-5"><span class="s1">}&#39;</span>
</span></code></pre></div></p>
<p><strong>Using Interactive CLI:</strong>
When prompted for "Extra Parameters" during <code>emd deploy</code>, enter:
<div class="language-json highlight"><pre><span></span><code><span id="__span-1-1"><span class="p">{</span>
</span><span id="__span-1-2"><span class="w">  </span><span class="nt">&quot;service_params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-1-3"><span class="w">    </span><span class="nt">&quot;api_key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;your-secure-api-key&quot;</span>
</span><span id="__span-1-4"><span class="w">  </span><span class="p">}</span>
</span><span id="__span-1-5"><span class="p">}</span>
</span></code></pre></div></p>
<h4 id="architecture-managing-api-keys">Managing API Keys</h4>
<ul>
<li><strong>Storage</strong>: Keys are securely stored in AWS Secrets Manager</li>
<li><strong>Access</strong>: Keys can be retrieved from the AWS Secrets Manager console</li>
<li><strong>Rotation</strong>: Update keys periodically through Secrets Manager or by redeploying</li>
</ul>
<h4 id="architecture-using-api-keys">Using API Keys</h4>
<p>Include the API key in your requests to the model endpoint:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-2-1">curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>https://your-endpoint.com/v1/chat/completions<span class="w"> </span><span class="se">\</span>
</span><span id="__span-2-2"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-2-3"><span class="w">  </span>-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer YOUR_API_KEY&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-2-4"><span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-2-5"><span class="s1">    &quot;model&quot;: &quot;your-model-id/tag&quot;,</span>
</span><span id="__span-2-6"><span class="s1">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]</span>
</span><span id="__span-2-7"><span class="s1">  }&#39;</span>
</span></code></pre></div></p>
<p>With Python:
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
</span><span id="__span-3-2">
</span><span id="__span-3-3"><span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span>
</span><span id="__span-3-4">    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://your-endpoint.com&quot;</span><span class="p">,</span>
</span><span id="__span-3-5">    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;YOUR_API_KEY&quot;</span>
</span><span id="__span-3-6"><span class="p">)</span>
</span><span id="__span-3-7">
</span><span id="__span-3-8"><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-3-9">    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;your-model-id/tag&quot;</span><span class="p">,</span>
</span><span id="__span-3-10">    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">}]</span>
</span><span id="__span-3-11"><span class="p">)</span>
</span><span id="__span-3-12"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></code></pre></div></p>
<h3 id="architecture-security-best-practices">Security Best Practices</h3>
<ol>
<li>
<p><strong>Enable HTTPS</strong>:
   <div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"># Create certificate in AWS Certificate Manager
</span><span id="__span-4-2">aws acm request-certificate --domain-name your-model-endpoint.com
</span><span id="__span-4-3">
</span><span id="__span-4-4"># Update ALB listener to use HTTPS
</span><span id="__span-4-5">aws elbv2 create-listener --load-balancer-arn &lt;your-alb-arn&gt; \
</span><span id="__span-4-6">  --protocol HTTPS --port 443 \
</span><span id="__span-4-7">  --certificates CertificateArn=&lt;certificate-arn&gt; \
</span><span id="__span-4-8">  --ssl-policy ELBSecurityPolicy-TLS13-1-2-2021-06 \
</span><span id="__span-4-9">  --default-actions Type=forward,TargetGroupArn=&lt;target-group-arn&gt;
</span></code></pre></div></p>
</li>
<li>
<p><strong>Rotate API keys regularly</strong>:</p>
</li>
<li>Update keys in AWS Secrets Manager</li>
<li>
<p>Redeploy models with new keys or update existing keys</p>
</li>
<li>
<p><strong>Implement network isolation</strong> when needed:</p>
</li>
<li>Deploy in private subnets with NAT gateway</li>
<li>Use VPC endpoints for AWS services</li>
</ol></section><section class="print-page" id="sdk_integration"><h1 id="sdk_integration-langchain-integration">LangChain Integration</h1>
<p>This guide covers how to integrate with deployed models using the LangChain framework.</p>
<h2 id="sdk_integration-llm-models">LLM Models</h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.integrations.langchain_clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerVllmChatModel</span>
</span><span id="__span-0-2"><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">StrOutputParser</span>
</span><span id="__span-0-3"><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
</span><span id="__span-0-4">
</span><span id="__span-0-5"><span class="c1"># Initialize the chat model</span>
</span><span id="__span-0-6"><span class="n">chat_model</span> <span class="o">=</span> <span class="n">SageMakerVllmChatModel</span><span class="p">(</span>
</span><span id="__span-0-7">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-0-8">    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-9">        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span><span id="__span-0-10">    <span class="p">}</span>
</span><span id="__span-0-11"><span class="p">)</span>
</span><span id="__span-0-12">
</span><span id="__span-0-13"><span class="c1"># Create a simple chain</span>
</span><span id="__span-0-14"><span class="n">chain</span> <span class="o">=</span> <span class="n">chat_model</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
</span><span id="__span-0-15">
</span><span id="__span-0-16"><span class="c1"># Define messages</span>
</span><span id="__span-0-17"><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-0-18">    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">),</span>
</span><span id="__span-0-19"><span class="p">]</span>
</span><span id="__span-0-20">
</span><span id="__span-0-21"><span class="c1"># Invoke the chain</span>
</span><span id="__span-0-22"><span class="n">response</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</span><span id="__span-0-23"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="sdk_integration-function-calling-with-langchain">Function Calling with LangChain</h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.tools.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">StructuredTool</span>
</span><span id="__span-1-2"><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.utils.function_calling</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="__span-1-3">    <span class="n">convert_to_openai_function</span><span class="p">,</span>
</span><span id="__span-1-4">    <span class="n">convert_to_openai_tool</span>
</span><span id="__span-1-5"><span class="p">)</span>
</span><span id="__span-1-6">
</span><span id="__span-1-7"><span class="c1"># Define a function</span>
</span><span id="__span-1-8"><span class="k">def</span><span class="w"> </span><span class="nf">get_weather</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">unit</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;celsius&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="__span-1-9"><span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the current weather in a given location&quot;&quot;&quot;</span>
</span><span id="__span-1-10">    <span class="c1"># This would call a weather API in a real application</span>
</span><span id="__span-1-11">    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;The weather in </span><span class="si">{</span><span class="n">location</span><span class="si">}</span><span class="s2"> is sunny and 25 degrees </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-1-12">
</span><span id="__span-1-13"><span class="c1"># Create a tool</span>
</span><span id="__span-1-14"><span class="n">weather_tool</span> <span class="o">=</span> <span class="n">StructuredTool</span><span class="o">.</span><span class="n">from_function</span><span class="p">(</span><span class="n">get_weather</span><span class="p">)</span>
</span><span id="__span-1-15">
</span><span id="__span-1-16"><span class="c1"># Convert to OpenAI tool format</span>
</span><span id="__span-1-17"><span class="n">openai_tool</span> <span class="o">=</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">weather_tool</span><span class="p">)</span>
</span><span id="__span-1-18">
</span><span id="__span-1-19"><span class="c1"># Initialize the model with tools</span>
</span><span id="__span-1-20"><span class="n">chat_model</span> <span class="o">=</span> <span class="n">SageMakerVllmChatModel</span><span class="p">(</span>
</span><span id="__span-1-21">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;Qwen2.5-7B-Instruct&quot;</span><span class="p">,</span>
</span><span id="__span-1-22">    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-23">        <span class="s2">&quot;tools&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">openai_tool</span><span class="p">],</span>
</span><span id="__span-1-24">        <span class="s2">&quot;tool_choice&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span>
</span><span id="__span-1-25">    <span class="p">}</span>
</span><span id="__span-1-26"><span class="p">)</span>
</span><span id="__span-1-27">
</span><span id="__span-1-28"><span class="c1"># Invoke with a query that should trigger tool use</span>
</span><span id="__span-1-29"><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-1-30">    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What&#39;s the weather like in Paris?&quot;</span><span class="p">)</span>
</span><span id="__span-1-31"><span class="p">]</span>
</span><span id="__span-1-32">
</span><span id="__span-1-33"><span class="n">response</span> <span class="o">=</span> <span class="n">chat_model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</span><span id="__span-1-34"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="sdk_integration-embedding-models">Embedding Models</h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</span><span id="__span-2-2"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.integrations.langchain_clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerVllmEmbeddings</span>
</span><span id="__span-2-3">
</span><span id="__span-2-4"><span class="c1"># Initialize the embedding model</span>
</span><span id="__span-2-5"><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SageMakerVllmEmbeddings</span><span class="p">(</span>
</span><span id="__span-2-6">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">,</span>
</span><span id="__span-2-7"><span class="p">)</span>
</span><span id="__span-2-8">
</span><span id="__span-2-9"><span class="c1"># Get embeddings for a single text</span>
</span><span id="__span-2-10"><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>
</span><span id="__span-2-11"><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span><span id="__span-2-12">
</span><span id="__span-2-13"><span class="c1"># Get embeddings for multiple documents</span>
</span><span id="__span-2-14"><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># 10 copies of the same text for demonstration</span>
</span><span id="__span-2-15"><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</span><span id="__span-2-16">
</span><span id="__span-2-17"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Single embedding dimension: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-2-18"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of document embeddings: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="sdk_integration-reranking-models">Reranking Models</h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><span class="kn">from</span><span class="w"> </span><span class="nn">emd.integrations.langchain_clients</span><span class="w"> </span><span class="kn">import</span> <span class="n">SageMakerVllmRerank</span>
</span><span id="__span-3-2">
</span><span id="__span-3-3"><span class="c1"># Initialize the reranker</span>
</span><span id="__span-3-4"><span class="n">rerank_model</span> <span class="o">=</span> <span class="n">SageMakerVllmRerank</span><span class="p">(</span>
</span><span id="__span-3-5">    <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;bge-reranker-v2-m3&quot;</span>
</span><span id="__span-3-6"><span class="p">)</span>
</span><span id="__span-3-7">
</span><span id="__span-3-8"><span class="c1"># Define documents and query</span>
</span><span id="__span-3-9"><span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-3-10">    <span class="s2">&quot;The giant panda is a bear species endemic to China.&quot;</span><span class="p">,</span>
</span><span id="__span-3-11">    <span class="s2">&quot;Paris is the capital of France.&quot;</span><span class="p">,</span>
</span><span id="__span-3-12">    <span class="s2">&quot;Machine learning is a subset of artificial intelligence.&quot;</span>
</span><span id="__span-3-13"><span class="p">]</span>
</span><span id="__span-3-14"><span class="n">query</span> <span class="o">=</span> <span class="s1">&#39;What is a panda?&#39;</span>
</span><span id="__span-3-15">
</span><span id="__span-3-16"><span class="c1"># Rerank documents based on relevance to the query</span>
</span><span id="__span-3-17"><span class="n">results</span> <span class="o">=</span> <span class="n">rerank_model</span><span class="o">.</span><span class="n">rerank</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">documents</span><span class="o">=</span><span class="n">docs</span><span class="p">)</span>
</span><span id="__span-3-18">
</span><span id="__span-3-19"><span class="c1"># Print results</span>
</span><span id="__span-3-20"><span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
</span><span id="__span-3-21">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">document</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-3-22">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">relevance_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-3-23">    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="sdk_integration-vision-models-vlm">Vision Models (VLM)</h2>
<p>For vision models, you can use the EMD CLI to invoke them:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-4-1"><span class="c1"># Upload an image to S3</span>
</span><span id="__span-4-2">aws<span class="w"> </span>s3<span class="w"> </span>cp<span class="w"> </span>image.jpg<span class="w"> </span>s3://your-bucket/image.jpg
</span><span id="__span-4-3">
</span><span id="__span-4-4"><span class="c1"># Invoke the vision model</span>
</span><span id="__span-4-5">emd<span class="w"> </span>invoke<span class="w"> </span>Qwen2-VL-7B-Instruct
</span></code></pre></div>
<p>When prompted:
- Enter the S3 path to your image: <code>s3://your-bucket/image.jpg</code>
- Enter your prompt: <code>What's in this image?</code></p></section><section class="print-page" id="swift_chat_integration"><h1 id="swift_chat_integration-swiftchat-integration">SwiftChat Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/aws-samples/swift-chat">SwiftChat</a>, a fast and responsive cross-platform AI chat application.</p>
<h2 id="swift_chat_integration-overview">Overview</h2>
<p>SwiftChat is a cross-platform AI chat application developed with React Native that supports multiple model providers, including OpenAI Compatible models. This makes it an excellent client for interacting with models deployed using Easy Model Deployer (EMD).</p>
<p>With SwiftChat, you can:
- Chat with your EMD-deployed models in real-time with streaming responses
- Use rich markdown features including tables, code blocks, and LaTeX
- Access your models across Android, iOS, and macOS platforms
- Enjoy a fast, responsive, and privacy-focused experience</p>
<h2 id="swift_chat_integration-key-features-of-swiftchat">Key Features of SwiftChat</h2>
<ul>
<li>Real-time streaming chat with AI</li>
<li>Rich Markdown support (tables, code blocks, LaTeX, etc.)</li>
<li>AI image generation with progress indicators</li>
<li>Multimodal support (images, videos &amp; documents)</li>
<li>Conversation history management</li>
<li>Cross-platform support (Android, iOS, macOS)</li>
<li>Tablet-optimized for iPad and Android tablets</li>
<li>Multiple AI model providers supported</li>
<li>Fully customizable system prompts</li>
</ul>
<h2 id="swift_chat_integration-integrating-emd-models-with-swiftchat">Integrating EMD Models with SwiftChat</h2>
<p>EMD-deployed models can be easily integrated with SwiftChat through its OpenAI Compatible interface. This allows you to use your own deployed models with all the features and convenience of the SwiftChat application.</p>
<h3 id="swift_chat_integration-prerequisites">Prerequisites</h3>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have installed SwiftChat on your device (Android, iOS, or macOS)</li>
<li>You have the base URL and API key for your deployed model</li>
</ol>
<h3 id="swift_chat_integration-configuration-steps">Configuration Steps</h3>
<ol>
<li>Open SwiftChat on your device</li>
<li>Navigate to the Settings page</li>
<li>Select the OpenAI tab</li>
<li>Under OpenAI Compatible, enter the following information:</li>
<li><strong>Base URL</strong>: The endpoint URL of your EMD-deployed model (e.g., <code>https://your-endpoint.execute-api.region.amazonaws.com</code>)</li>
<li><strong>API Key</strong>: Your API key for accessing the model</li>
<li><strong>Model ID</strong>: The ID of your deployed model(s), separate multiple models with commas</li>
<li>Select one of your models from the Text Model dropdown list</li>
</ol>
<p><img alt="SwiftChat OpenAI Compatible Settings" src="../images/sample.png" /></p>
<h2 id="swift_chat_integration-example-use-cases">Example Use Cases</h2>
<p>Once configured, you can use your EMD-deployed models in SwiftChat for various use cases:</p>
<ul>
<li><strong>Text Generation</strong>: Chat with your models using the real-time streaming interface</li>
<li><strong>Code Generation</strong>: Ask your models to write or explain code, with proper syntax highlighting</li>
<li><strong>Document Analysis</strong>: Upload documents for your models to analyze (if multimodal models are supported)</li>
<li><strong>Image Understanding</strong>: Share images with vision-enabled models for analysis and discussion</li>
</ul>
<h2 id="swift_chat_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues connecting to your EMD-deployed model:</p>
<ol>
<li>Verify that your model is properly deployed and running</li>
<li>Check that the Base URL is correct and includes the full endpoint path</li>
<li>Ensure your API key has the necessary permissions</li>
<li>Confirm that your model ID exactly matches the deployed model's identifier</li>
<li>Check network connectivity between your device and the model endpoint</li>
</ol>
<h2 id="swift_chat_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/aws-samples/swift-chat">SwiftChat GitHub Repository</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section><section class="print-page" id="dify_integration"><h1 id="dify_integration-dify-integration">Dify Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/langgenius/dify">Dify</a>, an open-source LLM app development platform.</p>
<h2 id="dify_integration-overview">Overview</h2>
<p>Dify is a comprehensive LLM app development platform that combines workflow automation, RAG pipeline, agent capabilities, model management, and observability features. With its intuitive interface, you can quickly build AI applications using models deployed with Easy Model Deployer (EMD).</p>
<p>With Dify, you can:
- Create AI applications with visual workflows
- Connect to various LLM providers, including custom OpenAI API-compatible endpoints
- Build RAG (Retrieval-Augmented Generation) pipelines
- Create AI agents with tools and function calling
- Monitor and analyze application performance
- Deploy your applications with ready-to-use APIs</p>
<h2 id="dify_integration-key-features-of-dify">Key Features of Dify</h2>
<ul>
<li><strong>Workflow</strong>: Build and test AI workflows on a visual canvas</li>
<li><strong>Comprehensive Model Support</strong>: Integration with hundreds of proprietary and open-source LLMs</li>
<li><strong>Prompt IDE</strong>: Intuitive interface for crafting prompts and comparing model performance</li>
<li><strong>RAG Pipeline</strong>: Extensive capabilities for document ingestion and retrieval</li>
<li><strong>Agent Capabilities</strong>: Define agents with LLM Function Calling or ReAct, with 50+ built-in tools</li>
<li><strong>LLMOps</strong>: Monitor and analyze application logs and performance</li>
<li><strong>Backend-as-a-Service</strong>: APIs for integrating Dify into your business logic</li>
</ul>
<h2 id="dify_integration-integrating-emd-models-with-dify">Integrating EMD Models with Dify</h2>
<p>EMD-deployed models can be easily integrated with Dify through its OpenAI API-compatible interface. This allows you to use your own deployed models with all the features and capabilities of the Dify platform.</p>
<h3 id="dify_integration-prerequisites">Prerequisites</h3>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have installed Dify (either using <a href="https://cloud.dify.ai">Dify Cloud</a> or <a href="https://docs.dify.ai/getting-started/install-self-hosted">self-hosted</a>)</li>
<li>You have the base URL and API key for your deployed model</li>
</ol>
<h3 id="dify_integration-configuration-steps">Configuration Steps</h3>
<ol>
<li>Log in to your Dify dashboard</li>
<li>Navigate to the <strong>Settings</strong> section</li>
<li>Select <strong>Model Providers</strong></li>
<li>Click on <strong>Add</strong> and select <strong>Custom</strong> or <strong>OpenAI API Compatible</strong></li>
<li>Configure the provider with the following information:</li>
<li><strong>Provider Name</strong>: A name for your EMD model provider (e.g., "EMD Models")</li>
<li><strong>Base URL</strong>: The endpoint URL of your EMD-deployed model (e.g., <code>https://your-endpoint.execute-api.region.amazonaws.com</code>)</li>
<li><strong>API Key</strong>: Your API key for accessing the model</li>
<li><strong>Available Models</strong>: Add the model IDs of your deployed models</li>
<li>Click <strong>Save</strong> to add the provider</li>
</ol>
<p>Once configured, your EMD-deployed models will appear in the model selection dropdown when creating applications in Dify.</p>
<h2 id="dify_integration-example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated into Dify, you can build various applications:</p>
<ul>
<li><strong>Conversational AI</strong>: Create chatbots and virtual assistants using your custom models</li>
<li><strong>Document Processing</strong>: Build RAG applications that use your models to process and analyze documents</li>
<li><strong>AI Workflows</strong>: Design complex workflows that incorporate your models at different stages</li>
<li><strong>Custom Agents</strong>: Create AI agents that use your models for reasoning and decision-making</li>
<li><strong>API Services</strong>: Expose your models as APIs with additional processing logic</li>
</ul>
<h2 id="dify_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues connecting to your EMD-deployed model:</p>
<ol>
<li>Verify that your model is properly deployed and running</li>
<li>Check that the Base URL is correct and includes the full endpoint path</li>
<li>Ensure your API key has the necessary permissions</li>
<li>Confirm that your model ID exactly matches the deployed model's identifier</li>
<li>Test the connection using a simple API request before integrating with Dify</li>
</ol>
<h2 id="dify_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/langgenius/dify">Dify GitHub Repository</a></li>
<li><a href="https://docs.dify.ai">Dify Documentation</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section><section class="print-page" id="langflow_integration"><h1 id="langflow_integration-langflow-integration">LangFlow Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/langflow-ai/langflow">LangFlow</a>, an open-source UI for LangChain.</p>
<h2 id="langflow_integration-overview">Overview</h2>
<p>LangFlow is a user-friendly interface that allows you to build LangChain applications using a drag-and-drop visual editor. It provides a way to prototype and experiment with various LangChain components, including language models, without writing code. By integrating EMD-deployed models with LangFlow, you can easily incorporate your custom models into complex LangChain workflows.</p>
<p>With LangFlow, you can:
- Create complex LangChain flows using a visual interface
- Connect your EMD-deployed models to various components
- Prototype and test different configurations
- Export your flows as Python code
- Share your flows with others</p>
<h2 id="langflow_integration-key-features-of-langflow">Key Features of LangFlow</h2>
<ul>
<li><strong>Visual Flow Builder</strong>: Drag-and-drop interface for creating LangChain flows</li>
<li><strong>Component Library</strong>: Pre-built components for various LangChain modules</li>
<li><strong>Real-time Testing</strong>: Test your flows directly in the interface</li>
<li><strong>Code Export</strong>: Export your flows as Python code</li>
<li><strong>Custom Components</strong>: Add your own custom components</li>
<li><strong>Flow Sharing</strong>: Share your flows with others</li>
</ul>
<h2 id="langflow_integration-integrating-emd-models-with-langflow">Integrating EMD Models with LangFlow</h2>
<p>EMD-deployed models can be integrated with LangFlow through its OpenAI API compatibility. This allows you to use your custom models in various LangChain components that support OpenAI-compatible APIs.</p>
<h3 id="langflow_integration-prerequisites">Prerequisites</h3>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have installed and set up LangFlow (either <a href="https://github.com/langflow-ai/langflow#installation">locally</a> or using <a href="https://github.com/langflow-ai/langflow#docker">Docker</a>)</li>
<li>You have the base URL and API key for your deployed model</li>
</ol>
<h3 id="langflow_integration-configuration-steps">Configuration Steps</h3>
<ol>
<li>Launch LangFlow and log in to the interface</li>
<li>Create a new flow or open an existing one</li>
<li>Add an LLM component to your flow (such as ChatOpenAI or OpenAI)</li>
<li>Configure the LLM component with the following settings:</li>
<li><strong>Base URL</strong>: The endpoint URL of your EMD-deployed model (e.g., <code>https://your-endpoint.execute-api.region.amazonaws.com</code>)</li>
<li><strong>API Key</strong>: Your API key for accessing the model</li>
<li><strong>Model Name</strong>: The ID of your deployed model</li>
<li>Connect the LLM component to other components in your flow</li>
<li>Test your flow using the "Build" button</li>
</ol>
<h2 id="langflow_integration-example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated into LangFlow, you can build various applications:</p>
<ul>
<li><strong>Conversational Agents</strong>: Create chatbots and virtual assistants using your custom models</li>
<li><strong>Document Processing</strong>: Build document processing pipelines with RAG (Retrieval-Augmented Generation)</li>
<li><strong>Knowledge Extraction</strong>: Extract structured information from unstructured text</li>
<li><strong>Content Generation</strong>: Generate content based on specific inputs and constraints</li>
<li><strong>Multi-step Reasoning</strong>: Create flows that break down complex problems into smaller steps</li>
</ul>
<h2 id="langflow_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues connecting to your EMD-deployed model:</p>
<ol>
<li>Verify that your model is properly deployed and running</li>
<li>Check that the Base URL is correct and includes the full endpoint path</li>
<li>Ensure your API key has the necessary permissions</li>
<li>Confirm that your model ID exactly matches the deployed model's identifier</li>
<li>Check the LangFlow logs for any error messages</li>
</ol>
<h2 id="langflow_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/langflow-ai/langflow">LangFlow GitHub Repository</a></li>
<li><a href="https://docs.langflow.org">LangFlow Documentation</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section><section class="print-page" id="flowise_integration"><h1 id="flowise_integration-flowise-integration">Flowise Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/FlowiseAI/Flowise">Flowise</a>, an open-source UI for building LLM applications.</p>
<h2 id="flowise_integration-overview">Overview</h2>
<p>Flowise is a powerful drag-and-drop interface for building custom AI workflows. It provides a visual way to connect various components like language models, embeddings, vector stores, and more to create complex LLM applications without writing code. By integrating EMD-deployed models with Flowise, you can leverage your custom models in sophisticated AI workflows.</p>
<p>With Flowise, you can:
- Build LLM applications using a visual interface
- Connect your EMD-deployed models to various components
- Create chatbots, document Q&amp;A systems, and other AI applications
- Deploy your workflows as API endpoints
- Share your workflows with others</p>
<h2 id="flowise_integration-key-features-of-flowise">Key Features of Flowise</h2>
<ul>
<li><strong>Visual Flow Builder</strong>: Drag-and-drop interface for creating AI workflows</li>
<li><strong>Component Library</strong>: Pre-built components for various LLM operations</li>
<li><strong>API Deployment</strong>: Deploy your workflows as API endpoints</li>
<li><strong>Chatbot Interface</strong>: Built-in chatbot interface for testing</li>
<li><strong>Custom Components</strong>: Add your own custom components</li>
<li><strong>Marketplace</strong>: Share and discover workflows created by the community</li>
</ul>
<h2 id="flowise_integration-integrating-emd-models-with-flowise">Integrating EMD Models with Flowise</h2>
<p>EMD-deployed models can be integrated with Flowise through its OpenAI API compatibility. This allows you to use your custom models in various Flowise components that support OpenAI-compatible APIs.</p>
<h3 id="flowise_integration-prerequisites">Prerequisites</h3>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have installed and set up Flowise (either <a href="https://github.com/FlowiseAI/Flowise#installation">locally</a> or using <a href="https://github.com/FlowiseAI/Flowise#docker">Docker</a>)</li>
<li>You have the base URL and API key for your deployed model</li>
</ol>
<h3 id="flowise_integration-configuration-steps">Configuration Steps</h3>
<ol>
<li>Launch Flowise and log in to the interface</li>
<li>Create a new canvas or open an existing one</li>
<li>From the components panel, search for and add the "ChatOpenAI" component to your canvas</li>
<li>Configure the ChatOpenAI component with the following settings:</li>
<li><strong>Base URL</strong>: The endpoint URL of your EMD-deployed model (e.g., <code>https://your-endpoint.execute-api.region.amazonaws.com</code>)</li>
<li><strong>API Key</strong>: Your API key for accessing the model</li>
<li><strong>Model Name</strong>: The ID of your deployed model</li>
<li>Connect the ChatOpenAI component to other components in your workflow</li>
<li>Test your workflow using the built-in chatbot interface</li>
</ol>
<h2 id="flowise_integration-example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated into Flowise, you can build various applications:</p>
<ul>
<li><strong>Conversational AI</strong>: Create chatbots and virtual assistants using your custom models</li>
<li><strong>Document Q&amp;A</strong>: Build systems that can answer questions based on document content</li>
<li><strong>Knowledge Bases</strong>: Create searchable knowledge bases with RAG (Retrieval-Augmented Generation)</li>
<li><strong>Content Generation</strong>: Generate content based on specific inputs and constraints</li>
<li><strong>Data Analysis</strong>: Extract insights from structured and unstructured data</li>
</ul>
<h2 id="flowise_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues connecting to your EMD-deployed model:</p>
<ol>
<li>Verify that your model is properly deployed and running</li>
<li>Check that the Base URL is correct and includes the full endpoint path</li>
<li>Ensure your API key has the necessary permissions</li>
<li>Confirm that your model ID exactly matches the deployed model's identifier</li>
<li>Check the Flowise logs for any error messages</li>
</ol>
<h2 id="flowise_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/FlowiseAI/Flowise">Flowise GitHub Repository</a></li>
<li><a href="https://docs.flowiseai.com">Flowise Documentation</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section><section class="print-page" id="ollama_integration"><h1 id="ollama_integration-ollama-integration">Ollama Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/ollama/ollama">Ollama</a>, an open-source framework for running large language models locally.</p>
<h2 id="ollama_integration-overview">Overview</h2>
<p>Ollama is a popular tool that allows you to run large language models locally on your own hardware. It provides a simple way to download, run, and manage various open-source models. By integrating EMD-deployed models with Ollama, you can create a hybrid setup that leverages both local models and your custom cloud-deployed models.</p>
<p>With Ollama integration, you can:
- Use both local models and EMD-deployed models in your applications
- Create fallback mechanisms between local and cloud models
- Compare performance between local and cloud-deployed versions
- Develop applications that work both online and offline
- Optimize for cost, performance, or privacy based on specific needs</p>
<h2 id="ollama_integration-key-features-of-ollama">Key Features of Ollama</h2>
<ul>
<li><strong>Local Model Execution</strong>: Run models on your own hardware</li>
<li><strong>Simple API</strong>: Easy-to-use REST API for model interaction</li>
<li><strong>Model Library</strong>: Access to various open-source models</li>
<li><strong>Customization</strong>: Create and customize model configurations</li>
<li><strong>Cross-platform</strong>: Available for macOS, Windows, and Linux</li>
<li><strong>Low Resource Usage</strong>: Optimized for running on consumer hardware</li>
</ul>
<h2 id="ollama_integration-integrating-emd-models-with-ollama">Integrating EMD Models with Ollama</h2>
<p>There are several ways to integrate EMD-deployed models with Ollama:</p>
<h3 id="ollama_integration-1-api-orchestration">1. API Orchestration</h3>
<p>You can build an orchestration layer that routes requests between Ollama's API and your EMD-deployed model's API based on specific criteria.</p>
<h4 id="ollama_integration-prerequisites">Prerequisites</h4>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have <a href="https://github.com/ollama/ollama#installation">installed Ollama</a> on your local machine</li>
<li>You have the base URL and API key for your deployed EMD model</li>
</ol>
<h4 id="ollama_integration-implementation-example">Implementation Example</h4>
<p>Here's a simple Python example that routes requests between Ollama and an EMD-deployed model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
</span><span id="__span-0-2"><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="__span-0-3">
</span><span id="__span-0-4"><span class="k">def</span><span class="w"> </span><span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span><span id="__span-0-5"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-6"><span class="sd">    Generate text using either Ollama (local) or EMD-deployed model (cloud)</span>
</span><span id="__span-0-7">
</span><span id="__span-0-8"><span class="sd">    Args:</span>
</span><span id="__span-0-9"><span class="sd">        prompt (str): The input prompt</span>
</span><span id="__span-0-10"><span class="sd">        use_local (bool): Whether to use local Ollama model</span>
</span><span id="__span-0-11"><span class="sd">        max_tokens (int): Maximum tokens to generate</span>
</span><span id="__span-0-12">
</span><span id="__span-0-13"><span class="sd">    Returns:</span>
</span><span id="__span-0-14"><span class="sd">        str: Generated text</span>
</span><span id="__span-0-15"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-16">    <span class="k">if</span> <span class="n">use_local</span><span class="p">:</span>
</span><span id="__span-0-17">        <span class="c1"># Use Ollama API (local)</span>
</span><span id="__span-0-18">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-0-19">            <span class="s2">&quot;http://localhost:11434/api/generate&quot;</span><span class="p">,</span>
</span><span id="__span-0-20">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-21">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;llama3&quot;</span><span class="p">,</span>  <span class="c1"># or any other model you have pulled</span>
</span><span id="__span-0-22">                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
</span><span id="__span-0-23">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-0-24">            <span class="p">}</span>
</span><span id="__span-0-25">        <span class="p">)</span>
</span><span id="__span-0-26">        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-0-27">    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-28">        <span class="c1"># Use EMD-deployed model API (cloud)</span>
</span><span id="__span-0-29">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-0-30">            <span class="s2">&quot;https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions&quot;</span><span class="p">,</span>
</span><span id="__span-0-31">            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-32">                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
</span><span id="__span-0-33">                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer your-api-key&quot;</span>
</span><span id="__span-0-34">            <span class="p">},</span>
</span><span id="__span-0-35">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-36">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;your-deployed-model-id&quot;</span><span class="p">,</span>
</span><span id="__span-0-37">                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
</span><span id="__span-0-38">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-0-39">            <span class="p">}</span>
</span><span id="__span-0-40">        <span class="p">)</span>
</span><span id="__span-0-41">        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;choices&quot;</span><span class="p">,</span> <span class="p">[{}])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-0-42">
</span><span id="__span-0-43"><span class="c1"># Example usage</span>
</span><span id="__span-0-44"><span class="n">result</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-0-45"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span id="__span-0-46">
</span><span id="__span-0-47"><span class="n">result</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-0-48"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="ollama_integration-2-fallback-mechanism">2. Fallback Mechanism</h3>
<p>You can implement a fallback mechanism that tries the local Ollama model first and falls back to the EMD-deployed model if the local model fails or produces unsatisfactory results.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="k">def</span><span class="w"> </span><span class="nf">generate_with_fallback</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span><span id="__span-1-2"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-1-3"><span class="sd">    Try local model first, fall back to cloud model if needed</span>
</span><span id="__span-1-4">
</span><span id="__span-1-5"><span class="sd">    Args:</span>
</span><span id="__span-1-6"><span class="sd">        prompt (str): The input prompt</span>
</span><span id="__span-1-7"><span class="sd">        max_tokens (int): Maximum tokens to generate</span>
</span><span id="__span-1-8">
</span><span id="__span-1-9"><span class="sd">    Returns:</span>
</span><span id="__span-1-10"><span class="sd">        str: Generated text</span>
</span><span id="__span-1-11"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-1-12">    <span class="k">try</span><span class="p">:</span>
</span><span id="__span-1-13">        <span class="c1"># Try Ollama first</span>
</span><span id="__span-1-14">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-1-15">            <span class="s2">&quot;http://localhost:11434/api/generate&quot;</span><span class="p">,</span>
</span><span id="__span-1-16">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-17">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
</span><span id="__span-1-18">                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
</span><span id="__span-1-19">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-1-20">            <span class="p">},</span>
</span><span id="__span-1-21">            <span class="n">timeout</span><span class="o">=</span><span class="mi">5</span>  <span class="c1"># Set a timeout for local model</span>
</span><span id="__span-1-22">        <span class="p">)</span>
</span><span id="__span-1-23">
</span><span id="__span-1-24">        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
</span><span id="__span-1-25">            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-1-26">            <span class="k">if</span> <span class="n">result</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>  <span class="c1"># Simple quality check</span>
</span><span id="__span-1-27">                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>
</span><span id="__span-1-28">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-1-29">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Local model error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-1-30">
</span><span id="__span-1-31">    <span class="c1"># Fall back to EMD-deployed model</span>
</span><span id="__span-1-32">    <span class="k">try</span><span class="p">:</span>
</span><span id="__span-1-33">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-1-34">            <span class="s2">&quot;https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions&quot;</span><span class="p">,</span>
</span><span id="__span-1-35">            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-36">                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
</span><span id="__span-1-37">                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer your-api-key&quot;</span>
</span><span id="__span-1-38">            <span class="p">},</span>
</span><span id="__span-1-39">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-40">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;your-deployed-model-id&quot;</span><span class="p">,</span>
</span><span id="__span-1-41">                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
</span><span id="__span-1-42">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-1-43">            <span class="p">}</span>
</span><span id="__span-1-44">        <span class="p">)</span>
</span><span id="__span-1-45">
</span><span id="__span-1-46">        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
</span><span id="__span-1-47">            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;choices&quot;</span><span class="p">,</span> <span class="p">[{}])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-1-48">            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;cloud&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>
</span><span id="__span-1-49">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-1-50">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cloud model error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-1-51">
</span><span id="__span-1-52">    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Failed to generate response from both local and cloud models.&quot;</span><span class="p">}</span>
</span></code></pre></div>
<h2 id="ollama_integration-example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated with Ollama, you can build various applications:</p>
<ul>
<li><strong>Hybrid AI Applications</strong>: Applications that use local models for basic tasks and cloud models for more complex tasks</li>
<li><strong>Offline-First Applications</strong>: Applications that work offline with local models but enhance capabilities when online</li>
<li><strong>Cost-Optimized Solutions</strong>: Use local models for frequent, simple queries and cloud models for important or complex queries</li>
<li><strong>Privacy-Focused Applications</strong>: Process sensitive data locally and only use cloud models for non-sensitive data</li>
<li><strong>Development and Testing</strong>: Use local models during development and testing, and cloud models in production</li>
</ul>
<h2 id="ollama_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues with the integration:</p>
<ol>
<li>Verify that Ollama is running locally (<code>ollama list</code> should show available models)</li>
<li>Check that your EMD model is properly deployed and accessible</li>
<li>Ensure API endpoints and authentication details are correct</li>
<li>Check network connectivity if using cloud models</li>
<li>Monitor resource usage if local models are running slowly</li>
</ol>
<h2 id="ollama_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/ollama/ollama">Ollama GitHub Repository</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/api.md">Ollama API Documentation</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section><section class="print-page" id="nextchat_integration"><h1 id="nextchat_integration-nextchat-integration">NextChat Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web">NextChat</a>, an open-source cross-platform ChatGPT web UI.</p>
<h2 id="nextchat_integration-overview">Overview</h2>
<p>NextChat (ChatGPT-Next-Web) is a popular open-source project that provides a clean, intuitive web interface for interacting with various large language models. It supports multiple models including OpenAI's GPT models, and importantly, any OpenAI API-compatible endpoints. By integrating EMD-deployed models with NextChat, you can create a user-friendly interface for interacting with your custom models.</p>
<p>With NextChat, you can:
- Access your EMD-deployed models through a polished web interface
- Create and manage multiple conversations
- Save and share conversation history
- Customize the UI to match your preferences
- Deploy the interface on various platforms</p>
<h2 id="nextchat_integration-key-features-of-nextchat">Key Features of NextChat</h2>
<ul>
<li><strong>Clean User Interface</strong>: Modern, responsive design for desktop and mobile</li>
<li><strong>Multi-model Support</strong>: Switch between different models easily</li>
<li><strong>Conversation Management</strong>: Create, save, and organize conversations</li>
<li><strong>Prompt Templates</strong>: Create and use templates for common prompts</li>
<li><strong>Markdown Support</strong>: Rich text formatting with Markdown</li>
<li><strong>Self-hosting</strong>: Deploy on your own infrastructure</li>
<li><strong>Cross-platform</strong>: Available as a web app, PWA, or desktop application</li>
</ul>
<h2 id="nextchat_integration-integrating-emd-models-with-nextchat">Integrating EMD Models with NextChat</h2>
<p>EMD-deployed models can be easily integrated with NextChat through its OpenAI API compatibility. This allows you to use your custom models with all the features and convenience of the NextChat interface.</p>
<h3 id="nextchat_integration-prerequisites">Prerequisites</h3>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have access to NextChat (either through the <a href="https://chat.nextchat.dev">hosted version</a> or by <a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web#environment-variables">self-hosting</a>)</li>
<li>You have the base URL and API key for your deployed model</li>
</ol>
<h3 id="nextchat_integration-configuration-steps">Configuration Steps</h3>
<ol>
<li>Access NextChat through your browser</li>
<li>Click on the settings icon in the sidebar</li>
<li>Navigate to the "Models" section</li>
<li>Click "Add Custom Model"</li>
<li>Configure your EMD-deployed model with the following settings:</li>
<li><strong>Name</strong>: A name for your model (e.g., "My EMD Model")</li>
<li><strong>Base URL</strong>: The endpoint URL of your EMD-deployed model (e.g., <code>https://your-endpoint.execute-api.region.amazonaws.com</code>)</li>
<li><strong>API Key</strong>: Your API key for accessing the model</li>
<li><strong>Model Name</strong>: The ID of your deployed model</li>
<li>Save the configuration</li>
<li>Select your custom model from the model dropdown in the chat interface</li>
</ol>
<h3 id="nextchat_integration-self-hosting-nextchat-with-emd-integration">Self-hosting NextChat with EMD Integration</h3>
<p>If you prefer to self-host NextChat, you can configure it to use your EMD-deployed model by default:</p>
<ol>
<li>
<p>Clone the NextChat repository:
   <div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1">git<span class="w"> </span>clone<span class="w"> </span>https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web.git
</span><span id="__span-0-2"><span class="nb">cd</span><span class="w"> </span>ChatGPT-Next-Web
</span></code></pre></div></p>
</li>
<li>
<p>Create a <code>.env.local</code> file with the following environment variables:
   <div class="language-text highlight"><pre><span></span><code><span id="__span-1-1">OPENAI_API_KEY=your-api-key
</span><span id="__span-1-2">BASE_URL=https://your-endpoint.execute-api.region.amazonaws.com
</span><span id="__span-1-3">NEXT_PUBLIC_DEFAULT_MODEL=your-deployed-model-id
</span></code></pre></div></p>
</li>
<li>
<p>Build and run the application:
   <div class="language-bash highlight"><pre><span></span><code><span id="__span-2-1">npm<span class="w"> </span>install
</span><span id="__span-2-2">npm<span class="w"> </span>run<span class="w"> </span>build
</span><span id="__span-2-3">npm<span class="w"> </span>run<span class="w"> </span>start
</span></code></pre></div></p>
</li>
</ol>
<h2 id="nextchat_integration-example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated into NextChat, you can:</p>
<ul>
<li><strong>Personal Assistant</strong>: Create a personalized AI assistant using your custom-trained model</li>
<li><strong>Customer Support</strong>: Deploy a customer support interface with domain-specific knowledge</li>
<li><strong>Content Creation</strong>: Use the interface for brainstorming and content generation</li>
<li><strong>Educational Tool</strong>: Create an educational interface with specialized knowledge</li>
<li><strong>Research Assistant</strong>: Build a research assistant with access to specific research domains</li>
</ul>
<h2 id="nextchat_integration-troubleshooting">Troubleshooting</h2>
<p>If you encounter issues connecting to your EMD-deployed model:</p>
<ol>
<li>Verify that your model is properly deployed and running</li>
<li>Check that the Base URL is correct and includes the full endpoint path</li>
<li>Ensure your API key has the necessary permissions</li>
<li>Confirm that your model ID exactly matches the deployed model's identifier</li>
<li>Check browser console logs for any error messages</li>
</ol>
<h2 id="nextchat_integration-additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web">NextChat GitHub Repository</a></li>
<li><a href="https://docs.nextchat.dev">NextChat Documentation</a></li>
<li><a href="#supported_models">EMD Supported Models</a></li>
</ul></section></div>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/aws-samples/easy-model-deployer/" class="fa fa-code-fork" style="color: #fcfcfc"> aws-samples/easy-model-deployer</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "/aws-samples/easy-model-deployer/";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../js/print-site.js"></script>
      <script src="../copy-button.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
