{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Easy Model Deployer (EMD) Simple, Efficient, and Easy-to-Integrate Model Deployment What is EMD? EMD (Easy Model Deployer) is a lightweight tool for deploying AI models to production environments. It simplifies the process of deploying large language models (LLMs), vision models, embedding models, and more to AWS services or locally. With EMD, you can: Deploy models to SageMaker, ECS, EC2, or locally with minimal commands Skip complex infrastructure setup and container configuration Access models through an OpenAI-compatible API Integrate with popular frameworks and tools Optimize costs by choosing the right infrastructure EMD handles the technical complexity so you can focus on building applications with your models. Supported Models EMD supports a wide range of models, including: Large Language Models (LLMs) like Qwen, Llama, DeepSeek, and more Vision Language Models (VLMs) like Qwen-VL Embedding models like BGE and Jina Reranking models Audio transcription models For a complete list, see Supported Models . Use Cases AI Application Development : Build AI-powered applications with your own deployed models Cost-Effective Inference : Deploy models on the right infrastructure for your needs Private Model Hosting : Keep your models and data secure on your own infrastructure Integration with Existing Tools : Connect with popular frameworks and platforms Hybrid Deployments : Combine cloud and local deployments for optimal performance Getting Started Quick Start CLI Commands API Documentation Best Deployment Practices Architecture Overview","title":"Home"},{"location":"#what-is-emd","text":"EMD (Easy Model Deployer) is a lightweight tool for deploying AI models to production environments. It simplifies the process of deploying large language models (LLMs), vision models, embedding models, and more to AWS services or locally. With EMD, you can: Deploy models to SageMaker, ECS, EC2, or locally with minimal commands Skip complex infrastructure setup and container configuration Access models through an OpenAI-compatible API Integrate with popular frameworks and tools Optimize costs by choosing the right infrastructure EMD handles the technical complexity so you can focus on building applications with your models.","title":"What is EMD?"},{"location":"#supported-models","text":"EMD supports a wide range of models, including: Large Language Models (LLMs) like Qwen, Llama, DeepSeek, and more Vision Language Models (VLMs) like Qwen-VL Embedding models like BGE and Jina Reranking models Audio transcription models For a complete list, see Supported Models .","title":"Supported Models"},{"location":"#use-cases","text":"AI Application Development : Build AI-powered applications with your own deployed models Cost-Effective Inference : Deploy models on the right infrastructure for your needs Private Model Hosting : Keep your models and data secure on your own infrastructure Integration with Existing Tools : Connect with popular frameworks and platforms Hybrid Deployments : Combine cloud and local deployments for optimal performance","title":"Use Cases"},{"location":"#getting-started","text":"Quick Start CLI Commands API Documentation Best Deployment Practices Architecture Overview","title":"Getting Started"},{"location":"api/","text":"API Documentation Getting Started : To obtain the base URL and API key for your deployed models, run emd status in your terminal. The command will display a table with your deployed models and their details, including a link to retrieve the API key from AWS Secrets Manager. The base URL is shown at the bottom of the output. Example output: Models \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Model ID \u2502 Qwen2.5-0.5B-Instruct/dev \u2502 \u2502 Status \u2502 CREATE_COMPLETE \u2502 \u2502 Service Type \u2502 Amazon SageMaker AI Real-time inference with OpenAI Compatible API \u2502 \u2502 Instance Type \u2502 ml.g5.2xlarge \u2502 \u2502 Create Time \u2502 2025-05-08 12:27:05 UTC \u2502 \u2502 Query Model API Key \u2502 https://console.aws.amazon.com/secretsmanager/secret?name=EMD-APIKey- \u2502 \u2502 \u2502 Secrets&region=us-east-1 \u2502 \u2502 SageMakerEndpointName \u2502 EMD-Model-qwen2-5-0-5b-instruct-endpoint \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Base URL http://your-emd-endpoint.region.elb.amazonaws.com/v1 List Models Returns a list of available models. Endpoint: GET /v1/models Curl Example: curl https://BASE_URL/v1/models Python Example: from openai import OpenAI client = OpenAI ( # No API key needed for listing models base_url = \"https://BASE_URL\" ) # List available models models = client . models . list () for model in models . data : print ( model . id ) Chat Completions Create a model response for a conversation. Endpoint: POST /v1/chat/completions Parameters: model (required): ID of the model to use (e.g., \"Qwen2.5-7B-Instruct/dev\", \"Llama-3.3-70B-Instruct/dev\") messages (required): Array of message objects with role and content temperature : Sampling temperature (0-2, default: 1) top_p : Nucleus sampling parameter (0-1, default: 1) n : Number of chat completion choices to generate (default: 1) stream : Whether to stream partial progress (default: false) stop : Sequences where the API will stop generating max_tokens : Maximum number of tokens to generate presence_penalty : Penalty for new tokens based on presence (-2.0 to 2.0) frequency_penalty : Penalty for new tokens based on frequency (-2.0 to 2.0) function_call : Controls how the model responds to function calls functions : List of functions the model may generate JSON inputs for Curl Example: curl https://BASE_URL/v1/chat/completions \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen2.5-7B-Instruct/dev\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"} ], \"temperature\": 0.7 }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Create a chat completion response = client . chat . completions . create ( model = \"Qwen2.5-7B-Instruct/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], temperature = 0.7 , stream = False ) # Print the response print ( response . choices [ 0 ] . message . content ) Streaming Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Create a streaming chat completion stream = client . chat . completions . create ( model = \"Llama-3.3-70B-Instruct/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Write a short poem about AI.\" } ], stream = True ) # Process the stream for chunk in stream : if chunk . choices [ 0 ] . delta . content is not None : print ( chunk . choices [ 0 ] . delta . content , end = \"\" ) print () Embeddings Some embedding models may have additional parameters or usage guidelines specified in their official documentation. For model-specific details, please refer to the provider's documentation. Get vector representations of text. Endpoint: POST /v1/embeddings Parameters: model (required): ID of the model to use (e.g., \"bge-m3/dev\") input (required): Input text to embed or array of texts user : A unique identifier for the end-user Curl Example: curl https://BASE_URL/v1/embeddings \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"bge-m3/dev\", \"input\": \"The food was delicious and the waiter...\" }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Get embeddings for a single text response = client . embeddings . create ( model = \"bge-m3/dev\" , # Embedding model ID with tag input = \"The food was delicious and the service was excellent.\" ) # Print the embedding vector print ( response . data [ 0 ] . embedding ) # Get embeddings for multiple texts response = client . embeddings . create ( model = \"bge-m3/dev\" , # Embedding model ID with tag input = [ \"The food was delicious and the service was excellent.\" , \"The restaurant was very expensive and the food was mediocre.\" ] ) # Print the number of embeddings print ( f \"Generated { len ( response . data ) } embeddings\" ) Rerank Some reranking models may have additional parameters or usage guidelines specified in their official documentation. For model-specific details, please refer to the provider's documentation. Rerank a list of documents based on their relevance to a query. Endpoint: POST /v1/rerank Parameters: model (required): ID of the model to use (e.g., \"bge-reranker-v2-m3/dev\") query (required): The search query documents (required): List of documents to rerank max_rerank : Maximum number of documents to rerank (default: all) return_metadata : Whether to return metadata (default: false) Curl Example: curl https://BASE_URL/v1/rerank \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"bge-reranker-v2-m3/dev\", \"query\": \"What is the capital of France?\", \"documents\": [ \"Paris is the capital of France.\", \"Berlin is the capital of Germany.\", \"London is the capital of England.\" ] }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Rerank documents based on a query response = client . reranking . create ( model = \"bge-reranker-v2-m3/dev\" , # Reranking model ID with tag query = \"What is the capital of France?\" , documents = [ \"Paris is the capital of France.\" , \"Berlin is the capital of Germany.\" , \"London is the capital of England.\" ], max_rerank = 3 ) # Print the reranked documents for result in response . data : print ( f \"Document: { result . document } \" ) print ( f \"Relevance Score: { result . relevance_score } \" ) print ( \"---\" ) Invocations General-purpose endpoint for model invocations. Endpoint: POST /v1/invocations Parameters: model (required): ID of the model to use input : Input data for the model parameters : Additional parameters for the model Curl Example: curl https://BASE_URL/v1/invocations \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen2.5-7B-Instruct/dev\", \"input\": { \"query\": \"What is machine learning?\" }, \"parameters\": { \"max_tokens\": 100 } }' Python Example: import requests import json # Set up the API endpoint and headers url = \"https://BASE_URL/v1/invocations\" headers = { \"Authorization\" : \"Bearer YOUR_API_KEY\" , \"Content-Type\" : \"application/json\" } # Prepare the payload payload = { \"model\" : \"Qwen2.5-7B-Instruct/dev\" , # Model ID with tag \"input\" : { \"query\" : \"What is machine learning?\" }, \"parameters\" : { \"max_tokens\" : 100 } } # Make the API call response = requests . post ( url , headers = headers , data = json . dumps ( payload )) # Print the response print ( response . json ()) Vision Models Process images along with text prompts. Endpoint: POST /v1/chat/completions Parameters: Same as Chat Completions, but with messages that include image content. Python Example: from openai import OpenAI import base64 # Function to encode the image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Path to your image image_path = \"path/to/your/image.jpg\" base64_image = encode_image ( image_path ) client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) response = client . chat . completions . create ( model = \"Qwen2-VL-7B-Instruct/dev\" , # Vision model ID with tag messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What's in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" } } ] } ] ) print ( response . choices [ 0 ] . message . content ) Audio Transcription Transcribe audio files to text. Endpoint: POST /v1/audio/transcriptions Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) audio_file_path = \"path/to/audio.mp3\" with open ( audio_file_path , \"rb\" ) as audio_file : response = client . audio . transcriptions . create ( model = \"whisper-large-v3/dev\" , # ASR model ID with tag file = audio_file ) print ( response . text ) # Transcribed text","title":"API Reference"},{"location":"api/#api-documentation","text":"Getting Started : To obtain the base URL and API key for your deployed models, run emd status in your terminal. The command will display a table with your deployed models and their details, including a link to retrieve the API key from AWS Secrets Manager. The base URL is shown at the bottom of the output. Example output: Models \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Model ID \u2502 Qwen2.5-0.5B-Instruct/dev \u2502 \u2502 Status \u2502 CREATE_COMPLETE \u2502 \u2502 Service Type \u2502 Amazon SageMaker AI Real-time inference with OpenAI Compatible API \u2502 \u2502 Instance Type \u2502 ml.g5.2xlarge \u2502 \u2502 Create Time \u2502 2025-05-08 12:27:05 UTC \u2502 \u2502 Query Model API Key \u2502 https://console.aws.amazon.com/secretsmanager/secret?name=EMD-APIKey- \u2502 \u2502 \u2502 Secrets&region=us-east-1 \u2502 \u2502 SageMakerEndpointName \u2502 EMD-Model-qwen2-5-0-5b-instruct-endpoint \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Base URL http://your-emd-endpoint.region.elb.amazonaws.com/v1","title":"API Documentation"},{"location":"api/#list-models","text":"Returns a list of available models. Endpoint: GET /v1/models Curl Example: curl https://BASE_URL/v1/models Python Example: from openai import OpenAI client = OpenAI ( # No API key needed for listing models base_url = \"https://BASE_URL\" ) # List available models models = client . models . list () for model in models . data : print ( model . id )","title":"List Models"},{"location":"api/#chat-completions","text":"Create a model response for a conversation. Endpoint: POST /v1/chat/completions Parameters: model (required): ID of the model to use (e.g., \"Qwen2.5-7B-Instruct/dev\", \"Llama-3.3-70B-Instruct/dev\") messages (required): Array of message objects with role and content temperature : Sampling temperature (0-2, default: 1) top_p : Nucleus sampling parameter (0-1, default: 1) n : Number of chat completion choices to generate (default: 1) stream : Whether to stream partial progress (default: false) stop : Sequences where the API will stop generating max_tokens : Maximum number of tokens to generate presence_penalty : Penalty for new tokens based on presence (-2.0 to 2.0) frequency_penalty : Penalty for new tokens based on frequency (-2.0 to 2.0) function_call : Controls how the model responds to function calls functions : List of functions the model may generate JSON inputs for Curl Example: curl https://BASE_URL/v1/chat/completions \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen2.5-7B-Instruct/dev\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"} ], \"temperature\": 0.7 }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Create a chat completion response = client . chat . completions . create ( model = \"Qwen2.5-7B-Instruct/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], temperature = 0.7 , stream = False ) # Print the response print ( response . choices [ 0 ] . message . content ) Streaming Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Create a streaming chat completion stream = client . chat . completions . create ( model = \"Llama-3.3-70B-Instruct/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Write a short poem about AI.\" } ], stream = True ) # Process the stream for chunk in stream : if chunk . choices [ 0 ] . delta . content is not None : print ( chunk . choices [ 0 ] . delta . content , end = \"\" ) print ()","title":"Chat Completions"},{"location":"api/#embeddings","text":"Some embedding models may have additional parameters or usage guidelines specified in their official documentation. For model-specific details, please refer to the provider's documentation. Get vector representations of text. Endpoint: POST /v1/embeddings Parameters: model (required): ID of the model to use (e.g., \"bge-m3/dev\") input (required): Input text to embed or array of texts user : A unique identifier for the end-user Curl Example: curl https://BASE_URL/v1/embeddings \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"bge-m3/dev\", \"input\": \"The food was delicious and the waiter...\" }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Get embeddings for a single text response = client . embeddings . create ( model = \"bge-m3/dev\" , # Embedding model ID with tag input = \"The food was delicious and the service was excellent.\" ) # Print the embedding vector print ( response . data [ 0 ] . embedding ) # Get embeddings for multiple texts response = client . embeddings . create ( model = \"bge-m3/dev\" , # Embedding model ID with tag input = [ \"The food was delicious and the service was excellent.\" , \"The restaurant was very expensive and the food was mediocre.\" ] ) # Print the number of embeddings print ( f \"Generated { len ( response . data ) } embeddings\" )","title":"Embeddings"},{"location":"api/#rerank","text":"Some reranking models may have additional parameters or usage guidelines specified in their official documentation. For model-specific details, please refer to the provider's documentation. Rerank a list of documents based on their relevance to a query. Endpoint: POST /v1/rerank Parameters: model (required): ID of the model to use (e.g., \"bge-reranker-v2-m3/dev\") query (required): The search query documents (required): List of documents to rerank max_rerank : Maximum number of documents to rerank (default: all) return_metadata : Whether to return metadata (default: false) Curl Example: curl https://BASE_URL/v1/rerank \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"bge-reranker-v2-m3/dev\", \"query\": \"What is the capital of France?\", \"documents\": [ \"Paris is the capital of France.\", \"Berlin is the capital of Germany.\", \"London is the capital of England.\" ] }' Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) # Rerank documents based on a query response = client . reranking . create ( model = \"bge-reranker-v2-m3/dev\" , # Reranking model ID with tag query = \"What is the capital of France?\" , documents = [ \"Paris is the capital of France.\" , \"Berlin is the capital of Germany.\" , \"London is the capital of England.\" ], max_rerank = 3 ) # Print the reranked documents for result in response . data : print ( f \"Document: { result . document } \" ) print ( f \"Relevance Score: { result . relevance_score } \" ) print ( \"---\" )","title":"Rerank"},{"location":"api/#invocations","text":"General-purpose endpoint for model invocations. Endpoint: POST /v1/invocations Parameters: model (required): ID of the model to use input : Input data for the model parameters : Additional parameters for the model Curl Example: curl https://BASE_URL/v1/invocations \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen2.5-7B-Instruct/dev\", \"input\": { \"query\": \"What is machine learning?\" }, \"parameters\": { \"max_tokens\": 100 } }' Python Example: import requests import json # Set up the API endpoint and headers url = \"https://BASE_URL/v1/invocations\" headers = { \"Authorization\" : \"Bearer YOUR_API_KEY\" , \"Content-Type\" : \"application/json\" } # Prepare the payload payload = { \"model\" : \"Qwen2.5-7B-Instruct/dev\" , # Model ID with tag \"input\" : { \"query\" : \"What is machine learning?\" }, \"parameters\" : { \"max_tokens\" : 100 } } # Make the API call response = requests . post ( url , headers = headers , data = json . dumps ( payload )) # Print the response print ( response . json ())","title":"Invocations"},{"location":"api/#vision-models","text":"Process images along with text prompts. Endpoint: POST /v1/chat/completions Parameters: Same as Chat Completions, but with messages that include image content. Python Example: from openai import OpenAI import base64 # Function to encode the image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Path to your image image_path = \"path/to/your/image.jpg\" base64_image = encode_image ( image_path ) client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) response = client . chat . completions . create ( model = \"Qwen2-VL-7B-Instruct/dev\" , # Vision model ID with tag messages = [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What's in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" } } ] } ] ) print ( response . choices [ 0 ] . message . content )","title":"Vision Models"},{"location":"api/#audio-transcription","text":"Transcribe audio files to text. Endpoint: POST /v1/audio/transcriptions Python Example: from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://BASE_URL\" ) audio_file_path = \"path/to/audio.mp3\" with open ( audio_file_path , \"rb\" ) as audio_file : response = client . audio . transcriptions . create ( model = \"whisper-large-v3/dev\" , # ASR model ID with tag file = audio_file ) print ( response . text ) # Transcribed text","title":"Audio Transcription"},{"location":"architecture/","text":"Architecture EMD deploys models to AWS using a simple three-step process: User/Client initiates model deployment task, triggering pipeline to start model building. AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR. AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS). Key AWS Services Used CodePipeline : Orchestrates the entire workflow CodeBuild : Builds model containers CloudFormation : Provisions infrastructure ECR : Stores model containers Target Services : SageMaker, EC2, or ECS hosts your model EMD handles all IAM permissions and security configurations automatically. Model Deployment Cost Estimation EMD leverages several AWS services to deploy models. Below is an estimated cost breakdown for deploying a single model (assuming a 5GB model file and 10-minute CodeBuild execution). US East (N. Virginia) Region Cost Estimation Service Usage Estimated Cost (USD) Notes S3 Storage 5GB model file $0.00/month $0.023 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months CodeBuild BUILD_GENERAL1_LARGE for 10 minutes $0.10 $0.005 per build-minute CodePipeline 1 pipeline execution $0.00 First pipeline is free, then $1.00 per active pipeline/month CloudFormation Stack creation $0.00 No charge for CloudFormation service ECR ~2GB Docker image $0.10/month $0.10 per GB-month for private repository storage Total Deployment Cost $0.10 + $0.10/month One-time deployment cost + monthly storage Target Service Costs (Post-Deployment) SageMaker : ml.g4dn.xlarge: ~$0.736/hour EC2 : g4dn.xlarge: ~$0.526/hour ECS : Fargate or EC2 costs for container hosting Secrets Manager : $0.40/month for API key storage China North (Beijing) Region Cost Estimation Service Usage Estimated Cost (CNY) Notes S3 Storage 5GB model file \u00a50.00/month \u00a50.21 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months (verify availability in China regions) CodeBuild BUILD_GENERAL1_LARGE for 10 minutes \u00a50.80 \u00a50.08 per build-minute CodePipeline 1 pipeline execution \u00a50.00 First pipeline is free, then \u00a57.00 per active pipeline/month CloudFormation Stack creation \u00a50.00 No charge for CloudFormation service ECR ~2GB Docker image \u00a50.84/month \u00a50.42 per GB-month for private repository storage Total Deployment Cost \u00a50.80 + \u00a50.84/month One-time deployment cost + monthly storage Target Service Costs (Post-Deployment) SageMaker : ml.g4dn.xlarge: ~\u00a56.18/hour EC2 : g4dn.xlarge: ~\u00a54.42/hour ECS : Fargate or EC2 costs for container hosting Secrets Manager : \u00a53.36/month for API key storage Note : All prices are estimates as of 2024. Actual costs may vary based on your specific AWS region, usage patterns, and any applicable discounts. We recommend using AWS Cost Explorer to monitor and forecast your actual costs. Security Considerations API Key Authentication EMD supports API key authentication for securing access to your deployed models: Setting Up API Keys Using Command Line: emd deploy --model-id <model-id> --instance-type <instance-type> --engine-type <engine-type> --service-type <service-type> --extra-params '{ \"service_params\": { \"api_key\": \"your-secure-api-key\" } }' Using Interactive CLI: When prompted for \"Extra Parameters\" during emd deploy , enter: { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } Managing API Keys Storage : Keys are securely stored in AWS Secrets Manager Access : Keys can be retrieved from the AWS Secrets Manager console Rotation : Update keys periodically through Secrets Manager or by redeploying Using API Keys Include the API key in your requests to the model endpoint: curl -X POST https://your-endpoint.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{ \"model\": \"your-model-id/tag\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}] }' With Python: import openai client = openai . OpenAI ( base_url = \"https://your-endpoint.com\" , api_key = \"YOUR_API_KEY\" ) response = client . chat . completions . create ( model = \"your-model-id/tag\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] ) print ( response . choices [ 0 ] . message . content ) Security Best Practices Enable HTTPS : # Create certificate in AWS Certificate Manager aws acm request-certificate --domain-name your-model-endpoint.com # Update ALB listener to use HTTPS aws elbv2 create-listener --load-balancer-arn <your-alb-arn> \\ --protocol HTTPS --port 443 \\ --certificates CertificateArn=<certificate-arn> \\ --ssl-policy ELBSecurityPolicy-TLS13-1-2-2021-06 \\ --default-actions Type=forward,TargetGroupArn=<target-group-arn> Rotate API keys regularly : Update keys in AWS Secrets Manager Redeploy models with new keys or update existing keys Implement network isolation when needed: Deploy in private subnets with NAT gateway Use VPC endpoints for AWS services","title":"Architecture"},{"location":"architecture/#architecture","text":"EMD deploys models to AWS using a simple three-step process: User/Client initiates model deployment task, triggering pipeline to start model building. AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR. AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).","title":"Architecture"},{"location":"architecture/#key-aws-services-used","text":"CodePipeline : Orchestrates the entire workflow CodeBuild : Builds model containers CloudFormation : Provisions infrastructure ECR : Stores model containers Target Services : SageMaker, EC2, or ECS hosts your model EMD handles all IAM permissions and security configurations automatically.","title":"Key AWS Services Used"},{"location":"architecture/#model-deployment-cost-estimation","text":"EMD leverages several AWS services to deploy models. Below is an estimated cost breakdown for deploying a single model (assuming a 5GB model file and 10-minute CodeBuild execution).","title":"Model Deployment Cost Estimation"},{"location":"architecture/#us-east-n-virginia-region-cost-estimation","text":"Service Usage Estimated Cost (USD) Notes S3 Storage 5GB model file $0.00/month $0.023 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months CodeBuild BUILD_GENERAL1_LARGE for 10 minutes $0.10 $0.005 per build-minute CodePipeline 1 pipeline execution $0.00 First pipeline is free, then $1.00 per active pipeline/month CloudFormation Stack creation $0.00 No charge for CloudFormation service ECR ~2GB Docker image $0.10/month $0.10 per GB-month for private repository storage Total Deployment Cost $0.10 + $0.10/month One-time deployment cost + monthly storage","title":"US East (N. Virginia) Region Cost Estimation"},{"location":"architecture/#target-service-costs-post-deployment","text":"SageMaker : ml.g4dn.xlarge: ~$0.736/hour EC2 : g4dn.xlarge: ~$0.526/hour ECS : Fargate or EC2 costs for container hosting Secrets Manager : $0.40/month for API key storage","title":"Target Service Costs (Post-Deployment)"},{"location":"architecture/#china-north-beijing-region-cost-estimation","text":"Service Usage Estimated Cost (CNY) Notes S3 Storage 5GB model file \u00a50.00/month \u00a50.21 per GB-month for standard storage. Free tier includes 5GB of S3 standard storage for 12 months (verify availability in China regions) CodeBuild BUILD_GENERAL1_LARGE for 10 minutes \u00a50.80 \u00a50.08 per build-minute CodePipeline 1 pipeline execution \u00a50.00 First pipeline is free, then \u00a57.00 per active pipeline/month CloudFormation Stack creation \u00a50.00 No charge for CloudFormation service ECR ~2GB Docker image \u00a50.84/month \u00a50.42 per GB-month for private repository storage Total Deployment Cost \u00a50.80 + \u00a50.84/month One-time deployment cost + monthly storage","title":"China North (Beijing) Region Cost Estimation"},{"location":"architecture/#target-service-costs-post-deployment_1","text":"SageMaker : ml.g4dn.xlarge: ~\u00a56.18/hour EC2 : g4dn.xlarge: ~\u00a54.42/hour ECS : Fargate or EC2 costs for container hosting Secrets Manager : \u00a53.36/month for API key storage Note : All prices are estimates as of 2024. Actual costs may vary based on your specific AWS region, usage patterns, and any applicable discounts. We recommend using AWS Cost Explorer to monitor and forecast your actual costs.","title":"Target Service Costs (Post-Deployment)"},{"location":"architecture/#security-considerations","text":"","title":"Security Considerations"},{"location":"architecture/#api-key-authentication","text":"EMD supports API key authentication for securing access to your deployed models:","title":"API Key Authentication"},{"location":"architecture/#setting-up-api-keys","text":"Using Command Line: emd deploy --model-id <model-id> --instance-type <instance-type> --engine-type <engine-type> --service-type <service-type> --extra-params '{ \"service_params\": { \"api_key\": \"your-secure-api-key\" } }' Using Interactive CLI: When prompted for \"Extra Parameters\" during emd deploy , enter: { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } }","title":"Setting Up API Keys"},{"location":"architecture/#managing-api-keys","text":"Storage : Keys are securely stored in AWS Secrets Manager Access : Keys can be retrieved from the AWS Secrets Manager console Rotation : Update keys periodically through Secrets Manager or by redeploying","title":"Managing API Keys"},{"location":"architecture/#using-api-keys","text":"Include the API key in your requests to the model endpoint: curl -X POST https://your-endpoint.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{ \"model\": \"your-model-id/tag\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}] }' With Python: import openai client = openai . OpenAI ( base_url = \"https://your-endpoint.com\" , api_key = \"YOUR_API_KEY\" ) response = client . chat . completions . create ( model = \"your-model-id/tag\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] ) print ( response . choices [ 0 ] . message . content )","title":"Using API Keys"},{"location":"architecture/#security-best-practices","text":"Enable HTTPS : # Create certificate in AWS Certificate Manager aws acm request-certificate --domain-name your-model-endpoint.com # Update ALB listener to use HTTPS aws elbv2 create-listener --load-balancer-arn <your-alb-arn> \\ --protocol HTTPS --port 443 \\ --certificates CertificateArn=<certificate-arn> \\ --ssl-policy ELBSecurityPolicy-TLS13-1-2-2021-06 \\ --default-actions Type=forward,TargetGroupArn=<target-group-arn> Rotate API keys regularly : Update keys in AWS Secrets Manager Redeploy models with new keys or update existing keys Implement network isolation when needed: Deploy in private subnets with NAT gateway Use VPC endpoints for AWS services","title":"Security Best Practices"},{"location":"best_deployment_practices/","text":"Best Deployment Practices This document provides examples of best practices for deploying models using EMD for various use cases. Example Model Deployments Qwen 3 Series emd deploy --model-id Qwen3-30B-A3B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id Qwen3-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id Qwen3-8B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime GLM Z1/0414 Series emd deploy --model-id GLM-Z1-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id GLM-4-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime Mistral Small Series emd deploy --model-id Mistral-Small-3.1-24B-Instruct-2503 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime Gemma 3 Series emd deploy --model-id gemma-3-27b-it --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime Qwen Series Qwen2.5-VL-32B-Instruct emd deploy --model-id Qwen2.5-VL-32B-Instruct --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime QwQ-32B emd deploy --model-id QwQ-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime Deploying to Specific GPU Types Choosing the right GPU type is critical for optimal performance and cost-efficiency. Use the --instance-type parameter to specify the GPU instance. Example: Deploying Qwen2.5-7B on g5.2xlarge emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.2xlarge --engine-type vllm --service-type sagemaker_realtime Achieving Longer Context Windows To enable longer context windows, use the --extra-params option with engine-specific parameters. Example: Deploying model with 16k context window emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.4xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{ \"engine_params\": { \"cli_args\": \"--max_model_len 16000 --max_num_seqs 4\" } }' Example: Deploying model on G4dn instance emd deploy --model-id Qwen2.5-14B-Instruct-AWQ --instance-type g4dn.2xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{ \"engine_params\": { \"environment_variables\": \"export VLLM_ATTENTION_BACKEND=XFORMERS && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\", \"default_cli_args\": \" --chat-template emd/models/chat_templates/qwen_2d5_add_prefill_chat_template.jinja --max_model_len 12000 --max_num_seqs 10 --gpu_memory_utilization 0.95 --disable-log-stats --enable-auto-tool-choice --tool-call-parser hermes\" } }' Extra Parameters Usage The --extra-params option allows you to customize various aspects of your deployment. This section provides a detailed reference for the available parameters organized by category. Parameter Structure The extra parameters are structured as a JSON object with the following top-level categories: { \"model_params\" : {}, \"service_params\" : {}, \"instance_params\" : {}, \"engine_params\" : {}, \"framework_params\" : {} } Model Parameters Model parameters control how the model is loaded and prepared. Model Source Configuration { \"model_params\" : { \"model_files_s3_path\" : \"s3://your-bucket/model-path\" , \"model_files_local_path\" : \"/path/to/local/model\" , \"model_files_download_source\" : \"huggingface|modelscope|auto\" , \"huggingface_model_id\" : \"organization/model-name\" , \"modelscope_model_id\" : \"organization/model-name\" , \"need_prepare_model\" : false } } model_files_s3_path : Load model directly from an S3 path model_files_local_path : Load model from a local path (only for local deployment) model_files_download_source : Specify the source for downloading model files huggingface_model_id : Specify a custom Hugging Face model ID modelscope_model_id : Specify a custom ModelScope model ID need_prepare_model : Set to false to skip downloading and uploading model files (reduces deployment time) Service Parameters Service parameters configure the deployment service behavior. API Security { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } api_key : Set a custom API key for securing access to your model endpoint SageMaker-specific Parameters { \"service_params\" : { \"max_capacity\" : 2 , \"min_capacity\" : 1 , \"auto_scaling_target_value\" : 15 , \"sagemaker_endpoint_name\" : \"custom-endpoint-name\" } } max_capacity : Maximum number of instances for auto-scaling min_capacity : Minimum number of instances for auto-scaling auto_scaling_target_value : Target value for auto-scaling (in requests per minute) sagemaker_endpoint_name : Custom name for the SageMaker endpoint ECS-specific Parameters { \"service_params\" : { \"desired_capacity\" : 2 , \"max_size\" : 4 , \"vpc_id\" : \"vpc-12345\" , \"subnet_ids\" : \"subnet-12345,subnet-67890\" } } desired_capacity : Desired number of ECS tasks max_size : Maximum number of ECS tasks for auto-scaling vpc_id : Custom VPC ID for deployment subnet_ids : Comma-separated list of subnet IDs Engine Parameters Engine parameters control the behavior of the inference engine. Common Engine Parameters { \"engine_params\" : { \"environment_variables\" : \"export VAR1=value1 && export VAR2=value2\" , \"cli_args\" : \"--specific-engine-arg value\" , \"default_cli_args\" : \"--common-engine-arg value\" } } environment_variables : Set environment variables for the engine cli_args : Specific command line arguments for the engine default_cli_args : Default command line arguments for the engine vLLM-specific Parameters { \"engine_params\" : { \"cli_args\" : \"--max_model_len 16000 --max_num_seqs 4 --gpu_memory_utilization 0.9\" , \"environment_variables\" : \"export VLLM_ATTENTION_BACKEND=FLASHINFER && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\" } } cli_args : Command line arguments specific to vLLM Common vLLM parameters: --max_model_len : Maximum context length --max_num_seqs : Maximum number of sequences --gpu_memory_utilization : GPU memory utilization (0.0-1.0) --disable-log-stats : Disable logging of statistics --enable-auto-tool-choice : Enable automatic tool choice --tool-call-parser : Specify tool call parser (e.g., hermes, pythonic) --enable-reasoning : Enable reasoning capabilities --reasoning-parser : Specify reasoning parser (e.g., deepseek_r1, granite) --chat-template : Path to chat template file TGI-specific Parameters { \"engine_params\" : { \"default_cli_args\" : \"--max-total-tokens 30000 --max-concurrent-requests 30\" } } Common TGI parameters: --max-total-tokens : Maximum total tokens --max-concurrent-requests : Maximum concurrent requests --max-batch-size : Maximum batch size --max-input-tokens : Maximum input tokens Framework Parameters Framework parameters configure the web framework serving the model. FastAPI Parameters { \"framework_params\" : { \"limit_concurrency\" : 200 , \"timeout_keep_alive\" : 120 , \"uvicorn_log_level\" : \"info\" } } limit_concurrency : Maximum number of concurrent connections timeout_keep_alive : Timeout for keeping connections alive (in seconds) uvicorn_log_level : Log level for Uvicorn server (debug, info, warning, error, critical) Example Configurations Example: High-throughput Configuration { \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 8000 --max_num_seqs 20 --gpu_memory_utilization 0.95\" }, \"framework_params\" : { \"limit_concurrency\" : 500 , \"timeout_keep_alive\" : 30 }, \"service_params\" : { \"max_capacity\" : 3 , \"min_capacity\" : 1 , \"auto_scaling_target_value\" : 20 } } Example: Long Context Configuration { \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 32000 --max_num_seqs 2 --gpu_memory_utilization 0.9\" }, \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } Example: Secure API with Custom Endpoint Name { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" , \"sagemaker_endpoint_name\" : \"my-custom-llm-endpoint\" } } Model Source Configuration You can load models from different locations by adding appropriate values in the extra-params parameter: Load model from S3 { \"model_params\" :{ \"model_files_s3_path\" : \"<S3_PATH>\" } } Load model from local path (only applicable for local deployment) { \"model_params\" : { \"model_files_local_path\" : \"<LOCAL_PATH>\" } } Skip downloading and uploading model files in codebuild, which will significantly reduce deployment time { \"model_params\" : { \"need_prepare_model\" : false } } Specify the download source for model files { \"model_params\" :{ \"model_files_download_source\" : \"huggingface|modelscope|auto(default)\" } } Specify the model ID on huggingface or modelscope { \"model_params\" : { \"huggingface_model_id\" : \"model id on huggingface\" , \"modelscope_model_id\" : \"model id on modelscope\" } } Environmental variables LOCAL_DEPLOY_PORT: Local deployment port, default: 8080 Common Troubleshooting This section covers common issues you might encounter during model deployment and their solutions. Memory-Related Issues If your deployment fails with out-of-memory (OOM) errors: CUDA out of memory. Tried to allocate X.XX GiB Try these solutions: Use a larger instance type : Upgrade to an instance with more GPU memory (e.g., from g5.2xlarge to g5.4xlarge) For large models (>30B parameters), consider using multiple GPUs with g5.12xlarge or g5.48xlarge Adjust engine parameters : { \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 8000 --max_num_seqs 4 --gpu_memory_utilization 0.8\" } } Reduce max_model_len to decrease context window size Lower max_num_seqs to reduce concurrent sequences Set gpu_memory_utilization to a lower value (e.g., 0.8 instead of the default 0.9) Use quantized models : Deploy AWQ or GPTQ quantized versions of models (e.g., Qwen2.5-7B-Instruct-AWQ instead of Qwen2.5-7B-Instruct) Deployment Timeout Issues If your deployment times out during model preparation: Skip model preparation : { \"model_params\" : { \"need_prepare_model\" : false } } Use pre-downloaded models : { \"model_params\" : { \"model_files_s3_path\" : \"s3://your-bucket/model-path\" } } API Connection Issues If you can't connect to your deployed model's API: Check endpoint status : emd status Ensure the status is \"InService\" or \"Running\" Verify API key : Ensure you're using the correct API key in your requests If you've set a custom API key, make sure to include it in your requests Test with curl : curl -X POST https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer your-api-key\" \\ -d '{\"model\": \"your-model-id\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}' Performance Optimization If your model is running slowly: Increase GPU utilization : { \"engine_params\" : { \"default_cli_args\" : \"--gpu_memory_utilization 0.95\" } } Adjust batch size and concurrency : { \"engine_params\" : { \"default_cli_args\" : \"--max_num_seqs 20\" }, \"framework_params\" : { \"limit_concurrency\" : 200 } } Enable optimizations (for vLLM): { \"engine_params\" : { \"environment_variables\" : \"export VLLM_ATTENTION_BACKEND=FLASHINFER && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\" } }","title":"Best Deployment Practices"},{"location":"best_deployment_practices/#best-deployment-practices","text":"This document provides examples of best practices for deploying models using EMD for various use cases.","title":"Best Deployment Practices"},{"location":"best_deployment_practices/#example-model-deployments","text":"","title":"Example Model Deployments"},{"location":"best_deployment_practices/#qwen-3-series","text":"emd deploy --model-id Qwen3-30B-A3B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id Qwen3-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id Qwen3-8B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"Qwen 3 Series"},{"location":"best_deployment_practices/#glm-z10414-series","text":"emd deploy --model-id GLM-Z1-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime emd deploy --model-id GLM-4-32B-0414 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"GLM Z1/0414 Series"},{"location":"best_deployment_practices/#mistral-small-series","text":"emd deploy --model-id Mistral-Small-3.1-24B-Instruct-2503 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"Mistral Small Series"},{"location":"best_deployment_practices/#gemma-3-series","text":"emd deploy --model-id gemma-3-27b-it --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"Gemma 3 Series"},{"location":"best_deployment_practices/#qwen-series","text":"","title":"Qwen Series"},{"location":"best_deployment_practices/#qwen25-vl-32b-instruct","text":"emd deploy --model-id Qwen2.5-VL-32B-Instruct --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"Qwen2.5-VL-32B-Instruct"},{"location":"best_deployment_practices/#qwq-32b","text":"emd deploy --model-id QwQ-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime","title":"QwQ-32B"},{"location":"best_deployment_practices/#deploying-to-specific-gpu-types","text":"Choosing the right GPU type is critical for optimal performance and cost-efficiency. Use the --instance-type parameter to specify the GPU instance.","title":"Deploying to Specific GPU Types"},{"location":"best_deployment_practices/#example-deploying-qwen25-7b-on-g52xlarge","text":"emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.2xlarge --engine-type vllm --service-type sagemaker_realtime","title":"Example: Deploying Qwen2.5-7B on g5.2xlarge"},{"location":"best_deployment_practices/#achieving-longer-context-windows","text":"To enable longer context windows, use the --extra-params option with engine-specific parameters.","title":"Achieving Longer Context Windows"},{"location":"best_deployment_practices/#example-deploying-model-with-16k-context-window","text":"emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.4xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{ \"engine_params\": { \"cli_args\": \"--max_model_len 16000 --max_num_seqs 4\" } }'","title":"Example: Deploying model with 16k context window"},{"location":"best_deployment_practices/#example-deploying-model-on-g4dn-instance","text":"emd deploy --model-id Qwen2.5-14B-Instruct-AWQ --instance-type g4dn.2xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{ \"engine_params\": { \"environment_variables\": \"export VLLM_ATTENTION_BACKEND=XFORMERS && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\", \"default_cli_args\": \" --chat-template emd/models/chat_templates/qwen_2d5_add_prefill_chat_template.jinja --max_model_len 12000 --max_num_seqs 10 --gpu_memory_utilization 0.95 --disable-log-stats --enable-auto-tool-choice --tool-call-parser hermes\" } }'","title":"Example: Deploying model on G4dn instance"},{"location":"best_deployment_practices/#extra-parameters-usage","text":"The --extra-params option allows you to customize various aspects of your deployment. This section provides a detailed reference for the available parameters organized by category.","title":"Extra Parameters Usage"},{"location":"best_deployment_practices/#parameter-structure","text":"The extra parameters are structured as a JSON object with the following top-level categories: { \"model_params\" : {}, \"service_params\" : {}, \"instance_params\" : {}, \"engine_params\" : {}, \"framework_params\" : {} }","title":"Parameter Structure"},{"location":"best_deployment_practices/#model-parameters","text":"Model parameters control how the model is loaded and prepared.","title":"Model Parameters"},{"location":"best_deployment_practices/#model-source-configuration","text":"{ \"model_params\" : { \"model_files_s3_path\" : \"s3://your-bucket/model-path\" , \"model_files_local_path\" : \"/path/to/local/model\" , \"model_files_download_source\" : \"huggingface|modelscope|auto\" , \"huggingface_model_id\" : \"organization/model-name\" , \"modelscope_model_id\" : \"organization/model-name\" , \"need_prepare_model\" : false } } model_files_s3_path : Load model directly from an S3 path model_files_local_path : Load model from a local path (only for local deployment) model_files_download_source : Specify the source for downloading model files huggingface_model_id : Specify a custom Hugging Face model ID modelscope_model_id : Specify a custom ModelScope model ID need_prepare_model : Set to false to skip downloading and uploading model files (reduces deployment time)","title":"Model Source Configuration"},{"location":"best_deployment_practices/#service-parameters","text":"Service parameters configure the deployment service behavior.","title":"Service Parameters"},{"location":"best_deployment_practices/#api-security","text":"{ \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } api_key : Set a custom API key for securing access to your model endpoint","title":"API Security"},{"location":"best_deployment_practices/#sagemaker-specific-parameters","text":"{ \"service_params\" : { \"max_capacity\" : 2 , \"min_capacity\" : 1 , \"auto_scaling_target_value\" : 15 , \"sagemaker_endpoint_name\" : \"custom-endpoint-name\" } } max_capacity : Maximum number of instances for auto-scaling min_capacity : Minimum number of instances for auto-scaling auto_scaling_target_value : Target value for auto-scaling (in requests per minute) sagemaker_endpoint_name : Custom name for the SageMaker endpoint","title":"SageMaker-specific Parameters"},{"location":"best_deployment_practices/#ecs-specific-parameters","text":"{ \"service_params\" : { \"desired_capacity\" : 2 , \"max_size\" : 4 , \"vpc_id\" : \"vpc-12345\" , \"subnet_ids\" : \"subnet-12345,subnet-67890\" } } desired_capacity : Desired number of ECS tasks max_size : Maximum number of ECS tasks for auto-scaling vpc_id : Custom VPC ID for deployment subnet_ids : Comma-separated list of subnet IDs","title":"ECS-specific Parameters"},{"location":"best_deployment_practices/#engine-parameters","text":"Engine parameters control the behavior of the inference engine.","title":"Engine Parameters"},{"location":"best_deployment_practices/#common-engine-parameters","text":"{ \"engine_params\" : { \"environment_variables\" : \"export VAR1=value1 && export VAR2=value2\" , \"cli_args\" : \"--specific-engine-arg value\" , \"default_cli_args\" : \"--common-engine-arg value\" } } environment_variables : Set environment variables for the engine cli_args : Specific command line arguments for the engine default_cli_args : Default command line arguments for the engine","title":"Common Engine Parameters"},{"location":"best_deployment_practices/#vllm-specific-parameters","text":"{ \"engine_params\" : { \"cli_args\" : \"--max_model_len 16000 --max_num_seqs 4 --gpu_memory_utilization 0.9\" , \"environment_variables\" : \"export VLLM_ATTENTION_BACKEND=FLASHINFER && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\" } } cli_args : Command line arguments specific to vLLM Common vLLM parameters: --max_model_len : Maximum context length --max_num_seqs : Maximum number of sequences --gpu_memory_utilization : GPU memory utilization (0.0-1.0) --disable-log-stats : Disable logging of statistics --enable-auto-tool-choice : Enable automatic tool choice --tool-call-parser : Specify tool call parser (e.g., hermes, pythonic) --enable-reasoning : Enable reasoning capabilities --reasoning-parser : Specify reasoning parser (e.g., deepseek_r1, granite) --chat-template : Path to chat template file","title":"vLLM-specific Parameters"},{"location":"best_deployment_practices/#tgi-specific-parameters","text":"{ \"engine_params\" : { \"default_cli_args\" : \"--max-total-tokens 30000 --max-concurrent-requests 30\" } } Common TGI parameters: --max-total-tokens : Maximum total tokens --max-concurrent-requests : Maximum concurrent requests --max-batch-size : Maximum batch size --max-input-tokens : Maximum input tokens","title":"TGI-specific Parameters"},{"location":"best_deployment_practices/#framework-parameters","text":"Framework parameters configure the web framework serving the model.","title":"Framework Parameters"},{"location":"best_deployment_practices/#fastapi-parameters","text":"{ \"framework_params\" : { \"limit_concurrency\" : 200 , \"timeout_keep_alive\" : 120 , \"uvicorn_log_level\" : \"info\" } } limit_concurrency : Maximum number of concurrent connections timeout_keep_alive : Timeout for keeping connections alive (in seconds) uvicorn_log_level : Log level for Uvicorn server (debug, info, warning, error, critical)","title":"FastAPI Parameters"},{"location":"best_deployment_practices/#example-configurations","text":"","title":"Example Configurations"},{"location":"best_deployment_practices/#example-high-throughput-configuration","text":"{ \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 8000 --max_num_seqs 20 --gpu_memory_utilization 0.95\" }, \"framework_params\" : { \"limit_concurrency\" : 500 , \"timeout_keep_alive\" : 30 }, \"service_params\" : { \"max_capacity\" : 3 , \"min_capacity\" : 1 , \"auto_scaling_target_value\" : 20 } }","title":"Example: High-throughput Configuration"},{"location":"best_deployment_practices/#example-long-context-configuration","text":"{ \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 32000 --max_num_seqs 2 --gpu_memory_utilization 0.9\" }, \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } }","title":"Example: Long Context Configuration"},{"location":"best_deployment_practices/#example-secure-api-with-custom-endpoint-name","text":"{ \"service_params\" : { \"api_key\" : \"your-secure-api-key\" , \"sagemaker_endpoint_name\" : \"my-custom-llm-endpoint\" } }","title":"Example: Secure API with Custom Endpoint Name"},{"location":"best_deployment_practices/#model-source-configuration_1","text":"You can load models from different locations by adding appropriate values in the extra-params parameter: Load model from S3 { \"model_params\" :{ \"model_files_s3_path\" : \"<S3_PATH>\" } } Load model from local path (only applicable for local deployment) { \"model_params\" : { \"model_files_local_path\" : \"<LOCAL_PATH>\" } } Skip downloading and uploading model files in codebuild, which will significantly reduce deployment time { \"model_params\" : { \"need_prepare_model\" : false } } Specify the download source for model files { \"model_params\" :{ \"model_files_download_source\" : \"huggingface|modelscope|auto(default)\" } } Specify the model ID on huggingface or modelscope { \"model_params\" : { \"huggingface_model_id\" : \"model id on huggingface\" , \"modelscope_model_id\" : \"model id on modelscope\" } }","title":"Model Source Configuration"},{"location":"best_deployment_practices/#environmental-variables","text":"LOCAL_DEPLOY_PORT: Local deployment port, default: 8080","title":"Environmental variables"},{"location":"best_deployment_practices/#common-troubleshooting","text":"This section covers common issues you might encounter during model deployment and their solutions.","title":"Common Troubleshooting"},{"location":"best_deployment_practices/#memory-related-issues","text":"If your deployment fails with out-of-memory (OOM) errors: CUDA out of memory. Tried to allocate X.XX GiB Try these solutions: Use a larger instance type : Upgrade to an instance with more GPU memory (e.g., from g5.2xlarge to g5.4xlarge) For large models (>30B parameters), consider using multiple GPUs with g5.12xlarge or g5.48xlarge Adjust engine parameters : { \"engine_params\" : { \"default_cli_args\" : \"--max_model_len 8000 --max_num_seqs 4 --gpu_memory_utilization 0.8\" } } Reduce max_model_len to decrease context window size Lower max_num_seqs to reduce concurrent sequences Set gpu_memory_utilization to a lower value (e.g., 0.8 instead of the default 0.9) Use quantized models : Deploy AWQ or GPTQ quantized versions of models (e.g., Qwen2.5-7B-Instruct-AWQ instead of Qwen2.5-7B-Instruct)","title":"Memory-Related Issues"},{"location":"best_deployment_practices/#deployment-timeout-issues","text":"If your deployment times out during model preparation: Skip model preparation : { \"model_params\" : { \"need_prepare_model\" : false } } Use pre-downloaded models : { \"model_params\" : { \"model_files_s3_path\" : \"s3://your-bucket/model-path\" } }","title":"Deployment Timeout Issues"},{"location":"best_deployment_practices/#api-connection-issues","text":"If you can't connect to your deployed model's API: Check endpoint status : emd status Ensure the status is \"InService\" or \"Running\" Verify API key : Ensure you're using the correct API key in your requests If you've set a custom API key, make sure to include it in your requests Test with curl : curl -X POST https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer your-api-key\" \\ -d '{\"model\": \"your-model-id\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'","title":"API Connection Issues"},{"location":"best_deployment_practices/#performance-optimization","text":"If your model is running slowly: Increase GPU utilization : { \"engine_params\" : { \"default_cli_args\" : \"--gpu_memory_utilization 0.95\" } } Adjust batch size and concurrency : { \"engine_params\" : { \"default_cli_args\" : \"--max_num_seqs 20\" }, \"framework_params\" : { \"limit_concurrency\" : 200 } } Enable optimizations (for vLLM): { \"engine_params\" : { \"environment_variables\" : \"export VLLM_ATTENTION_BACKEND=FLASHINFER && export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\" } }","title":"Performance Optimization"},{"location":"commands/","text":"CLI Commands This document provides a comprehensive guide to the command-line interface (CLI) commands available in the Easy Model Deployer (EMD) tool. Overview EMD provides the following main commands: Command Description bootstrap Initialize AWS resources for model deployment deploy Deploy models to AWS infrastructure status Display status of deployed models invoke Test deployed models with sample requests example Generate sample code for API integration destroy Remove deployed models and clean up resources list-supported-models Display available models profile Configure AWS profile credentials version Display tool version information Command Details bootstrap Initialize AWS resources required for model deployment. emd bootstrap [ OPTIONS ] Options: Option Description --skip-confirm Skip confirmation prompts Example: emd bootstrap This command creates the necessary AWS resources, including an S3 bucket and CloudFormation stack, required for model deployment. deploy Deploy models to AWS infrastructure. emd deploy [ OPTIONS ] Options: Option Description --model-id TEXT Model ID to deploy -i, --instance-type TEXT The instance type to use -e, --engine-type TEXT The name of the inference engine -s, --service-type TEXT The name of the service --framework-type TEXT The name of the framework --model-tag TEXT Custom tag for the model deployment --extra-params TEXT Extra parameters in JSON format --skip-confirm Skip confirmation prompts --force-update-env-stack Force update environment stack --allow-local-deploy Allow local instance deployment --only-allow-local-deploy Only allow local instance deployment --dockerfile-local-path TEXT Custom Dockerfile path for building the model image --local-gpus TEXT Local GPU IDs to deploy the model (e.g., 0,1,2 ) Examples: Deploy a model with interactive prompts: emd deploy Deploy a specific model with parameters: emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.2xlarge --engine-type vllm --service-type sagemaker_realtime Deploy a model locally: emd deploy --allow-local-deploy Deploy with custom parameters: emd deploy --model-id Qwen2.5-7B-Instruct --extra-params '{\"engine_params\": {\"cli_args\": \"--max_model_len 16000 --max_num_seqs 4\"}}' status Display the status of deployed models. emd status [ MODEL_ID ] [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Optional model ID to check status for MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Check status of all deployed models: emd status Check status of a specific model: emd status Qwen2.5-7B-Instruct Check status of a specific model with a custom tag: emd status Qwen2.5-7B-Instruct custom-tag invoke Test deployed models with sample requests. emd invoke MODEL_ID [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Model ID to invoke MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Invoke a model: emd invoke DeepSeek-R1-Distill-Qwen-7B Invoke a model with a custom tag: emd invoke DeepSeek-R1-Distill-Qwen-7B custom-tag example Generate sample code for API integration with a deployed model. emd example MODEL_ID/MODEL_TAG Arguments: Argument Description MODEL_ID/MODEL_TAG Model ID and optional tag (separated by \"/\") Examples: Generate examples for a model: emd example Qwen2.5-7B-Instruct Generate examples for a model with a custom tag: emd example Qwen2.5-7B-Instruct/custom-tag destroy Remove deployed models and clean up resources. emd destroy MODEL_ID [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Model ID to destroy MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Destroy a model: emd destroy Qwen2.5-7B-Instruct Destroy a model with a custom tag: emd destroy Qwen2.5-7B-Instruct custom-tag list-supported-models Display available models that can be deployed. emd list-supported-models [ MODEL_ID ] [ OPTIONS ] Arguments: Argument Description MODEL_ID Optional model ID to filter results Options: Option Description -a, --detail Output model information in detail Examples: List all supported models: emd list-supported-models List detailed information for all models: emd list-supported-models --detail List information for a specific model: emd list-supported-models Qwen2.5-7B-Instruct profile Configure AWS profile credentials for deployment. emd profile COMMAND [ ARGS ] Commands: Command Description set-default-profile-name [NAME] Set the default profile name for deployment show-default-profile-name Show current default profile remove-default-profile-name Remove the default profile Examples: Set a default AWS profile: emd profile set-default-profile-name my-profile Show the current default profile: emd profile show-default-profile-name Remove the default profile: emd profile remove-default-profile-name version Display the current version of the EMD tool. emd version Example: emd version Environment Variables LOCAL_DEPLOY_PORT : Local deployment port (default: 8080 ) Additional Resources Installation Guide Best Deployment Practices Supported Models Langchain Interface","title":"CLI Commands"},{"location":"commands/#cli-commands","text":"This document provides a comprehensive guide to the command-line interface (CLI) commands available in the Easy Model Deployer (EMD) tool.","title":"CLI Commands"},{"location":"commands/#overview","text":"EMD provides the following main commands: Command Description bootstrap Initialize AWS resources for model deployment deploy Deploy models to AWS infrastructure status Display status of deployed models invoke Test deployed models with sample requests example Generate sample code for API integration destroy Remove deployed models and clean up resources list-supported-models Display available models profile Configure AWS profile credentials version Display tool version information","title":"Overview"},{"location":"commands/#command-details","text":"","title":"Command Details"},{"location":"commands/#bootstrap","text":"Initialize AWS resources required for model deployment. emd bootstrap [ OPTIONS ] Options: Option Description --skip-confirm Skip confirmation prompts Example: emd bootstrap This command creates the necessary AWS resources, including an S3 bucket and CloudFormation stack, required for model deployment.","title":"bootstrap"},{"location":"commands/#deploy","text":"Deploy models to AWS infrastructure. emd deploy [ OPTIONS ] Options: Option Description --model-id TEXT Model ID to deploy -i, --instance-type TEXT The instance type to use -e, --engine-type TEXT The name of the inference engine -s, --service-type TEXT The name of the service --framework-type TEXT The name of the framework --model-tag TEXT Custom tag for the model deployment --extra-params TEXT Extra parameters in JSON format --skip-confirm Skip confirmation prompts --force-update-env-stack Force update environment stack --allow-local-deploy Allow local instance deployment --only-allow-local-deploy Only allow local instance deployment --dockerfile-local-path TEXT Custom Dockerfile path for building the model image --local-gpus TEXT Local GPU IDs to deploy the model (e.g., 0,1,2 ) Examples: Deploy a model with interactive prompts: emd deploy Deploy a specific model with parameters: emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.2xlarge --engine-type vllm --service-type sagemaker_realtime Deploy a model locally: emd deploy --allow-local-deploy Deploy with custom parameters: emd deploy --model-id Qwen2.5-7B-Instruct --extra-params '{\"engine_params\": {\"cli_args\": \"--max_model_len 16000 --max_num_seqs 4\"}}'","title":"deploy"},{"location":"commands/#status","text":"Display the status of deployed models. emd status [ MODEL_ID ] [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Optional model ID to check status for MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Check status of all deployed models: emd status Check status of a specific model: emd status Qwen2.5-7B-Instruct Check status of a specific model with a custom tag: emd status Qwen2.5-7B-Instruct custom-tag","title":"status"},{"location":"commands/#invoke","text":"Test deployed models with sample requests. emd invoke MODEL_ID [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Model ID to invoke MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Invoke a model: emd invoke DeepSeek-R1-Distill-Qwen-7B Invoke a model with a custom tag: emd invoke DeepSeek-R1-Distill-Qwen-7B custom-tag","title":"invoke"},{"location":"commands/#example","text":"Generate sample code for API integration with a deployed model. emd example MODEL_ID/MODEL_TAG Arguments: Argument Description MODEL_ID/MODEL_TAG Model ID and optional tag (separated by \"/\") Examples: Generate examples for a model: emd example Qwen2.5-7B-Instruct Generate examples for a model with a custom tag: emd example Qwen2.5-7B-Instruct/custom-tag","title":"example"},{"location":"commands/#destroy","text":"Remove deployed models and clean up resources. emd destroy MODEL_ID [ MODEL_TAG ] Arguments: Argument Description MODEL_ID Model ID to destroy MODEL_TAG Optional model tag (defaults to \"dev\") Examples: Destroy a model: emd destroy Qwen2.5-7B-Instruct Destroy a model with a custom tag: emd destroy Qwen2.5-7B-Instruct custom-tag","title":"destroy"},{"location":"commands/#list-supported-models","text":"Display available models that can be deployed. emd list-supported-models [ MODEL_ID ] [ OPTIONS ] Arguments: Argument Description MODEL_ID Optional model ID to filter results Options: Option Description -a, --detail Output model information in detail Examples: List all supported models: emd list-supported-models List detailed information for all models: emd list-supported-models --detail List information for a specific model: emd list-supported-models Qwen2.5-7B-Instruct","title":"list-supported-models"},{"location":"commands/#profile","text":"Configure AWS profile credentials for deployment. emd profile COMMAND [ ARGS ] Commands: Command Description set-default-profile-name [NAME] Set the default profile name for deployment show-default-profile-name Show current default profile remove-default-profile-name Remove the default profile Examples: Set a default AWS profile: emd profile set-default-profile-name my-profile Show the current default profile: emd profile show-default-profile-name Remove the default profile: emd profile remove-default-profile-name","title":"profile"},{"location":"commands/#version","text":"Display the current version of the EMD tool. emd version Example: emd version","title":"version"},{"location":"commands/#environment-variables","text":"LOCAL_DEPLOY_PORT : Local deployment port (default: 8080 )","title":"Environment Variables"},{"location":"commands/#additional-resources","text":"Installation Guide Best Deployment Practices Supported Models Langchain Interface","title":"Additional Resources"},{"location":"dify_integration/","text":"Dify Integration This guide covers how to integrate EMD-deployed models with Dify , an open-source LLM app development platform. Overview Dify is a comprehensive LLM app development platform that combines workflow automation, RAG pipeline, agent capabilities, model management, and observability features. With its intuitive interface, you can quickly build AI applications using models deployed with Easy Model Deployer (EMD). With Dify, you can: - Create AI applications with visual workflows - Connect to various LLM providers, including custom OpenAI API-compatible endpoints - Build RAG (Retrieval-Augmented Generation) pipelines - Create AI agents with tools and function calling - Monitor and analyze application performance - Deploy your applications with ready-to-use APIs Key Features of Dify Workflow : Build and test AI workflows on a visual canvas Comprehensive Model Support : Integration with hundreds of proprietary and open-source LLMs Prompt IDE : Intuitive interface for crafting prompts and comparing model performance RAG Pipeline : Extensive capabilities for document ingestion and retrieval Agent Capabilities : Define agents with LLM Function Calling or ReAct, with 50+ built-in tools LLMOps : Monitor and analyze application logs and performance Backend-as-a-Service : APIs for integrating Dify into your business logic Integrating EMD Models with Dify EMD-deployed models can be easily integrated with Dify through its OpenAI API-compatible interface. This allows you to use your own deployed models with all the features and capabilities of the Dify platform. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed Dify (either using Dify Cloud or self-hosted ) You have the base URL and API key for your deployed model Configuration Steps Log in to your Dify dashboard Navigate to the Settings section Select Model Providers Click on Add and select Custom or OpenAI API Compatible Configure the provider with the following information: Provider Name : A name for your EMD model provider (e.g., \"EMD Models\") Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Available Models : Add the model IDs of your deployed models Click Save to add the provider Once configured, your EMD-deployed models will appear in the model selection dropdown when creating applications in Dify. Example Use Cases With your EMD models integrated into Dify, you can build various applications: Conversational AI : Create chatbots and virtual assistants using your custom models Document Processing : Build RAG applications that use your models to process and analyze documents AI Workflows : Design complex workflows that incorporate your models at different stages Custom Agents : Create AI agents that use your models for reasoning and decision-making API Services : Expose your models as APIs with additional processing logic Troubleshooting If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Test the connection using a simple API request before integrating with Dify Additional Resources Dify GitHub Repository Dify Documentation EMD Supported Models","title":"Dify Integration"},{"location":"dify_integration/#dify-integration","text":"This guide covers how to integrate EMD-deployed models with Dify , an open-source LLM app development platform.","title":"Dify Integration"},{"location":"dify_integration/#overview","text":"Dify is a comprehensive LLM app development platform that combines workflow automation, RAG pipeline, agent capabilities, model management, and observability features. With its intuitive interface, you can quickly build AI applications using models deployed with Easy Model Deployer (EMD). With Dify, you can: - Create AI applications with visual workflows - Connect to various LLM providers, including custom OpenAI API-compatible endpoints - Build RAG (Retrieval-Augmented Generation) pipelines - Create AI agents with tools and function calling - Monitor and analyze application performance - Deploy your applications with ready-to-use APIs","title":"Overview"},{"location":"dify_integration/#key-features-of-dify","text":"Workflow : Build and test AI workflows on a visual canvas Comprehensive Model Support : Integration with hundreds of proprietary and open-source LLMs Prompt IDE : Intuitive interface for crafting prompts and comparing model performance RAG Pipeline : Extensive capabilities for document ingestion and retrieval Agent Capabilities : Define agents with LLM Function Calling or ReAct, with 50+ built-in tools LLMOps : Monitor and analyze application logs and performance Backend-as-a-Service : APIs for integrating Dify into your business logic","title":"Key Features of Dify"},{"location":"dify_integration/#integrating-emd-models-with-dify","text":"EMD-deployed models can be easily integrated with Dify through its OpenAI API-compatible interface. This allows you to use your own deployed models with all the features and capabilities of the Dify platform.","title":"Integrating EMD Models with Dify"},{"location":"dify_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed Dify (either using Dify Cloud or self-hosted ) You have the base URL and API key for your deployed model","title":"Prerequisites"},{"location":"dify_integration/#configuration-steps","text":"Log in to your Dify dashboard Navigate to the Settings section Select Model Providers Click on Add and select Custom or OpenAI API Compatible Configure the provider with the following information: Provider Name : A name for your EMD model provider (e.g., \"EMD Models\") Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Available Models : Add the model IDs of your deployed models Click Save to add the provider Once configured, your EMD-deployed models will appear in the model selection dropdown when creating applications in Dify.","title":"Configuration Steps"},{"location":"dify_integration/#example-use-cases","text":"With your EMD models integrated into Dify, you can build various applications: Conversational AI : Create chatbots and virtual assistants using your custom models Document Processing : Build RAG applications that use your models to process and analyze documents AI Workflows : Design complex workflows that incorporate your models at different stages Custom Agents : Create AI agents that use your models for reasoning and decision-making API Services : Expose your models as APIs with additional processing logic","title":"Example Use Cases"},{"location":"dify_integration/#troubleshooting","text":"If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Test the connection using a simple API request before integrating with Dify","title":"Troubleshooting"},{"location":"dify_integration/#additional-resources","text":"Dify GitHub Repository Dify Documentation EMD Supported Models","title":"Additional Resources"},{"location":"emd_client/","text":"Usse EMD client to invoke deployed models emd invoke MODEL_ID MODEL_TAG ( Optional ) LLM models emd invoke DeepSeek-R1-Distill-Qwen-7B ... Invoking model DeepSeek-R1-Distill-Qwen-7B with tag dev Write a prompt, press Enter to generate a response ( Ctrl+C to abort ) , User: how to solve the problem of making more profit Assistant:<think> Okay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the expenses. Let me think about how I can approach this. ... VLM models upload image to a s3 path aws s3 cp image.jpg s3://your-bucket/image.jpg invoke the model emd invoke Qwen2-VL-7B-Instruct ... Invoking model Qwen2-VL-7B-Instruct with tag dev Enter image path ( local or s3 file ) : s3://your-bucket/image.jpg Enter prompt: What ' s in this image? ... Video(Txt2Video) models input prompt for video generation emd invoke txt2video-LTX ... Invoking model txt2video-LTX with tag dev Write a prompt, press Enter to generate a response ( Ctrl+C to abort ) , User: Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers ' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show. ... download generated video from output_path Embedding models emd invoke bge-base-en-v1.5 ... Invoking model bge-base-en-v1.5 with tag dev Enter the sentence: hello ... Rerank models emd invoke bge-reranker-v2-m3 ... Enter the text_a ( string ) : What is the capital of France? Enter the text_b ( string ) : The capital of France is Paris. ... ASR models(whisper) upload audio to a s3 path aws s3 cp xx.wav s3://your-bucket/xx.wav invoke the model emd invoke whisper ... Enter the s3 path to the audio file: s3://your-bucket/xx.wav Enter model [ large-v3-turbo/large-v3 ] : large-v3-turbo ...","title":"Usse EMD client to invoke deployed models"},{"location":"emd_client/#usse-emd-client-to-invoke-deployed-models","text":"emd invoke MODEL_ID MODEL_TAG ( Optional )","title":"Usse EMD client to invoke deployed models"},{"location":"emd_client/#llm-models","text":"emd invoke DeepSeek-R1-Distill-Qwen-7B ... Invoking model DeepSeek-R1-Distill-Qwen-7B with tag dev Write a prompt, press Enter to generate a response ( Ctrl+C to abort ) , User: how to solve the problem of making more profit Assistant:<think> Okay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the expenses. Let me think about how I can approach this. ...","title":"LLM models"},{"location":"emd_client/#vlm-models","text":"upload image to a s3 path aws s3 cp image.jpg s3://your-bucket/image.jpg invoke the model emd invoke Qwen2-VL-7B-Instruct ... Invoking model Qwen2-VL-7B-Instruct with tag dev Enter image path ( local or s3 file ) : s3://your-bucket/image.jpg Enter prompt: What ' s in this image? ...","title":"VLM models"},{"location":"emd_client/#videotxt2video-models","text":"input prompt for video generation emd invoke txt2video-LTX ... Invoking model txt2video-LTX with tag dev Write a prompt, press Enter to generate a response ( Ctrl+C to abort ) , User: Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers ' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show. ... download generated video from output_path","title":"Video(Txt2Video) models"},{"location":"emd_client/#embedding-models","text":"emd invoke bge-base-en-v1.5 ... Invoking model bge-base-en-v1.5 with tag dev Enter the sentence: hello ...","title":"Embedding models"},{"location":"emd_client/#rerank-models","text":"emd invoke bge-reranker-v2-m3 ... Enter the text_a ( string ) : What is the capital of France? Enter the text_b ( string ) : The capital of France is Paris. ...","title":"Rerank models"},{"location":"emd_client/#asr-modelswhisper","text":"upload audio to a s3 path aws s3 cp xx.wav s3://your-bucket/xx.wav invoke the model emd invoke whisper ... Enter the s3 path to the audio file: s3://your-bucket/xx.wav Enter model [ large-v3-turbo/large-v3 ] : large-v3-turbo ...","title":"ASR models(whisper)"},{"location":"flowise_integration/","text":"Flowise Integration This guide covers how to integrate EMD-deployed models with Flowise , an open-source UI for building LLM applications. Overview Flowise is a powerful drag-and-drop interface for building custom AI workflows. It provides a visual way to connect various components like language models, embeddings, vector stores, and more to create complex LLM applications without writing code. By integrating EMD-deployed models with Flowise, you can leverage your custom models in sophisticated AI workflows. With Flowise, you can: - Build LLM applications using a visual interface - Connect your EMD-deployed models to various components - Create chatbots, document Q&A systems, and other AI applications - Deploy your workflows as API endpoints - Share your workflows with others Key Features of Flowise Visual Flow Builder : Drag-and-drop interface for creating AI workflows Component Library : Pre-built components for various LLM operations API Deployment : Deploy your workflows as API endpoints Chatbot Interface : Built-in chatbot interface for testing Custom Components : Add your own custom components Marketplace : Share and discover workflows created by the community Integrating EMD Models with Flowise EMD-deployed models can be integrated with Flowise through its OpenAI API compatibility. This allows you to use your custom models in various Flowise components that support OpenAI-compatible APIs. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed and set up Flowise (either locally or using Docker ) You have the base URL and API key for your deployed model Configuration Steps Launch Flowise and log in to the interface Create a new canvas or open an existing one From the components panel, search for and add the \"ChatOpenAI\" component to your canvas Configure the ChatOpenAI component with the following settings: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Connect the ChatOpenAI component to other components in your workflow Test your workflow using the built-in chatbot interface Example Use Cases With your EMD models integrated into Flowise, you can build various applications: Conversational AI : Create chatbots and virtual assistants using your custom models Document Q&A : Build systems that can answer questions based on document content Knowledge Bases : Create searchable knowledge bases with RAG (Retrieval-Augmented Generation) Content Generation : Generate content based on specific inputs and constraints Data Analysis : Extract insights from structured and unstructured data Troubleshooting If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check the Flowise logs for any error messages Additional Resources Flowise GitHub Repository Flowise Documentation EMD Supported Models","title":"Flowise Integration"},{"location":"flowise_integration/#flowise-integration","text":"This guide covers how to integrate EMD-deployed models with Flowise , an open-source UI for building LLM applications.","title":"Flowise Integration"},{"location":"flowise_integration/#overview","text":"Flowise is a powerful drag-and-drop interface for building custom AI workflows. It provides a visual way to connect various components like language models, embeddings, vector stores, and more to create complex LLM applications without writing code. By integrating EMD-deployed models with Flowise, you can leverage your custom models in sophisticated AI workflows. With Flowise, you can: - Build LLM applications using a visual interface - Connect your EMD-deployed models to various components - Create chatbots, document Q&A systems, and other AI applications - Deploy your workflows as API endpoints - Share your workflows with others","title":"Overview"},{"location":"flowise_integration/#key-features-of-flowise","text":"Visual Flow Builder : Drag-and-drop interface for creating AI workflows Component Library : Pre-built components for various LLM operations API Deployment : Deploy your workflows as API endpoints Chatbot Interface : Built-in chatbot interface for testing Custom Components : Add your own custom components Marketplace : Share and discover workflows created by the community","title":"Key Features of Flowise"},{"location":"flowise_integration/#integrating-emd-models-with-flowise","text":"EMD-deployed models can be integrated with Flowise through its OpenAI API compatibility. This allows you to use your custom models in various Flowise components that support OpenAI-compatible APIs.","title":"Integrating EMD Models with Flowise"},{"location":"flowise_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed and set up Flowise (either locally or using Docker ) You have the base URL and API key for your deployed model","title":"Prerequisites"},{"location":"flowise_integration/#configuration-steps","text":"Launch Flowise and log in to the interface Create a new canvas or open an existing one From the components panel, search for and add the \"ChatOpenAI\" component to your canvas Configure the ChatOpenAI component with the following settings: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Connect the ChatOpenAI component to other components in your workflow Test your workflow using the built-in chatbot interface","title":"Configuration Steps"},{"location":"flowise_integration/#example-use-cases","text":"With your EMD models integrated into Flowise, you can build various applications: Conversational AI : Create chatbots and virtual assistants using your custom models Document Q&A : Build systems that can answer questions based on document content Knowledge Bases : Create searchable knowledge bases with RAG (Retrieval-Augmented Generation) Content Generation : Generate content based on specific inputs and constraints Data Analysis : Extract insights from structured and unstructured data","title":"Example Use Cases"},{"location":"flowise_integration/#troubleshooting","text":"If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check the Flowise logs for any error messages","title":"Troubleshooting"},{"location":"flowise_integration/#additional-resources","text":"Flowise GitHub Repository Flowise Documentation EMD Supported Models","title":"Additional Resources"},{"location":"installation/","text":"Quick Start Guide This guide provides simple step-by-step instructions for using Easy Model Deployer (EMD). Prerequisites Before installing Easy Model Deployer, ensure your environment meets the following requirements: Requirement Details Python Version 3.9 or higher required. EMD leverages modern Python features for optimal performance. pip The Python package installer must be available to install EMD and its dependencies. AWS Account Required for deploying models to AWS services (SageMaker, ECS, EC2). AWS CLI Configured with appropriate credentials and permissions for resource creation. Internet Connection Required for downloading model artifacts and dependencies. For local deployments, additional requirements apply. See the Local Deployment Guide for details. Installation Optional: Create a Virtual Environment It's recommended to install EMD in a virtual environment to avoid conflicts with other Python packages: # Create a virtual environment python -m venv .venv # Activate the virtual environment # On macOS/Linux: source .venv/bin/activate # On Windows: # .venv\\Scripts\\activate After activating the virtual environment, your terminal prompt should change to indicate the active environment. You can then proceed with installation. Easy Model Deployer can be installed using various package managers. Choose the method that best suits your workflow: Using pip: pip install easy-model-deployer Using pipx: # Install pipx if you don't have it pip install --user pipx pipx ensurepath # Install EMD pipx install easy-model-deployer Using uv: # Install uv if you don't have it pip install uv # Install EMD uv pip install easy-model-deployer Verification: After installation, verify that EMD is working correctly by running: emd version This should display the installed version of Easy Model Deployer. Upgrading: To upgrade to the latest version: # Using pip pip install --upgrade easy-model-deployer # Using pipx pipx upgrade easy-model-deployer # Using uv uv pip install --upgrade easy-model-deployer Note : After upgrading, you should run emd bootstrap again to ensure your environment is updated. Bootstrap Before deploying models, you need to bootstrap the environment: emd bootstrap Note : You need to run this command again after upgrading EMD with pip to update the environment. Deploy a Model Deploy a model using the interactive CLI: emd deploy The CLI will guide you through selecting: - Model series - Specific model - Deployment service (SageMaker, ECS, EC2, or Local) - Instance type or GPU IDs - Inference engine - Additional parameters You can also deploy models directly with command line parameters: emd deploy --model-series llama --model-name llama-3.3-70b-instruct-awq --service SageMaker For secure API access, you can configure an API key during deployment using the --extra-params option: emd deploy --model-id <model-id> --instance-type <instance-type> --engine-type <engine-type> --service-type <service-type> --extra-params '{ \"service_params\": { \"api_key\": \"your-secure-api-key\" } }' When using the interactive CLI ( emd deploy ), you can also provide this JSON configuration in the \"Extra Parameters\" step of the deployment process. Simply paste the JSON structure when prompted for additional parameters: { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } This API key will be required for authentication when accessing your deployed model's endpoint, enhancing security for your inference services. Note : For local deployment options and detailed model configurations, see the Local Deployment Guide . Check Deployment Status Monitor the status of your deployment: emd status This command shows all active deployments with their ModelIds, endpoints, and status. Invoke the Model Test your deployed model using the CLI: emd invoke <ModelId> Replace <ModelId> with the ID shown in the status output. Integration Options EMD provides an OpenAI-compatible API that allows you to integrate with your deployed models using standard tools and libraries: OpenAI Compatible API : The primary integration method using the OpenAI API format. Once you have the base URL and API key from the emd status command, you can access the API using the OpenAI SDK or any OpenAI-compatible client. API Documentation EMD Client : For direct Python integration LangChain Interface : For integration with LangChain applications The API uses an OpenAI-compatible format, making it easy to switch between OpenAI's services and your deployed models with minimal code changes. Destroy the Deployment When you no longer need the model, remove the deployment: emd destroy <ModelId> Replace <ModelId> with the ID shown in the status output. Advanced Options For more detailed information on: Advanced deployment parameters: See Best Deployment Practices Architecture details: See Architecture Supported models: See Supported Models Local deployment: See Local Deployment Guide CLI commands reference: See CLI Commands API documentation: See API Documentation","title":"Quick Start"},{"location":"installation/#quick-start-guide","text":"This guide provides simple step-by-step instructions for using Easy Model Deployer (EMD).","title":"Quick Start Guide"},{"location":"installation/#prerequisites","text":"Before installing Easy Model Deployer, ensure your environment meets the following requirements: Requirement Details Python Version 3.9 or higher required. EMD leverages modern Python features for optimal performance. pip The Python package installer must be available to install EMD and its dependencies. AWS Account Required for deploying models to AWS services (SageMaker, ECS, EC2). AWS CLI Configured with appropriate credentials and permissions for resource creation. Internet Connection Required for downloading model artifacts and dependencies. For local deployments, additional requirements apply. See the Local Deployment Guide for details.","title":"Prerequisites"},{"location":"installation/#installation","text":"Optional: Create a Virtual Environment It's recommended to install EMD in a virtual environment to avoid conflicts with other Python packages: # Create a virtual environment python -m venv .venv # Activate the virtual environment # On macOS/Linux: source .venv/bin/activate # On Windows: # .venv\\Scripts\\activate After activating the virtual environment, your terminal prompt should change to indicate the active environment. You can then proceed with installation. Easy Model Deployer can be installed using various package managers. Choose the method that best suits your workflow: Using pip: pip install easy-model-deployer Using pipx: # Install pipx if you don't have it pip install --user pipx pipx ensurepath # Install EMD pipx install easy-model-deployer Using uv: # Install uv if you don't have it pip install uv # Install EMD uv pip install easy-model-deployer Verification: After installation, verify that EMD is working correctly by running: emd version This should display the installed version of Easy Model Deployer. Upgrading: To upgrade to the latest version: # Using pip pip install --upgrade easy-model-deployer # Using pipx pipx upgrade easy-model-deployer # Using uv uv pip install --upgrade easy-model-deployer Note : After upgrading, you should run emd bootstrap again to ensure your environment is updated.","title":"Installation"},{"location":"installation/#bootstrap","text":"Before deploying models, you need to bootstrap the environment: emd bootstrap Note : You need to run this command again after upgrading EMD with pip to update the environment.","title":"Bootstrap"},{"location":"installation/#deploy-a-model","text":"Deploy a model using the interactive CLI: emd deploy The CLI will guide you through selecting: - Model series - Specific model - Deployment service (SageMaker, ECS, EC2, or Local) - Instance type or GPU IDs - Inference engine - Additional parameters You can also deploy models directly with command line parameters: emd deploy --model-series llama --model-name llama-3.3-70b-instruct-awq --service SageMaker For secure API access, you can configure an API key during deployment using the --extra-params option: emd deploy --model-id <model-id> --instance-type <instance-type> --engine-type <engine-type> --service-type <service-type> --extra-params '{ \"service_params\": { \"api_key\": \"your-secure-api-key\" } }' When using the interactive CLI ( emd deploy ), you can also provide this JSON configuration in the \"Extra Parameters\" step of the deployment process. Simply paste the JSON structure when prompted for additional parameters: { \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } This API key will be required for authentication when accessing your deployed model's endpoint, enhancing security for your inference services. Note : For local deployment options and detailed model configurations, see the Local Deployment Guide .","title":"Deploy a Model"},{"location":"installation/#check-deployment-status","text":"Monitor the status of your deployment: emd status This command shows all active deployments with their ModelIds, endpoints, and status.","title":"Check Deployment Status"},{"location":"installation/#invoke-the-model","text":"Test your deployed model using the CLI: emd invoke <ModelId> Replace <ModelId> with the ID shown in the status output.","title":"Invoke the Model"},{"location":"installation/#integration-options","text":"EMD provides an OpenAI-compatible API that allows you to integrate with your deployed models using standard tools and libraries: OpenAI Compatible API : The primary integration method using the OpenAI API format. Once you have the base URL and API key from the emd status command, you can access the API using the OpenAI SDK or any OpenAI-compatible client. API Documentation EMD Client : For direct Python integration LangChain Interface : For integration with LangChain applications The API uses an OpenAI-compatible format, making it easy to switch between OpenAI's services and your deployed models with minimal code changes.","title":"Integration Options"},{"location":"installation/#destroy-the-deployment","text":"When you no longer need the model, remove the deployment: emd destroy <ModelId> Replace <ModelId> with the ID shown in the status output.","title":"Destroy the Deployment"},{"location":"installation/#advanced-options","text":"For more detailed information on: Advanced deployment parameters: See Best Deployment Practices Architecture details: See Architecture Supported models: See Supported Models Local deployment: See Local Deployment Guide CLI commands reference: See CLI Commands API documentation: See API Documentation","title":"Advanced Options"},{"location":"langchain_interface/","text":"Usse Langchain interface to invoke deployed models LLM models from emd.integrations.langchain_clients import SageMakerVllmChatModel from langchain_core.output_parsers import StrOutputParser from langchain_core.messages import HumanMessage , AIMessage , SystemMessage from langchain.tools.base import StructuredTool from langchain_core.utils.function_calling import ( convert_to_openai_function , convert_to_openai_tool ) chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"temperature\" : 0.5 , } ) chain = chat_model | StrOutputParser () messages = [ HumanMessage ( content = \"9.11\u548c9.9\u4e24\u4e2a\u6570\u5b57\u54ea\u4e2a\u66f4\u5927\uff1f\" ), ] print ( chain . invoke ( messages )) VLM models upload image to a s3 path aws s3 cp image.jpg s3://your-bucket/image.jpg invoke the model emd invoke Qwen2-VL-7B-Instruct ... Invoking model Qwen2-VL-7B-Instruct with tag dev Enter image path ( local or s3 file ) : s3://your-bucket/image.jpg Enter prompt: What ' s in this image? ... Video(Txt2Video) models Not supported Embedding models import time from emd.integrations.langchain_clients import SageMakerVllmEmbeddings from emd.integrations.langchain_clients import SageMakerVllmRerank embedding_model = SageMakerVllmEmbeddings ( model_id = \"bge-m3\" , ) text = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' t0 = time . time () r1 = embedding_model . embed_query ( text ) t1 = time . time () embedding_model . embed_documents ([ text ] * 1000 ) t2 = time . time () print ( f \"embed_query: { t1 - t0 } \" ) print ( f \"embed_documents: { t2 - t1 } \" ) Rerank models import time from emd.integrations.langchain_clients import SageMakerVllmRerank docs = [ \"hi\" , 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' ] query = 'what is panda?' rerank_model = SageMakerVllmRerank ( model_id = \"bge-reranker-v2-m3\" ) print ( rerank_model . rerank ( query = query , documents = docs ))","title":"Usse Langchain interface to invoke deployed models"},{"location":"langchain_interface/#usse-langchain-interface-to-invoke-deployed-models","text":"","title":"Usse Langchain interface to invoke deployed models"},{"location":"langchain_interface/#llm-models","text":"from emd.integrations.langchain_clients import SageMakerVllmChatModel from langchain_core.output_parsers import StrOutputParser from langchain_core.messages import HumanMessage , AIMessage , SystemMessage from langchain.tools.base import StructuredTool from langchain_core.utils.function_calling import ( convert_to_openai_function , convert_to_openai_tool ) chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"temperature\" : 0.5 , } ) chain = chat_model | StrOutputParser () messages = [ HumanMessage ( content = \"9.11\u548c9.9\u4e24\u4e2a\u6570\u5b57\u54ea\u4e2a\u66f4\u5927\uff1f\" ), ] print ( chain . invoke ( messages ))","title":"LLM models"},{"location":"langchain_interface/#vlm-models","text":"upload image to a s3 path aws s3 cp image.jpg s3://your-bucket/image.jpg invoke the model emd invoke Qwen2-VL-7B-Instruct ... Invoking model Qwen2-VL-7B-Instruct with tag dev Enter image path ( local or s3 file ) : s3://your-bucket/image.jpg Enter prompt: What ' s in this image? ...","title":"VLM models"},{"location":"langchain_interface/#videotxt2video-models","text":"Not supported","title":"Video(Txt2Video) models"},{"location":"langchain_interface/#embedding-models","text":"import time from emd.integrations.langchain_clients import SageMakerVllmEmbeddings from emd.integrations.langchain_clients import SageMakerVllmRerank embedding_model = SageMakerVllmEmbeddings ( model_id = \"bge-m3\" , ) text = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' t0 = time . time () r1 = embedding_model . embed_query ( text ) t1 = time . time () embedding_model . embed_documents ([ text ] * 1000 ) t2 = time . time () print ( f \"embed_query: { t1 - t0 } \" ) print ( f \"embed_documents: { t2 - t1 } \" )","title":"Embedding models"},{"location":"langchain_interface/#rerank-models","text":"import time from emd.integrations.langchain_clients import SageMakerVllmRerank docs = [ \"hi\" , 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' ] query = 'what is panda?' rerank_model = SageMakerVllmRerank ( model_id = \"bge-reranker-v2-m3\" ) print ( rerank_model . rerank ( query = query , documents = docs ))","title":"Rerank models"},{"location":"langflow_integration/","text":"LangFlow Integration This guide covers how to integrate EMD-deployed models with LangFlow , an open-source UI for LangChain. Overview LangFlow is a user-friendly interface that allows you to build LangChain applications using a drag-and-drop visual editor. It provides a way to prototype and experiment with various LangChain components, including language models, without writing code. By integrating EMD-deployed models with LangFlow, you can easily incorporate your custom models into complex LangChain workflows. With LangFlow, you can: - Create complex LangChain flows using a visual interface - Connect your EMD-deployed models to various components - Prototype and test different configurations - Export your flows as Python code - Share your flows with others Key Features of LangFlow Visual Flow Builder : Drag-and-drop interface for creating LangChain flows Component Library : Pre-built components for various LangChain modules Real-time Testing : Test your flows directly in the interface Code Export : Export your flows as Python code Custom Components : Add your own custom components Flow Sharing : Share your flows with others Integrating EMD Models with LangFlow EMD-deployed models can be integrated with LangFlow through its OpenAI API compatibility. This allows you to use your custom models in various LangChain components that support OpenAI-compatible APIs. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed and set up LangFlow (either locally or using Docker ) You have the base URL and API key for your deployed model Configuration Steps Launch LangFlow and log in to the interface Create a new flow or open an existing one Add an LLM component to your flow (such as ChatOpenAI or OpenAI) Configure the LLM component with the following settings: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Connect the LLM component to other components in your flow Test your flow using the \"Build\" button Example Use Cases With your EMD models integrated into LangFlow, you can build various applications: Conversational Agents : Create chatbots and virtual assistants using your custom models Document Processing : Build document processing pipelines with RAG (Retrieval-Augmented Generation) Knowledge Extraction : Extract structured information from unstructured text Content Generation : Generate content based on specific inputs and constraints Multi-step Reasoning : Create flows that break down complex problems into smaller steps Troubleshooting If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check the LangFlow logs for any error messages Additional Resources LangFlow GitHub Repository LangFlow Documentation EMD Supported Models","title":"LangFlow Integration"},{"location":"langflow_integration/#langflow-integration","text":"This guide covers how to integrate EMD-deployed models with LangFlow , an open-source UI for LangChain.","title":"LangFlow Integration"},{"location":"langflow_integration/#overview","text":"LangFlow is a user-friendly interface that allows you to build LangChain applications using a drag-and-drop visual editor. It provides a way to prototype and experiment with various LangChain components, including language models, without writing code. By integrating EMD-deployed models with LangFlow, you can easily incorporate your custom models into complex LangChain workflows. With LangFlow, you can: - Create complex LangChain flows using a visual interface - Connect your EMD-deployed models to various components - Prototype and test different configurations - Export your flows as Python code - Share your flows with others","title":"Overview"},{"location":"langflow_integration/#key-features-of-langflow","text":"Visual Flow Builder : Drag-and-drop interface for creating LangChain flows Component Library : Pre-built components for various LangChain modules Real-time Testing : Test your flows directly in the interface Code Export : Export your flows as Python code Custom Components : Add your own custom components Flow Sharing : Share your flows with others","title":"Key Features of LangFlow"},{"location":"langflow_integration/#integrating-emd-models-with-langflow","text":"EMD-deployed models can be integrated with LangFlow through its OpenAI API compatibility. This allows you to use your custom models in various LangChain components that support OpenAI-compatible APIs.","title":"Integrating EMD Models with LangFlow"},{"location":"langflow_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed and set up LangFlow (either locally or using Docker ) You have the base URL and API key for your deployed model","title":"Prerequisites"},{"location":"langflow_integration/#configuration-steps","text":"Launch LangFlow and log in to the interface Create a new flow or open an existing one Add an LLM component to your flow (such as ChatOpenAI or OpenAI) Configure the LLM component with the following settings: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Connect the LLM component to other components in your flow Test your flow using the \"Build\" button","title":"Configuration Steps"},{"location":"langflow_integration/#example-use-cases","text":"With your EMD models integrated into LangFlow, you can build various applications: Conversational Agents : Create chatbots and virtual assistants using your custom models Document Processing : Build document processing pipelines with RAG (Retrieval-Augmented Generation) Knowledge Extraction : Extract structured information from unstructured text Content Generation : Generate content based on specific inputs and constraints Multi-step Reasoning : Create flows that break down complex problems into smaller steps","title":"Example Use Cases"},{"location":"langflow_integration/#troubleshooting","text":"If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check the LangFlow logs for any error messages","title":"Troubleshooting"},{"location":"langflow_integration/#additional-resources","text":"LangFlow GitHub Repository LangFlow Documentation EMD Supported Models","title":"Additional Resources"},{"location":"local_deployment/","text":"Local Model Deployment Guide This guide provides detailed instructions for deploying models locally using Easy Model Deployer (EMD). Local Deployment on EC2 Instance For deploying models using local GPU resources: Prerequisites It is recommended to launch an EC2 instance using the AMI \" Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.6 (Ubuntu 22.04) \". Deployment Command Deploy with: emd deploy --allow-local-deploy Command Line Deployment You can deploy models directly with command line parameters: emd deploy --model-series llama --model-name llama-3.3-70b-instruct-awq --service Local --gpu-ids 0 ,1,2,3 Additional Parameters You can provide additional parameters as a JSON string: emd deploy --extra-params '{\"engine_params\":{\"api_key\":\"YOUR_API_KEY\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}' Or from a file: emd deploy --extra-params path/to/params.json The extra parameters format: { \"model_params\" : { }, \"service_params\" :{ }, \"instance_params\" :{ }, \"engine_params\" :{ \"cli_args\" : \"<command line arguments of current engine>\" }, \"framework_params\" :{ \"uvicorn_log_level\" : \"info\" , \"limit_concurrency\" : 200 } } Common Model Configurations Non-reasoning Models Qwen2.5-72B-Instruct-AWQ ? Select the model series: qwen2.5 ? Select the model name: Qwen2.5-72B-Instruct-AWQ ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}} llama-3.3-70b-instruct-awq ? Select the model series: llama ? Select the model name: llama-3.3-70b-instruct-awq ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}} Reasoning Models DeepSeek-R1-Distill-Qwen-32B ? Select the model series: deepseek reasoning model ? Select the model name: DeepSeek-R1-Distill-Qwen-32B ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: vllm ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--enable-reasoning --reasoning-parser deepseek_r1 --max_model_len 16000 --disable-log-stats --chat-template emd/models/chat_templates/deepseek_r1_distill.jinja --max_num_seq 20 --gpu_memory_utilization 0.9\"}} deepseek-r1-distill-llama-70b-awq ? Select the model series: deepseek reasoning model ? Select the model name: deepseek-r1-distill-llama-70b-awq ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}} Tips for Local Deployment When you see \"Waiting for model: ...\", it means the deployment task has started. You can press Ctrl+C to stop the terminal output without affecting the deployment. For multi-GPU deployments, ensure all specified GPUs are available and have sufficient memory. Monitor GPU usage with tools like nvidia-smi during deployment and inference. For optimal performance, consider the recommended GPU memory requirements for each model. Advanced Options For more detailed information on: - Advanced deployment parameters: See Best Deployment Practices - Architecture details: See Architecture - Supported models: See Supported Models","title":"Local Deployment"},{"location":"local_deployment/#local-model-deployment-guide","text":"This guide provides detailed instructions for deploying models locally using Easy Model Deployer (EMD).","title":"Local Model Deployment Guide"},{"location":"local_deployment/#local-deployment-on-ec2-instance","text":"For deploying models using local GPU resources:","title":"Local Deployment on EC2 Instance"},{"location":"local_deployment/#prerequisites","text":"It is recommended to launch an EC2 instance using the AMI \" Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.6 (Ubuntu 22.04) \".","title":"Prerequisites"},{"location":"local_deployment/#deployment-command","text":"Deploy with: emd deploy --allow-local-deploy","title":"Deployment Command"},{"location":"local_deployment/#command-line-deployment","text":"You can deploy models directly with command line parameters: emd deploy --model-series llama --model-name llama-3.3-70b-instruct-awq --service Local --gpu-ids 0 ,1,2,3","title":"Command Line Deployment"},{"location":"local_deployment/#additional-parameters","text":"You can provide additional parameters as a JSON string: emd deploy --extra-params '{\"engine_params\":{\"api_key\":\"YOUR_API_KEY\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}' Or from a file: emd deploy --extra-params path/to/params.json The extra parameters format: { \"model_params\" : { }, \"service_params\" :{ }, \"instance_params\" :{ }, \"engine_params\" :{ \"cli_args\" : \"<command line arguments of current engine>\" }, \"framework_params\" :{ \"uvicorn_log_level\" : \"info\" , \"limit_concurrency\" : 200 } }","title":"Additional Parameters"},{"location":"local_deployment/#common-model-configurations","text":"","title":"Common Model Configurations"},{"location":"local_deployment/#non-reasoning-models","text":"","title":"Non-reasoning Models"},{"location":"local_deployment/#qwen25-72b-instruct-awq","text":"? Select the model series: qwen2.5 ? Select the model name: Qwen2.5-72B-Instruct-AWQ ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}","title":"Qwen2.5-72B-Instruct-AWQ"},{"location":"local_deployment/#llama-33-70b-instruct-awq","text":"? Select the model series: llama ? Select the model name: llama-3.3-70b-instruct-awq ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}","title":"llama-3.3-70b-instruct-awq"},{"location":"local_deployment/#reasoning-models","text":"","title":"Reasoning Models"},{"location":"local_deployment/#deepseek-r1-distill-qwen-32b","text":"? Select the model series: deepseek reasoning model ? Select the model name: DeepSeek-R1-Distill-Qwen-32B ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: vllm ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--enable-reasoning --reasoning-parser deepseek_r1 --max_model_len 16000 --disable-log-stats --chat-template emd/models/chat_templates/deepseek_r1_distill.jinja --max_num_seq 20 --gpu_memory_utilization 0.9\"}}","title":"DeepSeek-R1-Distill-Qwen-32B"},{"location":"local_deployment/#deepseek-r1-distill-llama-70b-awq","text":"? Select the model series: deepseek reasoning model ? Select the model name: deepseek-r1-distill-llama-70b-awq ? Select the service for deployment: Local ? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3 ? Select the inference engine to use: tgi ? (Optional) Additional deployment parameters: {\"engine_params\":{\"api_key\":\"<YOUR_API_KEY>\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}","title":"deepseek-r1-distill-llama-70b-awq"},{"location":"local_deployment/#tips-for-local-deployment","text":"When you see \"Waiting for model: ...\", it means the deployment task has started. You can press Ctrl+C to stop the terminal output without affecting the deployment. For multi-GPU deployments, ensure all specified GPUs are available and have sufficient memory. Monitor GPU usage with tools like nvidia-smi during deployment and inference. For optimal performance, consider the recommended GPU memory requirements for each model.","title":"Tips for Local Deployment"},{"location":"local_deployment/#advanced-options","text":"For more detailed information on: - Advanced deployment parameters: See Best Deployment Practices - Architecture details: See Architecture - Supported models: See Supported Models","title":"Advanced Options"},{"location":"nextchat_integration/","text":"NextChat Integration This guide covers how to integrate EMD-deployed models with NextChat , an open-source cross-platform ChatGPT web UI. Overview NextChat (ChatGPT-Next-Web) is a popular open-source project that provides a clean, intuitive web interface for interacting with various large language models. It supports multiple models including OpenAI's GPT models, and importantly, any OpenAI API-compatible endpoints. By integrating EMD-deployed models with NextChat, you can create a user-friendly interface for interacting with your custom models. With NextChat, you can: - Access your EMD-deployed models through a polished web interface - Create and manage multiple conversations - Save and share conversation history - Customize the UI to match your preferences - Deploy the interface on various platforms Key Features of NextChat Clean User Interface : Modern, responsive design for desktop and mobile Multi-model Support : Switch between different models easily Conversation Management : Create, save, and organize conversations Prompt Templates : Create and use templates for common prompts Markdown Support : Rich text formatting with Markdown Self-hosting : Deploy on your own infrastructure Cross-platform : Available as a web app, PWA, or desktop application Integrating EMD Models with NextChat EMD-deployed models can be easily integrated with NextChat through its OpenAI API compatibility. This allows you to use your custom models with all the features and convenience of the NextChat interface. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have access to NextChat (either through the hosted version or by self-hosting ) You have the base URL and API key for your deployed model Configuration Steps Access NextChat through your browser Click on the settings icon in the sidebar Navigate to the \"Models\" section Click \"Add Custom Model\" Configure your EMD-deployed model with the following settings: Name : A name for your model (e.g., \"My EMD Model\") Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Save the configuration Select your custom model from the model dropdown in the chat interface Self-hosting NextChat with EMD Integration If you prefer to self-host NextChat, you can configure it to use your EMD-deployed model by default: Clone the NextChat repository: git clone https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web.git cd ChatGPT-Next-Web Create a .env.local file with the following environment variables: OPENAI_API_KEY=your-api-key BASE_URL=https://your-endpoint.execute-api.region.amazonaws.com NEXT_PUBLIC_DEFAULT_MODEL=your-deployed-model-id Build and run the application: npm install npm run build npm run start Example Use Cases With your EMD models integrated into NextChat, you can: Personal Assistant : Create a personalized AI assistant using your custom-trained model Customer Support : Deploy a customer support interface with domain-specific knowledge Content Creation : Use the interface for brainstorming and content generation Educational Tool : Create an educational interface with specialized knowledge Research Assistant : Build a research assistant with access to specific research domains Troubleshooting If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check browser console logs for any error messages Additional Resources NextChat GitHub Repository NextChat Documentation EMD Supported Models","title":"NextChat Integration"},{"location":"nextchat_integration/#nextchat-integration","text":"This guide covers how to integrate EMD-deployed models with NextChat , an open-source cross-platform ChatGPT web UI.","title":"NextChat Integration"},{"location":"nextchat_integration/#overview","text":"NextChat (ChatGPT-Next-Web) is a popular open-source project that provides a clean, intuitive web interface for interacting with various large language models. It supports multiple models including OpenAI's GPT models, and importantly, any OpenAI API-compatible endpoints. By integrating EMD-deployed models with NextChat, you can create a user-friendly interface for interacting with your custom models. With NextChat, you can: - Access your EMD-deployed models through a polished web interface - Create and manage multiple conversations - Save and share conversation history - Customize the UI to match your preferences - Deploy the interface on various platforms","title":"Overview"},{"location":"nextchat_integration/#key-features-of-nextchat","text":"Clean User Interface : Modern, responsive design for desktop and mobile Multi-model Support : Switch between different models easily Conversation Management : Create, save, and organize conversations Prompt Templates : Create and use templates for common prompts Markdown Support : Rich text formatting with Markdown Self-hosting : Deploy on your own infrastructure Cross-platform : Available as a web app, PWA, or desktop application","title":"Key Features of NextChat"},{"location":"nextchat_integration/#integrating-emd-models-with-nextchat","text":"EMD-deployed models can be easily integrated with NextChat through its OpenAI API compatibility. This allows you to use your custom models with all the features and convenience of the NextChat interface.","title":"Integrating EMD Models with NextChat"},{"location":"nextchat_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have access to NextChat (either through the hosted version or by self-hosting ) You have the base URL and API key for your deployed model","title":"Prerequisites"},{"location":"nextchat_integration/#configuration-steps","text":"Access NextChat through your browser Click on the settings icon in the sidebar Navigate to the \"Models\" section Click \"Add Custom Model\" Configure your EMD-deployed model with the following settings: Name : A name for your model (e.g., \"My EMD Model\") Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model Name : The ID of your deployed model Save the configuration Select your custom model from the model dropdown in the chat interface","title":"Configuration Steps"},{"location":"nextchat_integration/#self-hosting-nextchat-with-emd-integration","text":"If you prefer to self-host NextChat, you can configure it to use your EMD-deployed model by default: Clone the NextChat repository: git clone https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web.git cd ChatGPT-Next-Web Create a .env.local file with the following environment variables: OPENAI_API_KEY=your-api-key BASE_URL=https://your-endpoint.execute-api.region.amazonaws.com NEXT_PUBLIC_DEFAULT_MODEL=your-deployed-model-id Build and run the application: npm install npm run build npm run start","title":"Self-hosting NextChat with EMD Integration"},{"location":"nextchat_integration/#example-use-cases","text":"With your EMD models integrated into NextChat, you can: Personal Assistant : Create a personalized AI assistant using your custom-trained model Customer Support : Deploy a customer support interface with domain-specific knowledge Content Creation : Use the interface for brainstorming and content generation Educational Tool : Create an educational interface with specialized knowledge Research Assistant : Build a research assistant with access to specific research domains","title":"Example Use Cases"},{"location":"nextchat_integration/#troubleshooting","text":"If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check browser console logs for any error messages","title":"Troubleshooting"},{"location":"nextchat_integration/#additional-resources","text":"NextChat GitHub Repository NextChat Documentation EMD Supported Models","title":"Additional Resources"},{"location":"ollama_integration/","text":"Ollama Integration This guide covers how to integrate EMD-deployed models with Ollama , an open-source framework for running large language models locally. Overview Ollama is a popular tool that allows you to run large language models locally on your own hardware. It provides a simple way to download, run, and manage various open-source models. By integrating EMD-deployed models with Ollama, you can create a hybrid setup that leverages both local models and your custom cloud-deployed models. With Ollama integration, you can: - Use both local models and EMD-deployed models in your applications - Create fallback mechanisms between local and cloud models - Compare performance between local and cloud-deployed versions - Develop applications that work both online and offline - Optimize for cost, performance, or privacy based on specific needs Key Features of Ollama Local Model Execution : Run models on your own hardware Simple API : Easy-to-use REST API for model interaction Model Library : Access to various open-source models Customization : Create and customize model configurations Cross-platform : Available for macOS, Windows, and Linux Low Resource Usage : Optimized for running on consumer hardware Integrating EMD Models with Ollama There are several ways to integrate EMD-deployed models with Ollama: 1. API Orchestration You can build an orchestration layer that routes requests between Ollama's API and your EMD-deployed model's API based on specific criteria. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed Ollama on your local machine You have the base URL and API key for your deployed EMD model Implementation Example Here's a simple Python example that routes requests between Ollama and an EMD-deployed model: import requests import json def generate_text ( prompt , use_local = True , max_tokens = 100 ): \"\"\" Generate text using either Ollama (local) or EMD-deployed model (cloud) Args: prompt (str): The input prompt use_local (bool): Whether to use local Ollama model max_tokens (int): Maximum tokens to generate Returns: str: Generated text \"\"\" if use_local : # Use Ollama API (local) response = requests . post ( \"http://localhost:11434/api/generate\" , json = { \"model\" : \"llama3\" , # or any other model you have pulled \"prompt\" : prompt , \"max_tokens\" : max_tokens } ) return response . json () . get ( \"response\" , \"\" ) else : # Use EMD-deployed model API (cloud) response = requests . post ( \"https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions\" , headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer your-api-key\" }, json = { \"model\" : \"your-deployed-model-id\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : prompt }], \"max_tokens\" : max_tokens } ) return response . json () . get ( \"choices\" , [{}])[ 0 ] . get ( \"message\" , {}) . get ( \"content\" , \"\" ) # Example usage result = generate_text ( \"Explain quantum computing\" , use_local = True ) print ( result ) result = generate_text ( \"Explain quantum computing\" , use_local = False ) print ( result ) 2. Fallback Mechanism You can implement a fallback mechanism that tries the local Ollama model first and falls back to the EMD-deployed model if the local model fails or produces unsatisfactory results. def generate_with_fallback ( prompt , max_tokens = 100 ): \"\"\" Try local model first, fall back to cloud model if needed Args: prompt (str): The input prompt max_tokens (int): Maximum tokens to generate Returns: str: Generated text \"\"\" try : # Try Ollama first response = requests . post ( \"http://localhost:11434/api/generate\" , json = { \"model\" : \"llama3\" , \"prompt\" : prompt , \"max_tokens\" : max_tokens }, timeout = 5 # Set a timeout for local model ) if response . status_code == 200 : result = response . json () . get ( \"response\" , \"\" ) if result and len ( result ) > 20 : # Simple quality check return { \"source\" : \"local\" , \"text\" : result } except Exception as e : print ( f \"Local model error: { e } \" ) # Fall back to EMD-deployed model try : response = requests . post ( \"https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions\" , headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer your-api-key\" }, json = { \"model\" : \"your-deployed-model-id\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : prompt }], \"max_tokens\" : max_tokens } ) if response . status_code == 200 : result = response . json () . get ( \"choices\" , [{}])[ 0 ] . get ( \"message\" , {}) . get ( \"content\" , \"\" ) return { \"source\" : \"cloud\" , \"text\" : result } except Exception as e : print ( f \"Cloud model error: { e } \" ) return { \"source\" : \"none\" , \"text\" : \"Failed to generate response from both local and cloud models.\" } Example Use Cases With your EMD models integrated with Ollama, you can build various applications: Hybrid AI Applications : Applications that use local models for basic tasks and cloud models for more complex tasks Offline-First Applications : Applications that work offline with local models but enhance capabilities when online Cost-Optimized Solutions : Use local models for frequent, simple queries and cloud models for important or complex queries Privacy-Focused Applications : Process sensitive data locally and only use cloud models for non-sensitive data Development and Testing : Use local models during development and testing, and cloud models in production Troubleshooting If you encounter issues with the integration: Verify that Ollama is running locally ( ollama list should show available models) Check that your EMD model is properly deployed and accessible Ensure API endpoints and authentication details are correct Check network connectivity if using cloud models Monitor resource usage if local models are running slowly Additional Resources Ollama GitHub Repository Ollama API Documentation EMD Supported Models","title":"Ollama Integration"},{"location":"ollama_integration/#ollama-integration","text":"This guide covers how to integrate EMD-deployed models with Ollama , an open-source framework for running large language models locally.","title":"Ollama Integration"},{"location":"ollama_integration/#overview","text":"Ollama is a popular tool that allows you to run large language models locally on your own hardware. It provides a simple way to download, run, and manage various open-source models. By integrating EMD-deployed models with Ollama, you can create a hybrid setup that leverages both local models and your custom cloud-deployed models. With Ollama integration, you can: - Use both local models and EMD-deployed models in your applications - Create fallback mechanisms between local and cloud models - Compare performance between local and cloud-deployed versions - Develop applications that work both online and offline - Optimize for cost, performance, or privacy based on specific needs","title":"Overview"},{"location":"ollama_integration/#key-features-of-ollama","text":"Local Model Execution : Run models on your own hardware Simple API : Easy-to-use REST API for model interaction Model Library : Access to various open-source models Customization : Create and customize model configurations Cross-platform : Available for macOS, Windows, and Linux Low Resource Usage : Optimized for running on consumer hardware","title":"Key Features of Ollama"},{"location":"ollama_integration/#integrating-emd-models-with-ollama","text":"There are several ways to integrate EMD-deployed models with Ollama:","title":"Integrating EMD Models with Ollama"},{"location":"ollama_integration/#1-api-orchestration","text":"You can build an orchestration layer that routes requests between Ollama's API and your EMD-deployed model's API based on specific criteria.","title":"1. API Orchestration"},{"location":"ollama_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed Ollama on your local machine You have the base URL and API key for your deployed EMD model","title":"Prerequisites"},{"location":"ollama_integration/#implementation-example","text":"Here's a simple Python example that routes requests between Ollama and an EMD-deployed model: import requests import json def generate_text ( prompt , use_local = True , max_tokens = 100 ): \"\"\" Generate text using either Ollama (local) or EMD-deployed model (cloud) Args: prompt (str): The input prompt use_local (bool): Whether to use local Ollama model max_tokens (int): Maximum tokens to generate Returns: str: Generated text \"\"\" if use_local : # Use Ollama API (local) response = requests . post ( \"http://localhost:11434/api/generate\" , json = { \"model\" : \"llama3\" , # or any other model you have pulled \"prompt\" : prompt , \"max_tokens\" : max_tokens } ) return response . json () . get ( \"response\" , \"\" ) else : # Use EMD-deployed model API (cloud) response = requests . post ( \"https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions\" , headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer your-api-key\" }, json = { \"model\" : \"your-deployed-model-id\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : prompt }], \"max_tokens\" : max_tokens } ) return response . json () . get ( \"choices\" , [{}])[ 0 ] . get ( \"message\" , {}) . get ( \"content\" , \"\" ) # Example usage result = generate_text ( \"Explain quantum computing\" , use_local = True ) print ( result ) result = generate_text ( \"Explain quantum computing\" , use_local = False ) print ( result )","title":"Implementation Example"},{"location":"ollama_integration/#2-fallback-mechanism","text":"You can implement a fallback mechanism that tries the local Ollama model first and falls back to the EMD-deployed model if the local model fails or produces unsatisfactory results. def generate_with_fallback ( prompt , max_tokens = 100 ): \"\"\" Try local model first, fall back to cloud model if needed Args: prompt (str): The input prompt max_tokens (int): Maximum tokens to generate Returns: str: Generated text \"\"\" try : # Try Ollama first response = requests . post ( \"http://localhost:11434/api/generate\" , json = { \"model\" : \"llama3\" , \"prompt\" : prompt , \"max_tokens\" : max_tokens }, timeout = 5 # Set a timeout for local model ) if response . status_code == 200 : result = response . json () . get ( \"response\" , \"\" ) if result and len ( result ) > 20 : # Simple quality check return { \"source\" : \"local\" , \"text\" : result } except Exception as e : print ( f \"Local model error: { e } \" ) # Fall back to EMD-deployed model try : response = requests . post ( \"https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions\" , headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer your-api-key\" }, json = { \"model\" : \"your-deployed-model-id\" , \"messages\" : [{ \"role\" : \"user\" , \"content\" : prompt }], \"max_tokens\" : max_tokens } ) if response . status_code == 200 : result = response . json () . get ( \"choices\" , [{}])[ 0 ] . get ( \"message\" , {}) . get ( \"content\" , \"\" ) return { \"source\" : \"cloud\" , \"text\" : result } except Exception as e : print ( f \"Cloud model error: { e } \" ) return { \"source\" : \"none\" , \"text\" : \"Failed to generate response from both local and cloud models.\" }","title":"2. Fallback Mechanism"},{"location":"ollama_integration/#example-use-cases","text":"With your EMD models integrated with Ollama, you can build various applications: Hybrid AI Applications : Applications that use local models for basic tasks and cloud models for more complex tasks Offline-First Applications : Applications that work offline with local models but enhance capabilities when online Cost-Optimized Solutions : Use local models for frequent, simple queries and cloud models for important or complex queries Privacy-Focused Applications : Process sensitive data locally and only use cloud models for non-sensitive data Development and Testing : Use local models during development and testing, and cloud models in production","title":"Example Use Cases"},{"location":"ollama_integration/#troubleshooting","text":"If you encounter issues with the integration: Verify that Ollama is running locally ( ollama list should show available models) Check that your EMD model is properly deployed and accessible Ensure API endpoints and authentication details are correct Check network connectivity if using cloud models Monitor resource usage if local models are running slowly","title":"Troubleshooting"},{"location":"ollama_integration/#additional-resources","text":"Ollama GitHub Repository Ollama API Documentation EMD Supported Models","title":"Additional Resources"},{"location":"openai_compatible/","text":"OpenAI Compatible API EMD provides an OpenAI-compatible API interface for all deployed models, making it easy to integrate with existing tools and libraries that support the OpenAI API format. Overview The OpenAI-compatible API allows you to: Use the familiar OpenAI API format for interacting with your deployed models Integrate with existing tools and libraries that support OpenAI's API Switch between OpenAI's services and your deployed models with minimal code changes Access advanced features like streaming responses and function calling Getting Started To use the OpenAI-compatible API with your deployed models: Deploy a model using EMD Retrieve the base URL and API key using emd status Use the OpenAI client library or direct API calls to interact with your model API Endpoints EMD's OpenAI-compatible API supports the following endpoints: /v1/models - List available models /v1/chat/completions - Create chat completions /v1/embeddings - Generate embeddings /v1/rerank - Rerank documents /v1/invocations - General-purpose model invocations /v1/audio/transcriptions - Transcribe audio (for supported models) Authentication All API requests require authentication using an API key. Include the API key in the Authorization header: Authorization: Bearer YOUR_API_KEY Examples Chat Completions import openai client = openai . OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Create a chat completion response = client . chat . completions . create ( model = \"YOUR_MODEL_ID/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], temperature = 0.7 , stream = False ) # Print the response print ( response . choices [ 0 ] . message . content ) Streaming Example from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Create a streaming chat completion stream = client . chat . completions . create ( model = \"YOUR_MODEL_ID/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Write a short poem about AI.\" } ], stream = True ) # Process the stream for chunk in stream : if chunk . choices [ 0 ] . delta . content is not None : print ( chunk . choices [ 0 ] . delta . content , end = \"\" ) print () Embeddings from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Get embeddings for a single text response = client . embeddings . create ( model = \"YOUR_EMBEDDING_MODEL_ID/dev\" , # Embedding model ID with tag input = \"The food was delicious and the service was excellent.\" ) # Print the embedding vector print ( response . data [ 0 ] . embedding ) Supported Features Depending on the model and deployment configuration, the following features are supported: Streaming responses : Real-time token generation Function calling : For models that support it Vision capabilities : For multimodal models Custom parameters : Temperature, top_p, max_tokens, etc. Limitations Some advanced OpenAI API features may not be available depending on the model Performance characteristics may differ from OpenAI's models API response formats match OpenAI's but may include additional fields specific to EMD Additional Resources For more detailed information, see: API Documentation Supported Models Integration Examples","title":"OpenAI Compatible API"},{"location":"openai_compatible/#openai-compatible-api","text":"EMD provides an OpenAI-compatible API interface for all deployed models, making it easy to integrate with existing tools and libraries that support the OpenAI API format.","title":"OpenAI Compatible API"},{"location":"openai_compatible/#overview","text":"The OpenAI-compatible API allows you to: Use the familiar OpenAI API format for interacting with your deployed models Integrate with existing tools and libraries that support OpenAI's API Switch between OpenAI's services and your deployed models with minimal code changes Access advanced features like streaming responses and function calling","title":"Overview"},{"location":"openai_compatible/#getting-started","text":"To use the OpenAI-compatible API with your deployed models: Deploy a model using EMD Retrieve the base URL and API key using emd status Use the OpenAI client library or direct API calls to interact with your model","title":"Getting Started"},{"location":"openai_compatible/#api-endpoints","text":"EMD's OpenAI-compatible API supports the following endpoints: /v1/models - List available models /v1/chat/completions - Create chat completions /v1/embeddings - Generate embeddings /v1/rerank - Rerank documents /v1/invocations - General-purpose model invocations /v1/audio/transcriptions - Transcribe audio (for supported models)","title":"API Endpoints"},{"location":"openai_compatible/#authentication","text":"All API requests require authentication using an API key. Include the API key in the Authorization header: Authorization: Bearer YOUR_API_KEY","title":"Authentication"},{"location":"openai_compatible/#examples","text":"","title":"Examples"},{"location":"openai_compatible/#chat-completions","text":"import openai client = openai . OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Create a chat completion response = client . chat . completions . create ( model = \"YOUR_MODEL_ID/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Hello!\" } ], temperature = 0.7 , stream = False ) # Print the response print ( response . choices [ 0 ] . message . content )","title":"Chat Completions"},{"location":"openai_compatible/#streaming-example","text":"from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Create a streaming chat completion stream = client . chat . completions . create ( model = \"YOUR_MODEL_ID/dev\" , # Model ID with tag messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Write a short poem about AI.\" } ], stream = True ) # Process the stream for chunk in stream : if chunk . choices [ 0 ] . delta . content is not None : print ( chunk . choices [ 0 ] . delta . content , end = \"\" ) print ()","title":"Streaming Example"},{"location":"openai_compatible/#embeddings","text":"from openai import OpenAI client = OpenAI ( api_key = \"YOUR_API_KEY\" , base_url = \"https://YOUR_EMD_ENDPOINT/v1\" ) # Get embeddings for a single text response = client . embeddings . create ( model = \"YOUR_EMBEDDING_MODEL_ID/dev\" , # Embedding model ID with tag input = \"The food was delicious and the service was excellent.\" ) # Print the embedding vector print ( response . data [ 0 ] . embedding )","title":"Embeddings"},{"location":"openai_compatible/#supported-features","text":"Depending on the model and deployment configuration, the following features are supported: Streaming responses : Real-time token generation Function calling : For models that support it Vision capabilities : For multimodal models Custom parameters : Temperature, top_p, max_tokens, etc.","title":"Supported Features"},{"location":"openai_compatible/#limitations","text":"Some advanced OpenAI API features may not be available depending on the model Performance characteristics may differ from OpenAI's models API response formats match OpenAI's but may include additional fields specific to EMD","title":"Limitations"},{"location":"openai_compatible/#additional-resources","text":"For more detailed information, see: API Documentation Supported Models Integration Examples","title":"Additional Resources"},{"location":"sdk_api/","text":"SDK API Documentation Getting Started : The EMD SDK provides a comprehensive Python interface for deploying, managing, and invoking machine learning models on AWS infrastructure. Install the SDK with pip install easy-model-deployer and import the modules you need. Quick example: from emd.sdk import bootstrap , deploy , destroy from emd.sdk.clients import SageMakerClient # Bootstrap infrastructure bootstrap () # Deploy a model result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) # Use the deployed model client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" ) response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] }) Bootstrap Infrastructure Initialize AWS resources required for model deployment. Function: bootstrap() Python Example: from emd.sdk.bootstrap import bootstrap # Initialize AWS infrastructure bootstrap () Advanced Example: from emd.sdk.bootstrap import create_env_stack # Create environment stack with custom parameters create_env_stack ( region = \"us-east-1\" , stack_name = \"my-emd-env-stack\" , bucket_name = \"my-emd-bucket\" , force_update = True ) Deploy Models Deploy machine learning models to AWS services. Function: deploy(model_id, instance_type, engine_type, service_type, **kwargs) Parameters: model_id (required): Model identifier (e.g., \"Qwen2.5-7B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\") instance_type (required): AWS instance type (e.g., \"ml.g5.xlarge\", \"g5.2xlarge\") engine_type (required): Inference engine (\"vllm\", \"tgi\", \"huggingface\") service_type (required): AWS service (\"sagemaker\", \"ecs\", \"ec2\") framework_type : API framework (default: \"fastapi\") model_tag : Model version tag (default: \"dev\") waiting_until_deploy_complete : Wait for completion (default: True) extra_params : Additional deployment parameters Basic Example: from emd.sdk.deploy import deploy # Deploy a model to SageMaker result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) print ( f \"Deployment ID: { result [ 'pipeline_execution_id' ] } \" ) print ( f \"Model Stack: { result [ 'model_stack_name' ] } \" ) Advanced Example: from emd.sdk.deploy import deploy # Deploy with custom parameters result = deploy ( model_id = \"DeepSeek-R1-Distill-Llama-8B\" , instance_type = \"ml.g5.2xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" , model_tag = \"production\" , extra_params = { \"engine_params\" : { \"cli_args\" : \"--max_model_len 16000 --max_num_seqs 4\" }, \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } ) Local Deployment Example: from emd.sdk.deploy import deploy_local # Deploy locally for testing deploy_local ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"cpu\" , service_type = \"local\" , engine_type = \"vllm\" , extra_params = { \"temperature\" : 0.7 } ) Model Status Check the deployment status of models. Function: get_model_status(model_id, model_tag) Python Example: from emd.sdk.status import get_model_status # Check status of a specific model status = get_model_status ( \"Qwen2.5-7B-Instruct\" , \"dev\" ) # Check in-progress deployments for deployment in status [ \"inprogress\" ]: print ( f \"Model: { deployment [ 'model_id' ] } \" ) print ( f \"Status: { deployment [ 'status' ] } \" ) print ( f \"Stage: { deployment . get ( 'stage_name' , 'N/A' ) } \" ) # Check completed deployments for deployment in status [ \"completed\" ]: print ( f \"Model: { deployment [ 'model_id' ] } \" ) print ( f \"Service: { deployment [ 'service_type' ] } \" ) print ( f \"Endpoint: { deployment . get ( 'endpoint_name' , 'N/A' ) } \" ) Pipeline Status Example: from emd.sdk.status import get_pipeline_execution_status # Check specific pipeline execution status = get_pipeline_execution_status ( pipeline_execution_id = \"execution-123\" , region = \"us-east-1\" ) print ( f \"Status: { status [ 'status' ] } \" ) print ( f \"Succeeded: { status [ 'is_succeeded' ] } \" ) SageMaker Client Interact with models deployed on Amazon SageMaker. Initialization: from emd.sdk.clients import SageMakerClient # Initialize with model ID client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , region_name = \"us-east-1\" ) # Or initialize with endpoint name directly client = SageMakerClient ( endpoint_name = \"my-sagemaker-endpoint\" , region_name = \"us-east-1\" ) Synchronous Invocation: # Basic chat completion response = client . invoke ({ \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is machine learning?\" } ], \"max_tokens\" : 200 , \"temperature\" : 0.7 }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) Streaming Example: # Stream response tokens for chunk in client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Tell me a story\" }], \"stream\" : True , \"max_tokens\" : 500 }): if chunk . get ( \"choices\" ) and chunk [ \"choices\" ][ 0 ] . get ( \"delta\" , {}) . get ( \"content\" ): print ( chunk [ \"choices\" ][ 0 ][ \"delta\" ][ \"content\" ], end = \"\" ) Asynchronous Invocation: # For long-running tasks async_response = client . invoke_async ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Write a detailed analysis\" }], \"max_tokens\" : 2000 }) # Wait for result result = async_response . get_result () print ( result ) ECS Client Interact with models deployed on Amazon ECS. Python Example: from emd.sdk.clients import ECSClient # Initialize client client = ECSClient ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" ) # Invoke model response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }], \"max_tokens\" : 100 , \"temperature\" : 0.8 }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) Streaming Example: # Stream response from ECS deployment for chunk in client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Explain quantum physics\" }], \"stream\" : True }): print ( chunk , end = \"\" ) Conversation Interface High-level interface for conversational AI interactions. Python Example: from emd.sdk.invoke import ConversationInvoker # Initialize conversation conversation = ConversationInvoker ( \"Qwen2.5-7B-Instruct\" , \"dev\" ) # Set system message conversation . add_system_message ( \"You are a helpful AI assistant.\" ) # Add user message and get response conversation . add_user_message ( \"What is artificial intelligence?\" ) response = conversation . invoke () print ( response ) # Continue conversation conversation . add_assistant_message ( response ) conversation . add_user_message ( \"Can you give me examples?\" ) response = conversation . invoke () print ( response ) Streaming Conversation: # Stream conversation response conversation . add_user_message ( \"Tell me about the future of AI\" ) for chunk in conversation . invoke ( stream = True ): print ( chunk , end = \"\" ) Destroy Deployments Remove deployed models and clean up resources. Function: destroy(model_id, model_tag, waiting_until_complete) Python Example: from emd.sdk.destroy import destroy # Destroy a deployed model destroy ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , waiting_until_complete = True ) Stop Pipeline Example: from emd.sdk.destroy import stop_pipeline_execution # Stop an active deployment pipeline stop_pipeline_execution ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , waiting_until_complete = True ) Embedding Models Work with text embedding models. Python Example: from emd.sdk.clients import SageMakerClient # Initialize embedding model client client = SageMakerClient ( model_id = \"bge-m3\" , model_tag = \"dev\" ) # Get embeddings for single text response = client . invoke ({ \"input\" : \"Machine learning is transforming technology\" , \"normalize\" : True }) embedding = response [ \"data\" ][ 0 ][ \"embedding\" ] print ( f \"Embedding dimension: { len ( embedding ) } \" ) # Get embeddings for multiple texts response = client . invoke ({ \"input\" : [ \"First document text\" , \"Second document text\" , \"Third document text\" ] }) print ( f \"Generated { len ( response [ 'data' ]) } embeddings\" ) Reranking Models Rerank documents based on relevance to a query. Python Example: from emd.sdk.clients import SageMakerClient # Initialize reranking model client client = SageMakerClient ( model_id = \"bge-reranker-v2-m3\" , model_tag = \"dev\" ) # Rerank documents response = client . invoke ({ \"query\" : \"What is machine learning?\" , \"documents\" : [ \"Machine learning is a subset of artificial intelligence.\" , \"Paris is the capital of France.\" , \"Deep learning uses neural networks.\" ], \"max_rerank\" : 3 }) # Print ranked results for i , result in enumerate ( response [ \"data\" ]): print ( f \"Rank { i + 1 } : { result [ 'document' ] } \" ) print ( f \"Score: { result [ 'relevance_score' ] : .4f } \" ) print ( \"---\" ) Vision Models Process images with vision-language models. Python Example: from emd.sdk.clients import SageMakerClient import base64 # Function to encode image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Initialize vision model client client = SageMakerClient ( model_id = \"Qwen2-VL-7B-Instruct\" , model_tag = \"dev\" ) # Process image with text base64_image = encode_image ( \"path/to/image.jpg\" ) response = client . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What's in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" } } ] } ] }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) AWS Lambda Integration Use the SDK in AWS Lambda functions. Lambda Function Example: import json from emd.sdk.clients import SageMakerClient def lambda_handler ( event , context ): # Initialize client client = SageMakerClient ( model_id = event [ 'model_id' ], region_name = context . invoked_function_arn . split ( ':' )[ 3 ] ) # Invoke model response = client . invoke ({ \"messages\" : event [ 'messages' ], \"max_tokens\" : event . get ( 'max_tokens' , 100 ) }) return { 'statusCode' : 200 , 'body' : json . dumps ( response ) } Model Management Lambda: import json from emd.sdk import deploy , destroy , get_model_status def lambda_handler ( event , context ): action = event [ 'action' ] model_id = event [ 'model_id' ] if action == 'deploy' : result = deploy ( model_id = model_id , instance_type = event [ 'instance_type' ], engine_type = event [ 'engine_type' ], service_type = event [ 'service_type' ], waiting_until_deploy_complete = False ) return { 'statusCode' : 200 , 'body' : json . dumps ( result )} elif action == 'destroy' : destroy ( model_id , waiting_until_complete = False ) return { 'statusCode' : 200 , 'body' : json . dumps ({ 'status' : 'initiated' })} elif action == 'status' : status = get_model_status ( model_id ) return { 'statusCode' : 200 , 'body' : json . dumps ( status )} Error Handling Handle common errors when using the SDK. Python Example: from emd.sdk import deploy from emd.sdk.clients import SageMakerClient from botocore.exceptions import ClientError try : # Deploy model result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) # Initialize client client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" ) # Invoke model response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello\" }] }) except RuntimeError as e : print ( f \"Deployment error: { e } \" ) except ValueError as e : print ( f \"Configuration error: { e } \" ) except ClientError as e : print ( f \"AWS error: { e } \" ) except Exception as e : print ( f \"Unexpected error: { e } \" ) Complete Workflow Example End-to-end example of deploying and using a model. Python Example: ```python from emd.sdk import bootstrap, deploy, get_model_status, destroy from emd.sdk.clients import SageMakerClient import time 1. Bootstrap infrastructure print(\"Setting up AWS infrastructure...\") bootstrap() 2. Deploy model print(\"Deploying model...\") deployment = deploy( model_id=\"Qwen2.5-7B-Instruct\", instance_type=\"ml.g5.xlarge\", engine_type=\"vllm\", service_type=\"sagemaker\", extra_params={ \"engine_params\": { \"cli_args\": \"--max_model_len 8000 --max_num_seqs 10\" } } ) print(f\"Deployment started: {deployment['pipeline_execution_id']}\") 3. Wait for deployment to complete print(\"Waiting for deployment...\") while True: status = get_model_status(\"Qwen2.5-7B-Instruct\") if status[\"completed\"]: print(\"Deployment completed!\") break elif status[\"inprogress\"]: print(\"Still deploying...\") time.sleep(30) else: print(\"No deployment found\") break 4. Use the deployed model client = SageMakerClient(model_id=\"Qwen2.5-7B-Instruct\") Test basic functionality response = client.invoke({ \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"} ], \"max_tokens\": 200, \"temperature\": 0.7 }) print(\"Model response:\") print(response[\"choices\"][0][\"message\"][\"content\"]) Test streaming print(\"\\nStreaming response:\") for chunk in client.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Count from 1 to 10\"}], \"stream\": True }): if chunk.get(\"choices\") and chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\"): print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\") 5. Clean up (optional) destroy(\"Qwen2.5-7B-Instruct\")","title":"SDK API"},{"location":"sdk_api/#sdk-api-documentation","text":"Getting Started : The EMD SDK provides a comprehensive Python interface for deploying, managing, and invoking machine learning models on AWS infrastructure. Install the SDK with pip install easy-model-deployer and import the modules you need. Quick example: from emd.sdk import bootstrap , deploy , destroy from emd.sdk.clients import SageMakerClient # Bootstrap infrastructure bootstrap () # Deploy a model result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) # Use the deployed model client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" ) response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }] })","title":"SDK API Documentation"},{"location":"sdk_api/#bootstrap-infrastructure","text":"Initialize AWS resources required for model deployment. Function: bootstrap() Python Example: from emd.sdk.bootstrap import bootstrap # Initialize AWS infrastructure bootstrap () Advanced Example: from emd.sdk.bootstrap import create_env_stack # Create environment stack with custom parameters create_env_stack ( region = \"us-east-1\" , stack_name = \"my-emd-env-stack\" , bucket_name = \"my-emd-bucket\" , force_update = True )","title":"Bootstrap Infrastructure"},{"location":"sdk_api/#deploy-models","text":"Deploy machine learning models to AWS services. Function: deploy(model_id, instance_type, engine_type, service_type, **kwargs) Parameters: model_id (required): Model identifier (e.g., \"Qwen2.5-7B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\") instance_type (required): AWS instance type (e.g., \"ml.g5.xlarge\", \"g5.2xlarge\") engine_type (required): Inference engine (\"vllm\", \"tgi\", \"huggingface\") service_type (required): AWS service (\"sagemaker\", \"ecs\", \"ec2\") framework_type : API framework (default: \"fastapi\") model_tag : Model version tag (default: \"dev\") waiting_until_deploy_complete : Wait for completion (default: True) extra_params : Additional deployment parameters Basic Example: from emd.sdk.deploy import deploy # Deploy a model to SageMaker result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) print ( f \"Deployment ID: { result [ 'pipeline_execution_id' ] } \" ) print ( f \"Model Stack: { result [ 'model_stack_name' ] } \" ) Advanced Example: from emd.sdk.deploy import deploy # Deploy with custom parameters result = deploy ( model_id = \"DeepSeek-R1-Distill-Llama-8B\" , instance_type = \"ml.g5.2xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" , model_tag = \"production\" , extra_params = { \"engine_params\" : { \"cli_args\" : \"--max_model_len 16000 --max_num_seqs 4\" }, \"service_params\" : { \"api_key\" : \"your-secure-api-key\" } } ) Local Deployment Example: from emd.sdk.deploy import deploy_local # Deploy locally for testing deploy_local ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"cpu\" , service_type = \"local\" , engine_type = \"vllm\" , extra_params = { \"temperature\" : 0.7 } )","title":"Deploy Models"},{"location":"sdk_api/#model-status","text":"Check the deployment status of models. Function: get_model_status(model_id, model_tag) Python Example: from emd.sdk.status import get_model_status # Check status of a specific model status = get_model_status ( \"Qwen2.5-7B-Instruct\" , \"dev\" ) # Check in-progress deployments for deployment in status [ \"inprogress\" ]: print ( f \"Model: { deployment [ 'model_id' ] } \" ) print ( f \"Status: { deployment [ 'status' ] } \" ) print ( f \"Stage: { deployment . get ( 'stage_name' , 'N/A' ) } \" ) # Check completed deployments for deployment in status [ \"completed\" ]: print ( f \"Model: { deployment [ 'model_id' ] } \" ) print ( f \"Service: { deployment [ 'service_type' ] } \" ) print ( f \"Endpoint: { deployment . get ( 'endpoint_name' , 'N/A' ) } \" ) Pipeline Status Example: from emd.sdk.status import get_pipeline_execution_status # Check specific pipeline execution status = get_pipeline_execution_status ( pipeline_execution_id = \"execution-123\" , region = \"us-east-1\" ) print ( f \"Status: { status [ 'status' ] } \" ) print ( f \"Succeeded: { status [ 'is_succeeded' ] } \" )","title":"Model Status"},{"location":"sdk_api/#sagemaker-client","text":"Interact with models deployed on Amazon SageMaker. Initialization: from emd.sdk.clients import SageMakerClient # Initialize with model ID client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , region_name = \"us-east-1\" ) # Or initialize with endpoint name directly client = SageMakerClient ( endpoint_name = \"my-sagemaker-endpoint\" , region_name = \"us-east-1\" ) Synchronous Invocation: # Basic chat completion response = client . invoke ({ \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"What is machine learning?\" } ], \"max_tokens\" : 200 , \"temperature\" : 0.7 }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) Streaming Example: # Stream response tokens for chunk in client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Tell me a story\" }], \"stream\" : True , \"max_tokens\" : 500 }): if chunk . get ( \"choices\" ) and chunk [ \"choices\" ][ 0 ] . get ( \"delta\" , {}) . get ( \"content\" ): print ( chunk [ \"choices\" ][ 0 ][ \"delta\" ][ \"content\" ], end = \"\" ) Asynchronous Invocation: # For long-running tasks async_response = client . invoke_async ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Write a detailed analysis\" }], \"max_tokens\" : 2000 }) # Wait for result result = async_response . get_result () print ( result )","title":"SageMaker Client"},{"location":"sdk_api/#ecs-client","text":"Interact with models deployed on Amazon ECS. Python Example: from emd.sdk.clients import ECSClient # Initialize client client = ECSClient ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" ) # Invoke model response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello!\" }], \"max_tokens\" : 100 , \"temperature\" : 0.8 }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) Streaming Example: # Stream response from ECS deployment for chunk in client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Explain quantum physics\" }], \"stream\" : True }): print ( chunk , end = \"\" )","title":"ECS Client"},{"location":"sdk_api/#conversation-interface","text":"High-level interface for conversational AI interactions. Python Example: from emd.sdk.invoke import ConversationInvoker # Initialize conversation conversation = ConversationInvoker ( \"Qwen2.5-7B-Instruct\" , \"dev\" ) # Set system message conversation . add_system_message ( \"You are a helpful AI assistant.\" ) # Add user message and get response conversation . add_user_message ( \"What is artificial intelligence?\" ) response = conversation . invoke () print ( response ) # Continue conversation conversation . add_assistant_message ( response ) conversation . add_user_message ( \"Can you give me examples?\" ) response = conversation . invoke () print ( response ) Streaming Conversation: # Stream conversation response conversation . add_user_message ( \"Tell me about the future of AI\" ) for chunk in conversation . invoke ( stream = True ): print ( chunk , end = \"\" )","title":"Conversation Interface"},{"location":"sdk_api/#destroy-deployments","text":"Remove deployed models and clean up resources. Function: destroy(model_id, model_tag, waiting_until_complete) Python Example: from emd.sdk.destroy import destroy # Destroy a deployed model destroy ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , waiting_until_complete = True ) Stop Pipeline Example: from emd.sdk.destroy import stop_pipeline_execution # Stop an active deployment pipeline stop_pipeline_execution ( model_id = \"Qwen2.5-7B-Instruct\" , model_tag = \"dev\" , waiting_until_complete = True )","title":"Destroy Deployments"},{"location":"sdk_api/#embedding-models","text":"Work with text embedding models. Python Example: from emd.sdk.clients import SageMakerClient # Initialize embedding model client client = SageMakerClient ( model_id = \"bge-m3\" , model_tag = \"dev\" ) # Get embeddings for single text response = client . invoke ({ \"input\" : \"Machine learning is transforming technology\" , \"normalize\" : True }) embedding = response [ \"data\" ][ 0 ][ \"embedding\" ] print ( f \"Embedding dimension: { len ( embedding ) } \" ) # Get embeddings for multiple texts response = client . invoke ({ \"input\" : [ \"First document text\" , \"Second document text\" , \"Third document text\" ] }) print ( f \"Generated { len ( response [ 'data' ]) } embeddings\" )","title":"Embedding Models"},{"location":"sdk_api/#reranking-models","text":"Rerank documents based on relevance to a query. Python Example: from emd.sdk.clients import SageMakerClient # Initialize reranking model client client = SageMakerClient ( model_id = \"bge-reranker-v2-m3\" , model_tag = \"dev\" ) # Rerank documents response = client . invoke ({ \"query\" : \"What is machine learning?\" , \"documents\" : [ \"Machine learning is a subset of artificial intelligence.\" , \"Paris is the capital of France.\" , \"Deep learning uses neural networks.\" ], \"max_rerank\" : 3 }) # Print ranked results for i , result in enumerate ( response [ \"data\" ]): print ( f \"Rank { i + 1 } : { result [ 'document' ] } \" ) print ( f \"Score: { result [ 'relevance_score' ] : .4f } \" ) print ( \"---\" )","title":"Reranking Models"},{"location":"sdk_api/#vision-models","text":"Process images with vision-language models. Python Example: from emd.sdk.clients import SageMakerClient import base64 # Function to encode image def encode_image ( image_path ): with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( 'utf-8' ) # Initialize vision model client client = SageMakerClient ( model_id = \"Qwen2-VL-7B-Instruct\" , model_tag = \"dev\" ) # Process image with text base64_image = encode_image ( \"path/to/image.jpg\" ) response = client . invoke ({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What's in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" } } ] } ] }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ])","title":"Vision Models"},{"location":"sdk_api/#aws-lambda-integration","text":"Use the SDK in AWS Lambda functions. Lambda Function Example: import json from emd.sdk.clients import SageMakerClient def lambda_handler ( event , context ): # Initialize client client = SageMakerClient ( model_id = event [ 'model_id' ], region_name = context . invoked_function_arn . split ( ':' )[ 3 ] ) # Invoke model response = client . invoke ({ \"messages\" : event [ 'messages' ], \"max_tokens\" : event . get ( 'max_tokens' , 100 ) }) return { 'statusCode' : 200 , 'body' : json . dumps ( response ) } Model Management Lambda: import json from emd.sdk import deploy , destroy , get_model_status def lambda_handler ( event , context ): action = event [ 'action' ] model_id = event [ 'model_id' ] if action == 'deploy' : result = deploy ( model_id = model_id , instance_type = event [ 'instance_type' ], engine_type = event [ 'engine_type' ], service_type = event [ 'service_type' ], waiting_until_deploy_complete = False ) return { 'statusCode' : 200 , 'body' : json . dumps ( result )} elif action == 'destroy' : destroy ( model_id , waiting_until_complete = False ) return { 'statusCode' : 200 , 'body' : json . dumps ({ 'status' : 'initiated' })} elif action == 'status' : status = get_model_status ( model_id ) return { 'statusCode' : 200 , 'body' : json . dumps ( status )}","title":"AWS Lambda Integration"},{"location":"sdk_api/#error-handling","text":"Handle common errors when using the SDK. Python Example: from emd.sdk import deploy from emd.sdk.clients import SageMakerClient from botocore.exceptions import ClientError try : # Deploy model result = deploy ( model_id = \"Qwen2.5-7B-Instruct\" , instance_type = \"ml.g5.xlarge\" , engine_type = \"vllm\" , service_type = \"sagemaker\" ) # Initialize client client = SageMakerClient ( model_id = \"Qwen2.5-7B-Instruct\" ) # Invoke model response = client . invoke ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello\" }] }) except RuntimeError as e : print ( f \"Deployment error: { e } \" ) except ValueError as e : print ( f \"Configuration error: { e } \" ) except ClientError as e : print ( f \"AWS error: { e } \" ) except Exception as e : print ( f \"Unexpected error: { e } \" )","title":"Error Handling"},{"location":"sdk_api/#complete-workflow-example","text":"End-to-end example of deploying and using a model. Python Example: ```python from emd.sdk import bootstrap, deploy, get_model_status, destroy from emd.sdk.clients import SageMakerClient import time","title":"Complete Workflow Example"},{"location":"sdk_api/#1-bootstrap-infrastructure","text":"print(\"Setting up AWS infrastructure...\") bootstrap()","title":"1. Bootstrap infrastructure"},{"location":"sdk_api/#2-deploy-model","text":"print(\"Deploying model...\") deployment = deploy( model_id=\"Qwen2.5-7B-Instruct\", instance_type=\"ml.g5.xlarge\", engine_type=\"vllm\", service_type=\"sagemaker\", extra_params={ \"engine_params\": { \"cli_args\": \"--max_model_len 8000 --max_num_seqs 10\" } } ) print(f\"Deployment started: {deployment['pipeline_execution_id']}\")","title":"2. Deploy model"},{"location":"sdk_api/#3-wait-for-deployment-to-complete","text":"print(\"Waiting for deployment...\") while True: status = get_model_status(\"Qwen2.5-7B-Instruct\") if status[\"completed\"]: print(\"Deployment completed!\") break elif status[\"inprogress\"]: print(\"Still deploying...\") time.sleep(30) else: print(\"No deployment found\") break","title":"3. Wait for deployment to complete"},{"location":"sdk_api/#4-use-the-deployed-model","text":"client = SageMakerClient(model_id=\"Qwen2.5-7B-Instruct\")","title":"4. Use the deployed model"},{"location":"sdk_api/#test-basic-functionality","text":"response = client.invoke({ \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"} ], \"max_tokens\": 200, \"temperature\": 0.7 }) print(\"Model response:\") print(response[\"choices\"][0][\"message\"][\"content\"])","title":"Test basic functionality"},{"location":"sdk_api/#test-streaming","text":"print(\"\\nStreaming response:\") for chunk in client.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Count from 1 to 10\"}], \"stream\": True }): if chunk.get(\"choices\") and chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\"): print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\")","title":"Test streaming"},{"location":"sdk_api/#5-clean-up-optional","text":"","title":"5. Clean up (optional)"},{"location":"sdk_api/#destroyqwen25-7b-instruct","text":"","title":"destroy(\"Qwen2.5-7B-Instruct\")"},{"location":"sdk_integration/","text":"LangChain Integration This guide covers how to integrate with deployed models using the LangChain framework. LLM Models from emd.integrations.langchain_clients import SageMakerVllmChatModel from langchain_core.output_parsers import StrOutputParser from langchain_core.messages import HumanMessage , AIMessage , SystemMessage # Initialize the chat model chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"temperature\" : 0.5 , } ) # Create a simple chain chain = chat_model | StrOutputParser () # Define messages messages = [ HumanMessage ( content = \"What is the capital of France?\" ), ] # Invoke the chain response = chain . invoke ( messages ) print ( response ) Function Calling with LangChain from langchain.tools.base import StructuredTool from langchain_core.utils.function_calling import ( convert_to_openai_function , convert_to_openai_tool ) # Define a function def get_weather ( location : str , unit : str = \"celsius\" ) -> str : \"\"\"Get the current weather in a given location\"\"\" # This would call a weather API in a real application return f \"The weather in { location } is sunny and 25 degrees { unit } \" # Create a tool weather_tool = StructuredTool . from_function ( get_weather ) # Convert to OpenAI tool format openai_tool = convert_to_openai_tool ( weather_tool ) # Initialize the model with tools chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"tools\" : [ openai_tool ], \"tool_choice\" : \"auto\" } ) # Invoke with a query that should trigger tool use messages = [ HumanMessage ( content = \"What's the weather like in Paris?\" ) ] response = chat_model . invoke ( messages ) print ( response ) Embedding Models import time from emd.integrations.langchain_clients import SageMakerVllmEmbeddings # Initialize the embedding model embedding_model = SageMakerVllmEmbeddings ( model_id = \"bge-m3\" , ) # Get embeddings for a single text text = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' embedding = embedding_model . embed_query ( text ) # Get embeddings for multiple documents documents = [ text ] * 10 # 10 copies of the same text for demonstration embeddings = embedding_model . embed_documents ( documents ) print ( f \"Single embedding dimension: { len ( embedding ) } \" ) print ( f \"Number of document embeddings: { len ( embeddings ) } \" ) Reranking Models from emd.integrations.langchain_clients import SageMakerVllmRerank # Initialize the reranker rerank_model = SageMakerVllmRerank ( model_id = \"bge-reranker-v2-m3\" ) # Define documents and query docs = [ \"The giant panda is a bear species endemic to China.\" , \"Paris is the capital of France.\" , \"Machine learning is a subset of artificial intelligence.\" ] query = 'What is a panda?' # Rerank documents based on relevance to the query results = rerank_model . rerank ( query = query , documents = docs ) # Print results for result in results : print ( f \"Document: { result . document } \" ) print ( f \"Score: { result . relevance_score } \" ) print ( \"---\" ) Vision Models (VLM) For vision models, you can use the EMD CLI to invoke them: # Upload an image to S3 aws s3 cp image.jpg s3://your-bucket/image.jpg # Invoke the vision model emd invoke Qwen2-VL-7B-Instruct When prompted: - Enter the S3 path to your image: s3://your-bucket/image.jpg - Enter your prompt: What's in this image?","title":"LangChain Integration"},{"location":"sdk_integration/#langchain-integration","text":"This guide covers how to integrate with deployed models using the LangChain framework.","title":"LangChain Integration"},{"location":"sdk_integration/#llm-models","text":"from emd.integrations.langchain_clients import SageMakerVllmChatModel from langchain_core.output_parsers import StrOutputParser from langchain_core.messages import HumanMessage , AIMessage , SystemMessage # Initialize the chat model chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"temperature\" : 0.5 , } ) # Create a simple chain chain = chat_model | StrOutputParser () # Define messages messages = [ HumanMessage ( content = \"What is the capital of France?\" ), ] # Invoke the chain response = chain . invoke ( messages ) print ( response )","title":"LLM Models"},{"location":"sdk_integration/#function-calling-with-langchain","text":"from langchain.tools.base import StructuredTool from langchain_core.utils.function_calling import ( convert_to_openai_function , convert_to_openai_tool ) # Define a function def get_weather ( location : str , unit : str = \"celsius\" ) -> str : \"\"\"Get the current weather in a given location\"\"\" # This would call a weather API in a real application return f \"The weather in { location } is sunny and 25 degrees { unit } \" # Create a tool weather_tool = StructuredTool . from_function ( get_weather ) # Convert to OpenAI tool format openai_tool = convert_to_openai_tool ( weather_tool ) # Initialize the model with tools chat_model = SageMakerVllmChatModel ( model_id = \"Qwen2.5-7B-Instruct\" , model_kwargs = { \"tools\" : [ openai_tool ], \"tool_choice\" : \"auto\" } ) # Invoke with a query that should trigger tool use messages = [ HumanMessage ( content = \"What's the weather like in Paris?\" ) ] response = chat_model . invoke ( messages ) print ( response )","title":"Function Calling with LangChain"},{"location":"sdk_integration/#embedding-models","text":"import time from emd.integrations.langchain_clients import SageMakerVllmEmbeddings # Initialize the embedding model embedding_model = SageMakerVllmEmbeddings ( model_id = \"bge-m3\" , ) # Get embeddings for a single text text = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' embedding = embedding_model . embed_query ( text ) # Get embeddings for multiple documents documents = [ text ] * 10 # 10 copies of the same text for demonstration embeddings = embedding_model . embed_documents ( documents ) print ( f \"Single embedding dimension: { len ( embedding ) } \" ) print ( f \"Number of document embeddings: { len ( embeddings ) } \" )","title":"Embedding Models"},{"location":"sdk_integration/#reranking-models","text":"from emd.integrations.langchain_clients import SageMakerVllmRerank # Initialize the reranker rerank_model = SageMakerVllmRerank ( model_id = \"bge-reranker-v2-m3\" ) # Define documents and query docs = [ \"The giant panda is a bear species endemic to China.\" , \"Paris is the capital of France.\" , \"Machine learning is a subset of artificial intelligence.\" ] query = 'What is a panda?' # Rerank documents based on relevance to the query results = rerank_model . rerank ( query = query , documents = docs ) # Print results for result in results : print ( f \"Document: { result . document } \" ) print ( f \"Score: { result . relevance_score } \" ) print ( \"---\" )","title":"Reranking Models"},{"location":"sdk_integration/#vision-models-vlm","text":"For vision models, you can use the EMD CLI to invoke them: # Upload an image to S3 aws s3 cp image.jpg s3://your-bucket/image.jpg # Invoke the vision model emd invoke Qwen2-VL-7B-Instruct When prompted: - Enter the S3 path to your image: s3://your-bucket/image.jpg - Enter your prompt: What's in this image?","title":"Vision Models (VLM)"},{"location":"supported_models/","text":"ModeId ModelSeries ModelType Supported Instances Supported Services Support China Region glm-4-9b-chat glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 GLM-4-9B-0414 glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 GLM-4-32B-0414 glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 GLM-Z1-9B-0414 glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 GLM-Z1-32B-0414 glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 GLM-Z1-Rumination-32B-0414 glm4 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-20b-chat-4bit-awq internlm2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-20b-chat internlm2.5 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-7b-chat internlm2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-7b-chat-4bit internlm2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e internlm2_5-1_8b-chat internlm2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-7B-Instruct qwen2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ qwen2.5 llm g5.12xlarge,g5.24xlarge,g5.48xlarge,inf2.24xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct qwen2.5 llm g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ-128k qwen2.5 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-32B-Instruct qwen2.5 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-0.5B-Instruct qwen2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-1.5B-Instruct qwen2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-3B-Instruct qwen2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct-AWQ qwen2.5 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct qwen2.5 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 QwQ-32B-Preview qwen reasoning model llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 QwQ-32B qwen reasoning model llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-8B qwen3 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-0.6B qwen3 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-1.7B qwen3 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-4B qwen3 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-14B-AWQ qwen3 llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-14B qwen3 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-32B-AWQ qwen3 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-32B qwen3 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-30B-A3B qwen3 llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen3-235B-A22B qwen3 llm \u2705 Qwen3-235B-A22B-FP8 qwen3 llm \u2705 llama-3.3-70b-instruct-awq llama llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-32B deepseek reasoning model llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-14B deepseek reasoning model llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-7B deepseek reasoning model llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B deepseek reasoning model llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B_ollama deepseek reasoning model llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B-GGUF deepseek reasoning model llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-32B-GGUF deepseek reasoning model llm g5.12xlarge,g5.24xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Llama-8B deepseek reasoning model llm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-distill-llama-70b-awq deepseek reasoning model llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-671b-1.58bit_gguf deepseek reasoning model llm g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.8xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.4xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-671b-2.51bit_gguf deepseek reasoning model llm g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1 deepseek reasoning model llm \u2705 deepseek-r1-671b-4bit_gguf deepseek reasoning model llm g5.24xlarge,g5.48xlarge,g6.24xlarge,g6.48xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-0528-Qwen3-8B deepseek reasoning model llm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-v3-UD-IQ1_M_ollama deepseek v3 llm g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e Baichuan-M1-14B-Instruct baichuan llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 ReaderLM-v2 jina llm g4dn.2xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 txgemma-9b-chat txgemma llm g5.12xlarge,g5.24xlarge,g5.48xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 txgemma-27b-chat txgemma llm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2-VL-72B-Instruct-AWQ qwen2vl vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 Qwen2.5-VL-72B-Instruct-AWQ qwen2vl vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 Qwen2.5-VL-32B-Instruct qwen2vl vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 QVQ-72B-Preview-AWQ qwen reasoning model vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u274e Qwen2-VL-7B-Instruct qwen2vl vlm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker_realtime,sagemaker_async \u2705 UI-TARS-1.5-7B agent vlm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker_realtime,sagemaker_async \u2705 InternVL2_5-78B-AWQ internvl2.5 vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u274e gemma-3-4b-it gemma3 vlm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 gemma-3-12b-it gemma3 vlm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 gemma-3-27b-it gemma3 vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Mistral-Small-3.1-24B-Instruct-2503 mistral vlm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e txt2video-LTX comfyui video g5.4xlarge,g5.8xlarge,g6e.2xlarge sagemaker_async \u274e whisper whisper whisper g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_async \u274e bce-embedding-base_v1 bce embedding g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-base-en-v1.5 bge embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-m3 bge embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 jina-embeddings-v3 jina embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 jina-embeddings-v4 jina embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 Qwen3-Embedding-0.6B qwen3 embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 Qwen3-Embedding-4B qwen3 embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 Qwen3-Embedding-8B qwen3 embedding g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-reranker-v2-m3 bge rerank g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-reranker-large bge rerank g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 jina-reranker-v2-base-multilingual jina rerank g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705","title":"Supported Models"},{"location":"swift_chat_integration/","text":"SwiftChat Integration This guide covers how to integrate EMD-deployed models with SwiftChat , a fast and responsive cross-platform AI chat application. Overview SwiftChat is a cross-platform AI chat application developed with React Native that supports multiple model providers, including OpenAI Compatible models. This makes it an excellent client for interacting with models deployed using Easy Model Deployer (EMD). With SwiftChat, you can: - Chat with your EMD-deployed models in real-time with streaming responses - Use rich markdown features including tables, code blocks, and LaTeX - Access your models across Android, iOS, and macOS platforms - Enjoy a fast, responsive, and privacy-focused experience Key Features of SwiftChat Real-time streaming chat with AI Rich Markdown support (tables, code blocks, LaTeX, etc.) AI image generation with progress indicators Multimodal support (images, videos & documents) Conversation history management Cross-platform support (Android, iOS, macOS) Tablet-optimized for iPad and Android tablets Multiple AI model providers supported Fully customizable system prompts Integrating EMD Models with SwiftChat EMD-deployed models can be easily integrated with SwiftChat through its OpenAI Compatible interface. This allows you to use your own deployed models with all the features and convenience of the SwiftChat application. Prerequisites You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed SwiftChat on your device (Android, iOS, or macOS) You have the base URL and API key for your deployed model Configuration Steps Open SwiftChat on your device Navigate to the Settings page Select the OpenAI tab Under OpenAI Compatible, enter the following information: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model ID : The ID of your deployed model(s), separate multiple models with commas Select one of your models from the Text Model dropdown list Example Use Cases Once configured, you can use your EMD-deployed models in SwiftChat for various use cases: Text Generation : Chat with your models using the real-time streaming interface Code Generation : Ask your models to write or explain code, with proper syntax highlighting Document Analysis : Upload documents for your models to analyze (if multimodal models are supported) Image Understanding : Share images with vision-enabled models for analysis and discussion Troubleshooting If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check network connectivity between your device and the model endpoint Additional Resources SwiftChat GitHub Repository EMD Supported Models","title":"SwiftChat Integration"},{"location":"swift_chat_integration/#swiftchat-integration","text":"This guide covers how to integrate EMD-deployed models with SwiftChat , a fast and responsive cross-platform AI chat application.","title":"SwiftChat Integration"},{"location":"swift_chat_integration/#overview","text":"SwiftChat is a cross-platform AI chat application developed with React Native that supports multiple model providers, including OpenAI Compatible models. This makes it an excellent client for interacting with models deployed using Easy Model Deployer (EMD). With SwiftChat, you can: - Chat with your EMD-deployed models in real-time with streaming responses - Use rich markdown features including tables, code blocks, and LaTeX - Access your models across Android, iOS, and macOS platforms - Enjoy a fast, responsive, and privacy-focused experience","title":"Overview"},{"location":"swift_chat_integration/#key-features-of-swiftchat","text":"Real-time streaming chat with AI Rich Markdown support (tables, code blocks, LaTeX, etc.) AI image generation with progress indicators Multimodal support (images, videos & documents) Conversation history management Cross-platform support (Android, iOS, macOS) Tablet-optimized for iPad and Android tablets Multiple AI model providers supported Fully customizable system prompts","title":"Key Features of SwiftChat"},{"location":"swift_chat_integration/#integrating-emd-models-with-swiftchat","text":"EMD-deployed models can be easily integrated with SwiftChat through its OpenAI Compatible interface. This allows you to use your own deployed models with all the features and convenience of the SwiftChat application.","title":"Integrating EMD Models with SwiftChat"},{"location":"swift_chat_integration/#prerequisites","text":"You have successfully deployed a model using EMD with the OpenAI Compatible API enabled You have installed SwiftChat on your device (Android, iOS, or macOS) You have the base URL and API key for your deployed model","title":"Prerequisites"},{"location":"swift_chat_integration/#configuration-steps","text":"Open SwiftChat on your device Navigate to the Settings page Select the OpenAI tab Under OpenAI Compatible, enter the following information: Base URL : The endpoint URL of your EMD-deployed model (e.g., https://your-endpoint.execute-api.region.amazonaws.com ) API Key : Your API key for accessing the model Model ID : The ID of your deployed model(s), separate multiple models with commas Select one of your models from the Text Model dropdown list","title":"Configuration Steps"},{"location":"swift_chat_integration/#example-use-cases","text":"Once configured, you can use your EMD-deployed models in SwiftChat for various use cases: Text Generation : Chat with your models using the real-time streaming interface Code Generation : Ask your models to write or explain code, with proper syntax highlighting Document Analysis : Upload documents for your models to analyze (if multimodal models are supported) Image Understanding : Share images with vision-enabled models for analysis and discussion","title":"Example Use Cases"},{"location":"swift_chat_integration/#troubleshooting","text":"If you encounter issues connecting to your EMD-deployed model: Verify that your model is properly deployed and running Check that the Base URL is correct and includes the full endpoint path Ensure your API key has the necessary permissions Confirm that your model ID exactly matches the deployed model's identifier Check network connectivity between your device and the model endpoint","title":"Troubleshooting"},{"location":"swift_chat_integration/#additional-resources","text":"SwiftChat GitHub Repository EMD Supported Models","title":"Additional Resources"}]}