{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Easy Model Deployer - Simple, Efficient, and Easy-to-Integrate  <p>EMD (Easy Model Deployer) is a lightweight tool designed to simplify model deployment. Built for developers who need reliable and scalable model serving without complex setup.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p> <p></p> <ol> <li> <p>User/Client initiates model deployment task, triggering pipeline to start model building.</p> </li> <li> <p>AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR.</p> </li> <li> <p>AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).</p> </li> </ol>"},{"location":"deployment/","title":"Deployment parameters","text":""},{"location":"deployment/#-force-update-env-stack","title":"--force-update-env-stack","text":"<p>No additional <code>emd bootstrap</code> required for deployment. Because of other commands, status/destroy etc. require pre-bootstrapping. Therefore, it is recommended to run <code>emd bootstrap</code> separately after each upgrade.</p>"},{"location":"deployment/#-allow-local-deploy","title":"--allow-local-deploy","text":"<p>Extra parameters passed to the model deployment. extra-params should be a Json object of dictionary format as follows:</p> <pre><code>{\n\n  \"model_params\": {\n  },\n  \"service_params\":{\n  },\n  \"instance_params\":{\n  },\n  \"engine_params\":{\n      \"cli_args\": \"&lt;command line arguments of current engine&gt;\",\n      \"api_key\":\"&lt;api key&gt;\"\n  },\n  \"framework_params\":{\n      \"uvicorn_log_level\":\"info\",\n      \"limit_concurrency\":200\n  }\n}\n</code></pre>"},{"location":"deployment/#local-deployment-on-the-ec2-instance","title":"Local deployment on the ec2 instance","text":"<p>This is suitable for deploying models using local GPU resources.</p>"},{"location":"deployment/#pre-requisites","title":"Pre-requisites","text":""},{"location":"deployment/#start-and-connect-to-ec2-instance","title":"Start and connect to EC2 instance","text":"<p>It is recommended to launch the instance using the AMI \"Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.6 (Ubuntu 22.04)\".</p>"},{"location":"deployment/#deploy-model-using-emd","title":"Deploy model using EMD","text":"<pre><code>emd deploy --allow-local-deploy\n</code></pre> <p>There some EMD configuration sample settings for model deployment in the following two sections: Non-reasoning Model deployment configuration and Reasoning Model deployment configuration. Wait for the model deployment to complete.</p>"},{"location":"deployment/#non-reasoning-model-deployment-configuration","title":"Non-reasoning Model deployment configuration","text":""},{"location":"deployment/#qwen25-72b-instruct-awq","title":"Qwen2.5-72B-Instruct-AWQ","text":"<pre><code>? Select the model series: qwen2.5\n? Select the model name: Qwen2.5-72B-Instruct-AWQ\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\n? Select the inference engine to use: tgi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"deployment/#llama-33-70b-instruct-awq","title":"llama-3.3-70b-instruct-awq","text":"<pre><code>? Select the model series: llama\n? Select the model name: llama-3.3-70b-instruct-awq\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\nengine type: tgi\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"deployment/#reasoning-model-deployment-configuration","title":"Reasoning Model deployment configuration","text":""},{"location":"deployment/#deepseek-r1-distill-qwen-32b","title":"DeepSeek-R1-Distill-Qwen-32B","text":"<pre><code>? Select the model series: deepseek reasoning model\n? Select the model name: DeepSeek-R1-Distill-Qwen-32B\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\nengine type: vllm\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--enable-reasoning --reasoning-parser deepseek_r1 --max_model_len 16000 --disable-log-stats --chat-template emd/models/chat_templates/deepseek_r1_distill.jinja --max_num_seq 20 --gpu_memory_utilization 0.9\"}}\n</code></pre>"},{"location":"deployment/#deepseek-r1-distill-llama-70b-awq","title":"deepseek-r1-distill-llama-70b-awq","text":"<pre><code>? Select the model series: deepseek reasoning model\n? Select the model name: deepseek-r1-distill-llama-70b-awq\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\n? Select the inference engine to use: tgi\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"emd_client/","title":"Usse EMD client to invoke deployed models","text":"<pre><code>emd invoke MODEL_ID MODEL_TAG (Optional)\n</code></pre>"},{"location":"emd_client/#llm-models","title":"LLM models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"emd_client/#vlm-models","title":"VLM models","text":"<ol> <li> <p>upload image to a s3 path  <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model Qwen2-VL-7B-Instruct with tag dev\nEnter image path(local or s3 file): s3://your-bucket/image.jpg\nEnter prompt: What's in this image?\n...\n</code></pre></p> </li> </ol>"},{"location":"emd_client/#videotxt2video-models","title":"Video(Txt2Video) models","text":"<ol> <li>input prompt for video generation <pre><code>emd invoke txt2video-LTX\n...\nInvoking model txt2video-LTX with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.\n...\n</code></pre></li> <li>download generated video from output_path</li> </ol>"},{"location":"emd_client/#embedding-models","title":"Embedding models","text":"<pre><code>emd invoke bge-base-en-v1.5\n...\nInvoking model bge-base-en-v1.5 with tag dev\nEnter the sentence: hello\n...\n</code></pre>"},{"location":"emd_client/#rerank-models","title":"Rerank models","text":"<pre><code>emd invoke bge-reranker-v2-m3\n...\nEnter the text_a (string): What is the capital of France?\nEnter the text_b (string): The capital of France is Paris.\n...\n</code></pre>"},{"location":"emd_client/#asr-modelswhisper","title":"ASR models(whisper)","text":"<ol> <li> <p>upload audio to a s3 path <pre><code>aws s3 cp xx.wav s3://your-bucket/xx.wav\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke whisper\n...\nEnter the s3 path to the audio file: s3://your-bucket/xx.wav\nEnter model [large-v3-turbo/large-v3]: large-v3-turbo\n...\n</code></pre></p> </li> </ol>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"installation/#setting-up-the-environment","title":"Setting up the Environment","text":"<ol> <li> <p>Create a virtual environment: <pre><code>python -m venv emd-env\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>source emd-env/bin/activate\n</code></pre></p> </li> <li> <p>Install the required packages: <pre><code>pip install https://github.com/aws-samples/easy-model-deployer/releases/download/main/emd-0.6.0-py3-none-any.whl\n</code></pre></p> </li> </ol>"},{"location":"langchain_interface/","title":"Usse Langchain interface to invoke deployed models","text":""},{"location":"langchain_interface/#llm-models","title":"LLM models","text":"<pre><code>from emd.integrations.langchain_clients import SageMakerVllmChatModel\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.messages import HumanMessage,AIMessage,SystemMessage\nfrom langchain.tools.base import StructuredTool\nfrom langchain_core.utils.function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool\n)\nchat_model = SageMakerVllmChatModel(\n    model_id=\"Qwen2.5-7B-Instruct\",\n    model_kwargs={\n        \"temperature\":0.5,\n    }\n)\nchain = chat_model | StrOutputParser()\nmessages = [\n        HumanMessage(content=\"9.11\u548c9.9\u4e24\u4e2a\u6570\u5b57\u54ea\u4e2a\u66f4\u5927\uff1f\"),\n    ]\nprint(chain.invoke(messages))\n</code></pre>"},{"location":"langchain_interface/#vlm-models","title":"VLM models","text":"<ol> <li> <p>upload image to a s3 path <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model Qwen2-VL-7B-Instruct with tag dev\nEnter image path(local or s3 file): s3://your-bucket/image.jpg\nEnter prompt: What's in this image?\n...\n</code></pre></p> </li> </ol>"},{"location":"langchain_interface/#videotxt2video-models","title":"Video(Txt2Video) models","text":"<p>Not supported</p>"},{"location":"langchain_interface/#embedding-models","title":"Embedding models","text":"<pre><code>import time\nfrom emd.integrations.langchain_clients import SageMakerVllmEmbeddings\nfrom emd.integrations.langchain_clients import SageMakerVllmRerank\nembedding_model = SageMakerVllmEmbeddings(\n    model_id=\"bge-m3\",\n)\ntext = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'\nt0 = time.time()\nr1 = embedding_model.embed_query(text)\nt1 = time.time()\nembedding_model.embed_documents([text]*1000)\nt2 = time.time()\nprint(f\"embed_query: {t1-t0}\")\nprint(f\"embed_documents: {t2-t1}\")\n</code></pre>"},{"location":"langchain_interface/#rerank-models","title":"Rerank models","text":"<pre><code>import time\nfrom emd.integrations.langchain_clients import SageMakerVllmRerank\ndocs = [\"hi\",'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']\nquery = 'what is panda?'\nrerank_model = SageMakerVllmRerank(\n    model_id=\"bge-reranker-v2-m3\"\n)\nprint(rerank_model.rerank(query=query,documents=docs))\n</code></pre>"},{"location":"openai_compatiable/","title":"Test OpenAI compatible interface","text":""},{"location":"openai_compatiable/#sample-code","title":"Sample Code","text":"<pre><code>import openai\n# Change the api_key here to the parameter you passed in via extra-parameter\napi_key = \"your_openai_api_key\"\ndef chat_with_openai_stream(prompt):\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=\"http://ec2-54-189-171-204.us-west-2.compute.amazonaws.com:8080/v1\"\n    )\n    response = client.chat.completions.create(\n        model=\"Qwen2.5-72B-Instruct-AWQ\",\n        # model=\"Qwen2.5-1.5B-Instruct\",\n        messages=[\n        {\"role\": \"user\", \"content\": prompt}\n        ],\n        stream=True,\n        temperature=0.6\n    )\n    print(\"AI: \", end=\"\", flush=True)\n    print(response)\n    for chunk in response:\n        content = chunk.choices[0].delta.content\n        think = getattr(chunk.choices[0].delta,\"reasoning_content\",None)\n        if think is not None:\n            print(think,end=\"\",flush=True)\n        else:\n            print(content, end=\"\", flush=True)\n    print(\"\\n\")\n\ndef chat_with_openai(prompt):\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=\"http://127.0.0.1:9000/v1\"\n    )\n    response = client.chat.completions.create(\n        model=\"DeepSeek-R1-Distill-Qwen-1.5B\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}],\n        stream=False\n    )\n    print(response)\n\n# Test the stream and non-stream interface\nchat_with_openai_stream(\"What is the capital of France?\")\nchat_with_openai(\"What is the capital of France?\")\n</code></pre>"},{"location":"supported_models/","title":"Supported Model","text":"ModeId ModelSeries ModelType Supported Engines Supported Instances Supported Services Support China Region glm-4-9b-chat glm4 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat-4bit-awq internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat internlm2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat-4bit internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e internlm2_5-1_8b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-7B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ qwen2.5 llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct qwen2.5 llm vllm g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ-128k qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-32B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-0.5B-Instruct qwen2.5 llm vllm,tgi g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-1.5B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-3B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct-AWQ qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 QwQ-32B-Preview qwen reasoning model llm huggingface,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 llama-3.3-70b-instruct-awq llama llm tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-32B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-14B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-7B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-1.5B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Llama-8B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e deepseek-r1-distill-llama-70b-awq deepseek reasoning model llm tgi,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Baichuan-M1-14B-Instruct baichuan llm huggingface g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e Qwen2-VL-72B-Instruct-AWQ qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u2705 QVQ-72B-Preview-AWQ qwen reasoning model vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e Qwen2-VL-7B-Instruct qwen2vl vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker,sagemaker_async \u2705 InternVL2_5-78B-AWQ internvl2.5 vlm lmdeploy g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e txt2video-LTX comfyui video comfyui g5.4xlarge,g5.8xlarge,g6e.2xlarge sagemaker_async \u274e whisper whisper whisper huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_async \u274e bge-base-en-v1.5 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705 bge-m3 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,ecs \u2705 bge-reranker-v2-m3 bge rerank vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705"}]}