{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Easy Model Deployer - Simple, Efficient, and Easy-to-Integrate"},{"location":"#about","title":"About","text":"<p>EMD (Easy Model Deployer) is a lightweight tool designed to simplify model deployment. Built for developers who need reliable and scalable model serving without complex setup.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p> <p></p> <ol> <li>User/Client initiates model deployment task, triggering pipeline to start model building.</li> <li>AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR.</li> <li>AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).</li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p> <p></p> <ol> <li> <p>User/Client initiates model deployment task, triggering pipeline to start model building.</p> </li> <li> <p>AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR.</p> </li> <li> <p>AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).</p> </li> </ol>"},{"location":"emd_client/","title":"Usse EMD client to invoke deployed models","text":"<pre><code>emd invoke MODEL_ID MODEL_TAG (Optional)\n</code></pre>"},{"location":"emd_client/#llm-models","title":"LLM models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"emd_client/#vlm-models","title":"VLM models","text":"<ol> <li> <p>upload image to a s3 path  <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model Qwen2-VL-7B-Instruct with tag dev\nEnter image path(local or s3 file): s3://your-bucket/image.jpg\nEnter prompt: What's in this image?\n...\n</code></pre></p> </li> </ol>"},{"location":"emd_client/#videotxt2edding-models","title":"Video(Txt2edding) models","text":"<ol> <li>input prompt for video generation <pre><code>emd invoke txt2video-LTX\n...\nInvoking model txt2video-LTX with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.\n...\n</code></pre></li> <li>download generated video from output_path</li> </ol>"},{"location":"emd_client/#embedding-models","title":"Embedding models","text":"<pre><code>emd invoke bge-base-en-v1.5\n...\nInvoking model bge-base-en-v1.5 with tag dev\nEnter the sentence: hello\n...\n</code></pre>"},{"location":"emd_client/#rerank-models","title":"Rerank models","text":"<pre><code>emd invoke bge-reranker-v2-m3\n...\nEnter the text_a (string): What is the capital of France?\nEnter the text_b (string): The capital of France is Paris.\n...\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"installation/#setting-up-the-environment","title":"Setting up the Environment","text":"<ol> <li> <p>Create a virtual environment: <pre><code>python -m venv emd-env\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>source emd-env/bin/activate\n</code></pre></p> </li> <li> <p>Install the required packages: <pre><code>pip install https://github.com/aws-samples/easy-model-deployer/releases/download/main/emd-0.6.0-py3-none-any.whl\n</code></pre></p> </li> </ol>"},{"location":"langchain_interface/","title":"Invocation guidelines","text":""},{"location":"langchain_interface/#use-emd-to-invoke-model","title":"Use EMD to invoke model","text":"<pre><code>emd invoke MODEL_ID MODEL_TAG (Optional)\n</code></pre>"},{"location":"langchain_interface/#for-llm-models","title":"For LLM models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"langchain_interface/#for-vlm-models","title":"For VLM models","text":"<ol> <li> <p>upload image to a s3 path  <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\n...\n</code></pre></p> </li> </ol>"},{"location":"langchain_interface/#for-embedding-models","title":"For Embedding models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"langchain_interface/#for-embedding-models_1","title":"For Embedding models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"langchain_interface/#f","title":"F","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p>"},{"location":"langchain_interface/#use-the-langchain-interface-to-invoke-model","title":"Use the langchain interface to invoke model","text":""},{"location":"langchain_interface/#use-the","title":"Use the","text":""},{"location":"openai_compatiable/","title":"Invocation guidelines","text":""},{"location":"openai_compatiable/#use-emd-to-invoke-model","title":"Use EMD to invoke model","text":"<pre><code>emd invoke MODEL_ID MODEL_TAG (Optional)\n</code></pre>"},{"location":"openai_compatiable/#for-llm-models","title":"For LLM models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"openai_compatiable/#for-vlm-models","title":"For VLM models","text":"<ol> <li> <p>upload image to a s3 path  <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\n...\n</code></pre></p> </li> </ol>"},{"location":"openai_compatiable/#for-embedding-models","title":"For Embedding models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"openai_compatiable/#for-embedding-models_1","title":"For Embedding models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"openai_compatiable/#f","title":"F","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p>"},{"location":"openai_compatiable/#use-the-langchain-interface-to-invoke-model","title":"Use the langchain interface to invoke model","text":""},{"location":"openai_compatiable/#use-the","title":"Use the","text":""},{"location":"supported_models/","title":"Supported Model","text":"ModeId ModelSeries ModelType Supported Engines Supported Instances Supported Services Support China Region glm-4-9b-chat glm4 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat-4bit-awq internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat internlm2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat-4bit internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e internlm2_5-1_8b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-7B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ qwen2.5 llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct qwen2.5 llm vllm g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ-128k qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-32B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-0.5B-Instruct qwen2.5 llm vllm,tgi g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-1.5B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-3B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct-AWQ qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 QwQ-32B-Preview qwen reasoning model llm huggingface,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 llama-3.3-70b-instruct-awq llama llm tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-32B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-14B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-7B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-1.5B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Llama-8B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e deepseek-r1-distill-llama-70b-awq deepseek reasoning model llm tgi,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Baichuan-M1-14B-Instruct baichuan llm huggingface g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e Qwen2-VL-72B-Instruct-AWQ qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u2705 QVQ-72B-Preview-AWQ qwen reasoning model vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e Qwen2-VL-7B-Instruct qwen2vl vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker,sagemaker_async \u2705 InternVL2_5-78B-AWQ internvl2.5 vlm lmdeploy g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e txt2video-LTX comfyui video comfyui g5.4xlarge,g5.8xlarge,g6e.2xlarge sagemaker_async \u274e whisper whisper whisper huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_async \u274e bge-base-en-v1.5 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705 bge-m3 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,ecs \u2705 bge-reranker-v2-m3 bge rerank vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705"}]}