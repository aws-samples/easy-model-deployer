{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Easy Model Deployer - Simple, Efficient, and Easy-to-Integrate  <p>EMD (Easy Model Deployer) is a lightweight tool designed to simplify model deployment. Built for developers who need reliable and scalable model serving without complex setup.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Deploy models to the cloud with EMD will use the following components in Amazon Web Services:</p> <p></p> <ol> <li> <p>User/Client initiates model deployment task, triggering pipeline to start model building.</p> </li> <li> <p>AWS CodeBuild constructs the large model using predefined configuration and publishes it to Amazon ECR.</p> </li> <li> <p>AWS CloudFormation creates a model infrastructure stack based on user selection and deploys the model from ECR to AWS services (Amazon SageMaker, EC2, ECS).</p> </li> </ol>"},{"location":"best_deployment_practices/","title":"Best Deployment Practices","text":"<p>This document provides examples of best practices for deploying models using EMD for various use cases.</p>"},{"location":"best_deployment_practices/#famous-models","title":"Famous Models","text":""},{"location":"best_deployment_practices/#mistral-small-series","title":"Mistral Small Series","text":"<pre><code>emd deploy --model-id Mistral-Small-3.1-24B-Instruct-2503 --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime\n</code></pre>"},{"location":"best_deployment_practices/#gemma-3-series","title":"Gemma 3 Series","text":"<pre><code>emd deploy --model-id gemma-3-27b-it --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime\n</code></pre>"},{"location":"best_deployment_practices/#qwen-series","title":"Qwen Series","text":""},{"location":"best_deployment_practices/#qwen25-vl-32b-instruct","title":"Qwen2.5-VL-32B-Instruct","text":"<pre><code>emd deploy --model-id Qwen2.5-VL-32B-Instruct --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime\n</code></pre>"},{"location":"best_deployment_practices/#qwq-32b","title":"QwQ-32B","text":"<pre><code>emd deploy --model-id QwQ-32B --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime\n</code></pre>"},{"location":"best_deployment_practices/#deploying-to-specific-gpu-types","title":"Deploying to Specific GPU Types","text":"<p>Choosing the right GPU type is critical for optimal performance and cost-efficiency. Use the <code>--instance-type</code> parameter to specify the GPU instance.</p>"},{"location":"best_deployment_practices/#example-deploying-qwen25-7b-on-g52xlarge","title":"Example: Deploying Qwen2.5-7B on g5.2xlarge","text":"<pre><code>emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.2xlarge --engine-type vllm --service-type sagemaker_realtime\n</code></pre>"},{"location":"best_deployment_practices/#achieving-longer-context-windows","title":"Achieving Longer Context Windows","text":"<p>To enable longer context windows, use the <code>--extra-params</code> option with engine-specific parameters.</p>"},{"location":"best_deployment_practices/#example-deploying-model-with-16k-context-window","title":"Example: Deploying model with 16k context window","text":"<pre><code>emd deploy --model-id Qwen2.5-7B-Instruct --instance-type g5.4xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{\n  \"engine_params\": {\n    \"vllm_cli_args\": \"--max_model_len 16000 --max_num_seqs 4\"\n  }\n}'\n</code></pre>"},{"location":"best_deployment_practices/#example-deploying-model-on-g4dn-instance","title":"Example: Deploying model on G4dn instance","text":"<pre><code>emd deploy --model-id Qwen2.5-14B-Instruct-AWQ --instance-type g4dn.2xlarge --engine-type vllm --service-type sagemaker_realtime --extra-params '{\n  \"engine_params\": {\n    \"environment_variables\": \"export VLLM_ATTENTION_BACKEND=XFORMERS &amp;&amp; export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\",\n    \"default_cli_args\": \" --chat-template emd/models/chat_templates/qwen_2d5_add_prefill_chat_template.jinja --max_model_len 12000 --max_num_seqs 10  --gpu_memory_utilization 0.95 --disable-log-stats --enable-auto-tool-choice --tool-call-parser hermes\"\n  }\n}'\n</code></pre>"},{"location":"best_deployment_practices/#example-customize-model-download-methods","title":"Example: Customize model download methods","text":"<ul> <li>You can load models from different locations by addingappropriate values in the extra-params parameter</li> <li>Load model from S3 <pre><code>{\n  \"model_params\":{\n    \"model_files_s3_path\":\"&lt;S3_PATH&gt;\"\n    }\n}\n</code></pre></li> <li>Load model from local path (only applicable for local deployment) <pre><code>{\n  \"model_params\": {    \"model_files_local_path\":\"&lt;LOCAL_PATH&gt;\"\n  }\n}\n</code></pre></li> <li>Skip downloading and uploading model files in codebuild, which will significantly reducedeployment time <pre><code>{\n  \"model_params\": {\n    \"need_prepare_model\":false\n  }\n}\n</code></pre></li> <li>Specify the download source for model files <pre><code>{\n  \"model_params\":{\n    \"model_files_download_source\":\"huggingface|modelscope|auto(default)\"\n    }\n}\n</code></pre></li> <li>Specify the model ID on huggingface or modelscope <pre><code>{\n  \"model_params\": {\n    \"huggingface_model_id\":\"model id on huggingface\",\"modelscope_model_id\":\"model id on modelscope\"\n    }\n}\n</code></pre></li> </ul>"},{"location":"best_deployment_practices/#environmental-variables","title":"Environmental variables","text":"<ul> <li><code>LOCAL_DEPLOY_PORT:</code> Local deployment port, default: <code>8080</code></li> </ul>"},{"location":"best_deployment_practices/#common-troubleshooting","title":"Common Troubleshooting","text":"<ul> <li>If your deployment fails due to out-of-memory issues, try:</li> <li>Using a larger instance type</li> <li>Reducing max_model_len and max_num_seqs in the engine parameters</li> <li>Setting a lower gpu_memory_utilization value (e.g., 0.8 instead of the default)</li> </ul>"},{"location":"emd_client/","title":"Usse EMD client to invoke deployed models","text":"<pre><code>emd invoke MODEL_ID MODEL_TAG (Optional)\n</code></pre>"},{"location":"emd_client/#llm-models","title":"LLM models","text":"<pre><code>emd invoke  DeepSeek-R1-Distill-Qwen-7B\n...\nInvoking model DeepSeek-R1-Distill-Qwen-7B with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: how to solve the problem of making more profit\nAssistant:&lt;think&gt;\n\nOkay, so I need to figure out how to make more profit. Profit is basically the money left after subtracting costs from revenue, right? So, increasing profit means either making more money from sales or reducing the\nexpenses. Let me think about how I can approach this.\n...\n</code></pre>"},{"location":"emd_client/#vlm-models","title":"VLM models","text":"<ol> <li> <p>upload image to a s3 path  <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model Qwen2-VL-7B-Instruct with tag dev\nEnter image path(local or s3 file): s3://your-bucket/image.jpg\nEnter prompt: What's in this image?\n...\n</code></pre></p> </li> </ol>"},{"location":"emd_client/#videotxt2video-models","title":"Video(Txt2Video) models","text":"<ol> <li>input prompt for video generation <pre><code>emd invoke txt2video-LTX\n...\nInvoking model txt2video-LTX with tag dev\nWrite a prompt, press Enter to generate a response (Ctrl+C to abort),\nUser: Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.\n...\n</code></pre></li> <li>download generated video from output_path</li> </ol>"},{"location":"emd_client/#embedding-models","title":"Embedding models","text":"<pre><code>emd invoke bge-base-en-v1.5\n...\nInvoking model bge-base-en-v1.5 with tag dev\nEnter the sentence: hello\n...\n</code></pre>"},{"location":"emd_client/#rerank-models","title":"Rerank models","text":"<pre><code>emd invoke bge-reranker-v2-m3\n...\nEnter the text_a (string): What is the capital of France?\nEnter the text_b (string): The capital of France is Paris.\n...\n</code></pre>"},{"location":"emd_client/#asr-modelswhisper","title":"ASR models(whisper)","text":"<ol> <li> <p>upload audio to a s3 path <pre><code>aws s3 cp xx.wav s3://your-bucket/xx.wav\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke whisper\n...\nEnter the s3 path to the audio file: s3://your-bucket/xx.wav\nEnter model [large-v3-turbo/large-v3]: large-v3-turbo\n...\n</code></pre></p> </li> </ol>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-guide","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"installation/#setting-up-the-environment","title":"Setting up the Environment","text":"<ol> <li> <p>Create a virtual environment: <pre><code>python -m venv emd-env\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>source emd-env/bin/activate\n</code></pre></p> </li> <li> <p>Install the required packages: <pre><code>pip install https://github.com/aws-samples/easy-model-deployer/releases/download/emd-0.7.1/emd-0.7.1-py3-none-any.whl\n</code></pre></p> </li> </ol>"},{"location":"installation/#deployment-parameters","title":"Deployment parameters","text":""},{"location":"installation/#-force-update-env-stack","title":"--force-update-env-stack","text":"<p>No additional <code>emd bootstrap</code> required for deployment. Because of other commands, status/destroy etc. require pre-bootstrapping. Therefore, it is recommended to run <code>emd bootstrap</code> separately after each upgrade.</p>"},{"location":"installation/#-extra-params","title":"--extra-params","text":"<p>Extra parameters passed to the model deployment. extra-params should be a Json object of dictionary format as follows:</p> <p><pre><code>{\n\n  \"model_params\": {\n  },\n  \"service_params\":{\n  },\n  \"instance_params\":{\n  },\n  \"engine_params\":{\n      \"cli_args\": \"&lt;command line arguments of current engine&gt;\",\n      \"api_key\":\"&lt;api key&gt;\"\n  },\n  \"framework_params\":{\n      \"uvicorn_log_level\":\"info\",\n      \"limit_concurrency\":200\n  }\n}\n</code></pre> To learn some practice examples, please refer to the Best Deployment Practices.</p>"},{"location":"installation/#local-deployment-on-the-ec2-instance","title":"Local deployment on the ec2 instance","text":"<p>This is suitable for deploying models using local GPU resources.</p>"},{"location":"installation/#pre-requisites","title":"Pre-requisites","text":""},{"location":"installation/#start-and-connect-to-ec2-instance","title":"Start and connect to EC2 instance","text":"<p>It is recommended to launch the instance using the AMI \"Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.6 (Ubuntu 22.04)\".</p>"},{"location":"installation/#deploy-model-using-emd","title":"Deploy model using EMD","text":"<pre><code>emd deploy --allow-local-deploy\n</code></pre> <p>There some EMD configuration sample settings for model deployment in the following two sections: Non-reasoning Model deployment configuration and Reasoning Model deployment configuration. Wait for the model deployment to complete.</p>"},{"location":"installation/#non-reasoning-model-deployment-configuration","title":"Non-reasoning Model deployment configuration","text":""},{"location":"installation/#qwen25-72b-instruct-awq","title":"Qwen2.5-72B-Instruct-AWQ","text":"<pre><code>? Select the model series: qwen2.5\n? Select the model name: Qwen2.5-72B-Instruct-AWQ\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\n? Select the inference engine to use: tgi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"installation/#llama-33-70b-instruct-awq","title":"llama-3.3-70b-instruct-awq","text":"<pre><code>? Select the model series: llama\n? Select the model name: llama-3.3-70b-instruct-awq\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\nengine type: tgi\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"installation/#reasoning-model-deployment-configuration","title":"Reasoning Model deployment configuration","text":""},{"location":"installation/#deepseek-r1-distill-qwen-32b","title":"DeepSeek-R1-Distill-Qwen-32B","text":"<pre><code>? Select the model series: deepseek reasoning model\n? Select the model name: DeepSeek-R1-Distill-Qwen-32B\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\nengine type: vllm\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--enable-reasoning --reasoning-parser deepseek_r1 --max_model_len 16000 --disable-log-stats --chat-template emd/models/chat_templates/deepseek_r1_distill.jinja --max_num_seq 20 --gpu_memory_utilization 0.9\"}}\n</code></pre>"},{"location":"installation/#deepseek-r1-distill-llama-70b-awq","title":"deepseek-r1-distill-llama-70b-awq","text":"<pre><code>? Select the model series: deepseek reasoning model\n? Select the model name: deepseek-r1-distill-llama-70b-awq\n? Select the service for deployment: Local\n? input the local gpu ids to deploy the model (e.g. 0,1,2): 0,1,2,3\n? Select the inference engine to use: tgi\nframework type: fastapi\n? (Optional) Additional deployment parameters (JSON string or local file path), you can skip by pressing Enter: {\"engine_params\":{\"api_key\":\"&lt;YOUR_API_KEY&gt;\", \"default_cli_args\": \"--max-total-tokens 30000 --max-concurrent-requests 30\"}}\n</code></pre>"},{"location":"installation/#examples","title":"Examples","text":""},{"location":"langchain_interface/","title":"Usse Langchain interface to invoke deployed models","text":""},{"location":"langchain_interface/#llm-models","title":"LLM models","text":"<pre><code>from emd.integrations.langchain_clients import SageMakerVllmChatModel\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.messages import HumanMessage,AIMessage,SystemMessage\nfrom langchain.tools.base import StructuredTool\nfrom langchain_core.utils.function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool\n)\nchat_model = SageMakerVllmChatModel(\n    model_id=\"Qwen2.5-7B-Instruct\",\n    model_kwargs={\n        \"temperature\":0.5,\n    }\n)\nchain = chat_model | StrOutputParser()\nmessages = [\n        HumanMessage(content=\"9.11\u548c9.9\u4e24\u4e2a\u6570\u5b57\u54ea\u4e2a\u66f4\u5927\uff1f\"),\n    ]\nprint(chain.invoke(messages))\n</code></pre>"},{"location":"langchain_interface/#vlm-models","title":"VLM models","text":"<ol> <li> <p>upload image to a s3 path <pre><code>aws s3 cp image.jpg s3://your-bucket/image.jpg\n</code></pre></p> </li> <li> <p>invoke the model <pre><code>emd invoke  Qwen2-VL-7B-Instruct\n...\nInvoking model Qwen2-VL-7B-Instruct with tag dev\nEnter image path(local or s3 file): s3://your-bucket/image.jpg\nEnter prompt: What's in this image?\n...\n</code></pre></p> </li> </ol>"},{"location":"langchain_interface/#videotxt2video-models","title":"Video(Txt2Video) models","text":"<p>Not supported</p>"},{"location":"langchain_interface/#embedding-models","title":"Embedding models","text":"<pre><code>import time\nfrom emd.integrations.langchain_clients import SageMakerVllmEmbeddings\nfrom emd.integrations.langchain_clients import SageMakerVllmRerank\nembedding_model = SageMakerVllmEmbeddings(\n    model_id=\"bge-m3\",\n)\ntext = 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'\nt0 = time.time()\nr1 = embedding_model.embed_query(text)\nt1 = time.time()\nembedding_model.embed_documents([text]*1000)\nt2 = time.time()\nprint(f\"embed_query: {t1-t0}\")\nprint(f\"embed_documents: {t2-t1}\")\n</code></pre>"},{"location":"langchain_interface/#rerank-models","title":"Rerank models","text":"<pre><code>import time\nfrom emd.integrations.langchain_clients import SageMakerVllmRerank\ndocs = [\"hi\",'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']\nquery = 'what is panda?'\nrerank_model = SageMakerVllmRerank(\n    model_id=\"bge-reranker-v2-m3\"\n)\nprint(rerank_model.rerank(query=query,documents=docs))\n</code></pre>"},{"location":"openai_compatiable/","title":"Test OpenAI compatible interface","text":""},{"location":"openai_compatiable/#sample-code","title":"Sample Code","text":"<pre><code>import openai\n# Change the api_key here to the parameter you passed in via extra-parameter\napi_key = \"your_openai_api_key\"\ndef chat_with_openai_stream(prompt):\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=\"http://ec2-54-189-171-204.us-west-2.compute.amazonaws.com:8080/v1\"\n    )\n    response = client.chat.completions.create(\n        model=\"Qwen2.5-72B-Instruct-AWQ\",\n        # model=\"Qwen2.5-1.5B-Instruct\",\n        messages=[\n        {\"role\": \"user\", \"content\": prompt}\n        ],\n        stream=True,\n        temperature=0.6\n    )\n    print(\"AI: \", end=\"\", flush=True)\n    print(response)\n    for chunk in response:\n        content = chunk.choices[0].delta.content\n        think = getattr(chunk.choices[0].delta,\"reasoning_content\",None)\n        if think is not None:\n            print(think,end=\"\",flush=True)\n        else:\n            print(content, end=\"\", flush=True)\n    print(\"\\n\")\n\ndef chat_with_openai(prompt):\n    client = openai.OpenAI(\n        api_key=api_key,\n        base_url=\"http://127.0.0.1:9000/v1\"\n    )\n    response = client.chat.completions.create(\n        model=\"DeepSeek-R1-Distill-Qwen-1.5B\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                  {\"role\": \"user\", \"content\": prompt}],\n        stream=False\n    )\n    print(response)\n\n# Test the stream and non-stream interface\nchat_with_openai_stream(\"What is the capital of France?\")\nchat_with_openai(\"What is the capital of France?\")\n</code></pre>"},{"location":"supported_models/","title":"Supported Model","text":"ModeId ModelSeries ModelType Supported Engines Supported Instances Supported Services Support China Region glm-4-9b-chat glm4 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-20b-chat-4bit-awq internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-20b-chat internlm2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-7b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 internlm2_5-7b-chat-4bit internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e internlm2_5-1_8b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-7B-Instruct qwen2.5 llm vllm,tgi,tgi g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ qwen2.5 llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge,inf2.24xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct qwen2.5 llm vllm g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ-128k qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-32B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-0.5B-Instruct qwen2.5 llm vllm,tgi g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-1.5B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-3B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct-AWQ qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,g4dn.2xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 QwQ-32B-Preview qwen reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 QwQ-32B qwen reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 llama-3.3-70b-instruct-awq llama llm tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-32B deepseek reasoning model llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-14B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-7B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B_ollama deepseek reasoning model llm ollama g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-1.5B-GGUF deepseek reasoning model llm llama.cpp g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Qwen-32B-GGUF deepseek reasoning model llm llama.cpp g5.12xlarge,g5.24xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 DeepSeek-R1-Distill-Llama-8B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-distill-llama-70b-awq deepseek reasoning model llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-671b-1.58bit_ollama deepseek reasoning model llm ollama g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e deepseek-r1-671b-1.58bit_gguf deepseek reasoning model llm llama.cpp,ktransformers g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.8xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.4xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-671b-2.51bit_gguf deepseek reasoning model llm ktransformers g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6.12xlarge,g6.16xlarge,g6.24xlarge,g6.48xlarge,g6e.8xlarge,g6e.12xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-r1-671b-4bit_gguf deepseek reasoning model llm ktransformers g5.24xlarge,g5.48xlarge,g6.24xlarge,g6.48xlarge,g6e.16xlarge,g6e.24xlarge,g6e.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 deepseek-v3-UD-IQ1_M_ollama deepseek v3 llm ollama g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e Baichuan-M1-14B-Instruct baichuan llm vllm,huggingface g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 ReaderLM-v2 jina llm vllm,tgi g4dn.2xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker_realtime,sagemaker_async,ecs \u2705 Qwen2-VL-72B-Instruct-AWQ qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 Qwen2.5-VL-72B-Instruct-AWQ qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 Qwen2.5-VL-32B-Instruct qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u2705 QVQ-72B-Preview-AWQ qwen reasoning model vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u274e Qwen2-VL-7B-Instruct qwen2vl vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker_realtime,sagemaker_async \u2705 InternVL2_5-78B-AWQ internvl2.5 vlm lmdeploy g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async \u274e gemma-3-4b-it gemma3 vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u274e gemma-3-12b-it gemma3 vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,sagemaker_async,ecs \u274e gemma-3-27b-it gemma3 vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e Mistral-Small-3.1-24B-Instruct-2503 mistral vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker_realtime,sagemaker_async,ecs \u274e txt2video-LTX comfyui video comfyui g5.4xlarge,g5.8xlarge,g6e.2xlarge sagemaker_async \u274e whisper whisper whisper huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_async \u274e bce-embedding-base_v1 bce embedding vllm g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-base-en-v1.5 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-m3 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 jina-embeddings-v3 jina embedding huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-reranker-v2-m3 bge rerank vllm g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 bge-reranker-large bge rerank vllm g4dn.2xlarge,g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705 jina-reranker-v2-base-multilingual jina rerank huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_realtime,ecs \u2705"}]}