{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Easy Model Deployer - Simple, Efficient, and Easy-to-Integrate  <p>Latest News \ud83d\udd25</p> <ul> <li>[2025/03] We officially released EMD!</li> </ul>"},{"location":"#about","title":"About","text":"<p>EMD (Easy Model Deployer) is a lightweight tool designed to simplify model deployment. Built for developers who need reliable and scalable model serving without complex setup.</p> <p>Key Features - One-click deployment of models to the cloud (Amazon SageMaker, Amazon ECS, Amazon EC2) - Diverse model types (LLMs, VLMs, Embeddings, Vision, etc.) - Rich inference engine (vLLM, TGI, Lmdeploy, etc.) - Different instance types (CPU/GPU/AWS Inferentia) - Convenient integration (OpenAI Compatible API, LangChain client, etc.)</p> <p>Notes</p> <ul> <li>Please check the Supported Models for complete list.</li> <li>OpenAI Compatible API is supported only for Amazon ECS and Amazon EC2 deployment.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>Install EMD with <code>pip</code>, currently only support for Python 3.9 and above:</p> <pre><code>curl  https://github.com/aws-samples/easy-model-deployer/releases/download/dev/emd-0.6.0-py3-none-any.whl -o emd-0.6.0-py3-none-any.whl &amp;&amp; pip install emd-0.6.0-py3-none-any.whl\"[all]\"\n</code></pre> <p>Visit our documentation to learn more.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#choose-your-default-aws-profile","title":"Choose your default aws profile.","text":"<p><pre><code>emd config set-default-profile-name\n</code></pre> Notes: If you don't set aws profile, it will use the default profile in your env (suitable for Temporary Credentials). Whenever you want to switch deployment accounts, run <code>emd config set-default-profile-name</code> </p>"},{"location":"#bootstrap-emd-stack","title":"Bootstrap emd stack","text":"<p><pre><code>emd bootstrap\n</code></pre> Notes: This is going to set up the necessary resources for model deployment. Whenever you change EMD version, run this command again. </p>"},{"location":"#choose-deployment-parameters-interactively-by-emd-deploy-or-deploy-with-one-command","title":"Choose deployment parameters interactively by <code>emd deploy</code> or deploy with one command","text":"<p><pre><code>emd deploy --model-id DeepSeek-R1-Distill-Qwen-1.5B --instance-type g5.8xlarge --engine-type vllm --framework-type fastapi --service-type sagemaker --extra-params {} --skip-confirm\n</code></pre> Notes: Get complete parameters by <code>emd deploy --help</code> and find the values of the required parameters here When you see \"Waiting for model: ...\",  it means the deployment task has started, you can quit the current task by ctrl+c. </p>"},{"location":"#check-deployment-status","title":"Check deployment status.","text":"<p><pre><code>emd status\n</code></pre>  Notes: EMD allows to launch multiple deployment tasks at the same time.</p>"},{"location":"#_1","title":"=======","text":"<p>Latest News \ud83d\udd25</p> <ul> <li>[2025/03] We officially released EMD! Check out our blog post.</li> </ul>"},{"location":"#about_1","title":"About","text":"<p>EMD (Easy Model Deployer) is a lightweight tool designed to simplify model deployment. Built for developers who need reliable and scalable model serving without complex setup.</p> <p>Key Features - One-click deployment of models to the cloud (Amazon SageMaker, Amazon ECS) or on-premises - Diverse model types (LLMs, VLMs, Embeddings, Vision, etc.) - Rich inference engine (vLLM, TGI, Lmdeploy, etc.) - Different instance types (CPU/GPU/AWS Inferentia) - Convenient integration (OpenAI Compatible API, LangChain client, etc.)</p> <p>Notes</p> <ul> <li>Please check the Supported Models for complete list.</li> <li>OpenAI Compatible API is supported only for Amazon ECS deployment.</li> </ul>"},{"location":"#getting-started_1","title":"Getting Started","text":""},{"location":"#installation_1","title":"Installation","text":"<p>Install EMD with <code>pip</code>, currently only support for Python 3.9 and above:</p> <pre><code>curl  https://github.com/aws-samples/easy-model-deployer/releases/download/dev/emd-0.6.0-py3-none-any.whl -o emd-0.6.0-py3-none-any.whl &amp;&amp; pip install emd-0.6.0-py3-none-any.whl\"[all]\"\n</code></pre> <p>Visit our documentation to learn more.</p>"},{"location":"#usage_1","title":"Usage","text":""},{"location":"#choose-your-default-aws-profile_1","title":"Choose your default aws profile.","text":"<p><pre><code>emd config set-default-profile-name\n</code></pre> Notes: If you don't set aws profile, it will use the default profile in your env (suitable for Temporary Credentials). Whenever you want to switch deployment accounts, run <code>emd config set-default-profile-name</code> </p>"},{"location":"#bootstrap-emd-stack_1","title":"Bootstrap emd stack","text":"<p><pre><code>emd bootstrap\n</code></pre> Notes: This is going to set up the necessary resources for model deployment. Whenever you change EMD version, run this command again. </p>"},{"location":"#choose-deployment-parameters-interactively-by-emd-deploy-or-deploy-with-one-command_1","title":"Choose deployment parameters interactively by <code>emd deploy</code> or deploy with one command","text":"<p><pre><code>emd deploy --model-id DeepSeek-R1-Distill-Qwen-1.5B --instance-type g5.8xlarge --engine-type vllm --framework-type fastapi --service-type sagemaker --extra-params {} --skip-confirm\n</code></pre> Notes: Get complete parameters by <code>emd deploy --help</code> and find the values of the required parameters here When you see \"Waiting for model: ...\",  it means the deployment task has started, you can quit the current task by ctrl+c. </p>"},{"location":"#check-deployment-status_1","title":"Check deployment status.","text":"<p><pre><code>emd status\n</code></pre>  Notes: EMD allows to launch multiple deployment tasks at the same time.</p>"},{"location":"#quick-functional-verfication-or-check-our-documentation-for-integration-examples","title":"Quick functional verfication or check our documentation for integration examples.","text":"<p><pre><code>emd invoke DeepSeek-R1-Distill-Qwen-1.5B\n</code></pre> Notes: Find ModelId in the output of <code>emd status</code>. </p>"},{"location":"#delete-the-deployed-model","title":"Delete the deployed model","text":"<p><pre><code>emd destroy DeepSeek-R1-Distill-Qwen-1.5B\n</code></pre> Notes: Find ModelId in the output of <code>emd status</code>. </p>"},{"location":"#documentation","title":"Documentation","text":"<p>For advanced configurations and detailed guides, visit our documentation site.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see CONTRIBUTING.md for guidelines.</p>"},{"location":"about/","title":"About Easy Model Deployer","text":"<p>Easy Model Deployer is a lightweight tool designed to simplify the machine learning model deployment process.</p>"},{"location":"about/#features","title":"Features","text":"<ul> <li>Simple deployment workflow</li> <li>Support for multiple ML frameworks</li> <li>Easy configuration</li> <li>Minimal dependencies</li> </ul>"},{"location":"about/#why-use-easy-model-deployer","title":"Why Use Easy Model Deployer?","text":"<p>Perfect for developers who want to quickly deploy ML models without dealing with complex infrastructure setup.</p>"},{"location":"about/#getting-started","title":"Getting Started","text":"<p>Check our Usage Guide to start deploying your models in minutes.</p>"},{"location":"supported_models/","title":"Supported Model","text":"ModeId ModelSeries ModelType Supported Engines Supported Instances Supported Services Support China Region glm-4-9b-chat glm4 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat-4bit-awq internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-20b-chat internlm2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 internlm2_5-7b-chat-4bit internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e internlm2_5-1_8b-chat internlm2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-7B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ qwen2.5 llm vllm,tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct qwen2.5 llm vllm g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-72B-Instruct-AWQ-128k qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-32B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-0.5B-Instruct qwen2.5 llm vllm,tgi g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge,inf2.8xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-1.5B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-3B-Instruct qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct-AWQ qwen2.5 llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u2705 Qwen2.5-14B-Instruct qwen2.5 llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 QwQ-32B-Preview qwen reasoning model llm huggingface,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 llama-3.3-70b-instruct-awq llama llm tgi g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-32B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-14B deepseek reasoning model llm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-7B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Qwen-1.5B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e DeepSeek-R1-Distill-Llama-8B deepseek reasoning model llm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,sagemaker_async,ecs \u274e deepseek-r1-distill-llama-70b-awq deepseek reasoning model llm tgi,vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u2705 Baichuan-M1-14B-Instruct baichuan llm huggingface g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async,ecs \u274e Qwen2-VL-72B-Instruct-AWQ qwen2vl vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u2705 QVQ-72B-Preview-AWQ qwen reasoning model vlm vllm g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e Qwen2-VL-7B-Instruct qwen2vl vlm vllm g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.12xlarge,g5.16xlarge,g5.24xlarge,g5.48xlarge,g6e.2xlarge sagemaker,sagemaker_async \u2705 InternVL2_5-78B-AWQ internvl2.5 vlm lmdeploy g5.12xlarge,g5.24xlarge,g5.48xlarge sagemaker,sagemaker_async \u274e txt2video-LTX comfyui video comfyui g5.4xlarge,g5.8xlarge,g6e.2xlarge sagemaker_async \u274e whisper whisper whisper huggingface g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker_async \u274e bge-base-en-v1.5 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705 bge-m3 bge embedding vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker,ecs \u2705 bge-reranker-v2-m3 bge rerank vllm g5.xlarge,g5.2xlarge,g5.4xlarge,g5.8xlarge,g5.16xlarge sagemaker \u2705"}]}