<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://github.com/aws-samples/easy-model-deployer/ollama_integration/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Ollama Integration - Easy Model Deployer</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/print-site-enum-headings1.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings2.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings3.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings4.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings5.css" rel="stylesheet" />
        <link href="../css/print-site-enum-headings6.css" rel="stylesheet" />
        <link href="../css/print-site.css" rel="stylesheet" />
        <link href="../css/print-site-readthedocs.css" rel="stylesheet" />
        <link href="../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Ollama Integration";
        var mkdocs_page_input_path = "ollama_integration.md";
        var mkdocs_page_url = "/aws-samples/easy-model-deployer/ollama_integration/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Easy Model Deployer
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../installation/">Quick Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../supported_models/">Supported Models</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../best_deployment_practices/">Best Deployment Practices</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../commands/">CLI Commands</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../api/">API Reference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../local_deployment/">Local Deployment</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../sdk_integration/">LangChain Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../swift_chat_integration/">SwiftChat Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dify_integration/">Dify Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../langflow_integration/">LangFlow Integration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../flowise_integration/">Flowise Integration</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Ollama Integration</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#key-features-of-ollama">Key Features of Ollama</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#integrating-emd-models-with-ollama">Integrating EMD Models with Ollama</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-api-orchestration">1. API Orchestration</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#prerequisites">Prerequisites</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#implementation-example">Implementation Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-fallback-mechanism">2. Fallback Mechanism</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#example-use-cases">Example Use Cases</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#additional-resources">Additional Resources</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nextchat_integration/">NextChat Integration</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Easy Model Deployer</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Ollama Integration</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/aws-samples/easy-model-deployer/edit/master/docs/ollama_integration.md">Edit on aws-samples/easy-model-deployer</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="ollama-integration">Ollama Integration</h1>
<p>This guide covers how to integrate EMD-deployed models with <a href="https://github.com/ollama/ollama">Ollama</a>, an open-source framework for running large language models locally.</p>
<h2 id="overview">Overview</h2>
<p>Ollama is a popular tool that allows you to run large language models locally on your own hardware. It provides a simple way to download, run, and manage various open-source models. By integrating EMD-deployed models with Ollama, you can create a hybrid setup that leverages both local models and your custom cloud-deployed models.</p>
<p>With Ollama integration, you can:
- Use both local models and EMD-deployed models in your applications
- Create fallback mechanisms between local and cloud models
- Compare performance between local and cloud-deployed versions
- Develop applications that work both online and offline
- Optimize for cost, performance, or privacy based on specific needs</p>
<h2 id="key-features-of-ollama">Key Features of Ollama</h2>
<ul>
<li><strong>Local Model Execution</strong>: Run models on your own hardware</li>
<li><strong>Simple API</strong>: Easy-to-use REST API for model interaction</li>
<li><strong>Model Library</strong>: Access to various open-source models</li>
<li><strong>Customization</strong>: Create and customize model configurations</li>
<li><strong>Cross-platform</strong>: Available for macOS, Windows, and Linux</li>
<li><strong>Low Resource Usage</strong>: Optimized for running on consumer hardware</li>
</ul>
<h2 id="integrating-emd-models-with-ollama">Integrating EMD Models with Ollama</h2>
<p>There are several ways to integrate EMD-deployed models with Ollama:</p>
<h3 id="1-api-orchestration">1. API Orchestration</h3>
<p>You can build an orchestration layer that routes requests between Ollama's API and your EMD-deployed model's API based on specific criteria.</p>
<h4 id="prerequisites">Prerequisites</h4>
<ol>
<li>You have successfully deployed a model using EMD with the OpenAI Compatible API enabled</li>
<li>You have <a href="https://github.com/ollama/ollama#installation">installed Ollama</a> on your local machine</li>
<li>You have the base URL and API key for your deployed EMD model</li>
</ol>
<h4 id="implementation-example">Implementation Example</h4>
<p>Here's a simple Python example that routes requests between Ollama and an EMD-deployed model:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
</span><span id="__span-0-2"><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="__span-0-3">
</span><span id="__span-0-4"><span class="k">def</span><span class="w"> </span><span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span><span id="__span-0-5"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-6"><span class="sd">    Generate text using either Ollama (local) or EMD-deployed model (cloud)</span>
</span><span id="__span-0-7">
</span><span id="__span-0-8"><span class="sd">    Args:</span>
</span><span id="__span-0-9"><span class="sd">        prompt (str): The input prompt</span>
</span><span id="__span-0-10"><span class="sd">        use_local (bool): Whether to use local Ollama model</span>
</span><span id="__span-0-11"><span class="sd">        max_tokens (int): Maximum tokens to generate</span>
</span><span id="__span-0-12">
</span><span id="__span-0-13"><span class="sd">    Returns:</span>
</span><span id="__span-0-14"><span class="sd">        str: Generated text</span>
</span><span id="__span-0-15"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-16">    <span class="k">if</span> <span class="n">use_local</span><span class="p">:</span>
</span><span id="__span-0-17">        <span class="c1"># Use Ollama API (local)</span>
</span><span id="__span-0-18">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-0-19">            <span class="s2">&quot;http://localhost:11434/api/generate&quot;</span><span class="p">,</span>
</span><span id="__span-0-20">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-21">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;llama3&quot;</span><span class="p">,</span>  <span class="c1"># or any other model you have pulled</span>
</span><span id="__span-0-22">                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
</span><span id="__span-0-23">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-0-24">            <span class="p">}</span>
</span><span id="__span-0-25">        <span class="p">)</span>
</span><span id="__span-0-26">        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-0-27">    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-28">        <span class="c1"># Use EMD-deployed model API (cloud)</span>
</span><span id="__span-0-29">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-0-30">            <span class="s2">&quot;https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions&quot;</span><span class="p">,</span>
</span><span id="__span-0-31">            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-32">                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
</span><span id="__span-0-33">                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer your-api-key&quot;</span>
</span><span id="__span-0-34">            <span class="p">},</span>
</span><span id="__span-0-35">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-0-36">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;your-deployed-model-id&quot;</span><span class="p">,</span>
</span><span id="__span-0-37">                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
</span><span id="__span-0-38">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-0-39">            <span class="p">}</span>
</span><span id="__span-0-40">        <span class="p">)</span>
</span><span id="__span-0-41">        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;choices&quot;</span><span class="p">,</span> <span class="p">[{}])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-0-42">
</span><span id="__span-0-43"><span class="c1"># Example usage</span>
</span><span id="__span-0-44"><span class="n">result</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-0-45"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span id="__span-0-46">
</span><span id="__span-0-47"><span class="n">result</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span> <span class="n">use_local</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-0-48"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="2-fallback-mechanism">2. Fallback Mechanism</h3>
<p>You can implement a fallback mechanism that tries the local Ollama model first and falls back to the EMD-deployed model if the local model fails or produces unsatisfactory results.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><span class="k">def</span><span class="w"> </span><span class="nf">generate_with_fallback</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span><span id="__span-1-2"><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-1-3"><span class="sd">    Try local model first, fall back to cloud model if needed</span>
</span><span id="__span-1-4">
</span><span id="__span-1-5"><span class="sd">    Args:</span>
</span><span id="__span-1-6"><span class="sd">        prompt (str): The input prompt</span>
</span><span id="__span-1-7"><span class="sd">        max_tokens (int): Maximum tokens to generate</span>
</span><span id="__span-1-8">
</span><span id="__span-1-9"><span class="sd">    Returns:</span>
</span><span id="__span-1-10"><span class="sd">        str: Generated text</span>
</span><span id="__span-1-11"><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-1-12">    <span class="k">try</span><span class="p">:</span>
</span><span id="__span-1-13">        <span class="c1"># Try Ollama first</span>
</span><span id="__span-1-14">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-1-15">            <span class="s2">&quot;http://localhost:11434/api/generate&quot;</span><span class="p">,</span>
</span><span id="__span-1-16">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-17">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
</span><span id="__span-1-18">                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
</span><span id="__span-1-19">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-1-20">            <span class="p">},</span>
</span><span id="__span-1-21">            <span class="n">timeout</span><span class="o">=</span><span class="mi">5</span>  <span class="c1"># Set a timeout for local model</span>
</span><span id="__span-1-22">        <span class="p">)</span>
</span><span id="__span-1-23">
</span><span id="__span-1-24">        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
</span><span id="__span-1-25">            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-1-26">            <span class="k">if</span> <span class="n">result</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>  <span class="c1"># Simple quality check</span>
</span><span id="__span-1-27">                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>
</span><span id="__span-1-28">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-1-29">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Local model error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-1-30">
</span><span id="__span-1-31">    <span class="c1"># Fall back to EMD-deployed model</span>
</span><span id="__span-1-32">    <span class="k">try</span><span class="p">:</span>
</span><span id="__span-1-33">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span><span id="__span-1-34">            <span class="s2">&quot;https://your-endpoint.execute-api.region.amazonaws.com/v1/chat/completions&quot;</span><span class="p">,</span>
</span><span id="__span-1-35">            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-36">                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
</span><span id="__span-1-37">                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer your-api-key&quot;</span>
</span><span id="__span-1-38">            <span class="p">},</span>
</span><span id="__span-1-39">            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span><span id="__span-1-40">                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;your-deployed-model-id&quot;</span><span class="p">,</span>
</span><span id="__span-1-41">                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
</span><span id="__span-1-42">                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span>
</span><span id="__span-1-43">            <span class="p">}</span>
</span><span id="__span-1-44">        <span class="p">)</span>
</span><span id="__span-1-45">
</span><span id="__span-1-46">        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
</span><span id="__span-1-47">            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;choices&quot;</span><span class="p">,</span> <span class="p">[{}])[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;message&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</span><span id="__span-1-48">            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;cloud&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>
</span><span id="__span-1-49">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="__span-1-50">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cloud model error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-1-51">
</span><span id="__span-1-52">    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Failed to generate response from both local and cloud models.&quot;</span><span class="p">}</span>
</span></code></pre></div>
<h2 id="example-use-cases">Example Use Cases</h2>
<p>With your EMD models integrated with Ollama, you can build various applications:</p>
<ul>
<li><strong>Hybrid AI Applications</strong>: Applications that use local models for basic tasks and cloud models for more complex tasks</li>
<li><strong>Offline-First Applications</strong>: Applications that work offline with local models but enhance capabilities when online</li>
<li><strong>Cost-Optimized Solutions</strong>: Use local models for frequent, simple queries and cloud models for important or complex queries</li>
<li><strong>Privacy-Focused Applications</strong>: Process sensitive data locally and only use cloud models for non-sensitive data</li>
<li><strong>Development and Testing</strong>: Use local models during development and testing, and cloud models in production</li>
</ul>
<h2 id="troubleshooting">Troubleshooting</h2>
<p>If you encounter issues with the integration:</p>
<ol>
<li>Verify that Ollama is running locally (<code>ollama list</code> should show available models)</li>
<li>Check that your EMD model is properly deployed and accessible</li>
<li>Ensure API endpoints and authentication details are correct</li>
<li>Check network connectivity if using cloud models</li>
<li>Monitor resource usage if local models are running slowly</li>
</ol>
<h2 id="additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://github.com/ollama/ollama">Ollama GitHub Repository</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/api.md">Ollama API Documentation</a></li>
<li><a href="../supported_models/">EMD Supported Models</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../flowise_integration/" class="btn btn-neutral float-left" title="Flowise Integration"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../nextchat_integration/" class="btn btn-neutral float-right" title="NextChat Integration">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/aws-samples/easy-model-deployer/" class="fa fa-code-fork" style="color: #fcfcfc"> aws-samples/easy-model-deployer</a>
        </span>
    
    
      <span><a href="../flowise_integration/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../nextchat_integration/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../js/print-site.js"></script>
      <script src="../copy-button.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
